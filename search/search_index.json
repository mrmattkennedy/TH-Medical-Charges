{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Matt Kennedy","text":"<p>Hi there  I'm Matt! I'm a data and software enthusiast, who has worked as a data scientist , data engineer , data analyst , software engineer , and I've done just about every role regarding data, and created a lot of different software.</p> <p>I love to share my findings and create visualizations to show what can be found and created from data. I also love to research and explore topics such as algorithm optimization, data compression, hyperparameter tuning, teaching machines to do stupid stuff, and much more. You can find some examples of my projects in the projects page.</p> <p>My experience includes creating software for a variety of applications, including companies as large as fortune 50, and as small as 100 employees, working with raw commercial retail data used to guide business decisions, to medical device data used to save lives.</p>"},{"location":"#contact-me","title":"Contact me","text":"<p>If you want to contact me, you can either click this email icon  to send me an email, or the same icon on the footer of this page.</p> <p>Additionally, you can find my linkedin profile at https://linkedin.com/in/mrmattkennedy - feel free to connect with me or send me a message.</p>"},{"location":"#favorite-projects","title":"Favorite projects","text":"<p>Some of these are available on my GitHub - others are not due to privacy concerns, sorry.</p> <p>For a full list of projects, check out the Projects page on the left.</p>"},{"location":"#scooter","title":"Scooter","text":"<p>A social media app built for Android, which is great for sharing text and image posts with friends. Includes sentiment analysis and document categorization, and optimized cloud caching.</p>"},{"location":"#lots-of-options","title":"Lots of Options","text":"<p>After trying to find historical options data and being unable to, I created a program that will download all options expirations available for &gt;8,000 symbols, processes them in real time every minute, and stores the data in a compressed format. This is over 200gb of data a day being downloaded and processed instantly, don't tell my ISP.</p>"},{"location":"#meleehubsmashgifs","title":"Meleehub/SmashGifs","text":"<p>A duo of projects aimed at the classic game Super Smash Bros Melee - MeleeHub was a website that was meant to share replay files and analyze game data for AI, and SmashGifs is a discord bot that will automatically turn binary .slp replay files into video highlights given a timestamp.</p>"},{"location":"#euchreai","title":"EuchreAI","text":"<p>Euchre has less than half of a deck of cards, but is somehow more complicated than most card games. This project aimed to teach an AI to play via deep reinforcement learning. Also was a great lesson in pseudo and true random number generation.</p>"},{"location":"#research","title":"Research","text":"<p>On the sidebar to the left, there are a few samples of articles available under the research tab. </p> <p>The first one, Exploring Medical Bills Across an Individual's Features, focuses on a more journalistic style of writing and exploring data using a basic dataset with a limited number of features. The goal of this is to provide a sample of my writing and style, but to keep the read short.</p> <p>The second article, Analyzing Census Populations, is a more in-depth and technical approach. This was not originally written with the intention of being shared as a journal article, but I thought it might be fun to provide here anyway and touch up a bit. The dataset has significantly more features and data, and more effort is spent on visualizing, predicting, and segmenting this data. If you have some time, it's an interesting read.</p> <p>Next, my paper on Working with Semantics and Parts-of-Speech for Non-Canonical Data explores the tasks of semantic role labeling and part-of-speech tagging when faced with non-canonical data. Topics covered include embracing the non-traditional data, redefining the rules of a task, breaking non-canonical sentences down into simpler, more canonical ones, and my favorite approach - just ignoring the problem.</p> <p>The last item is a replication of a study on traditional vs modern machine learning approaches to 30 day mortality predictions in ICU patients diagnosed with sepsis-3. This paper focuses on standard machine learning appraoches to short term mortality prediction, such as logistic regression and SAPS-II probabilistic modeling, and compares them to a modern machine-learning approach, XGBoost.</p>"},{"location":"#pictures-of-my-dog","title":"Pictures of my dog","text":""},{"location":"projects/projects_page/","title":"Projects","text":"<p>Below is a list of notable projects I've worked on, and a brief description of each, as well as skills used for each project, and a Github link if one is available.</p>"},{"location":"projects/projects_page/#stonks","title":"Stonks","text":"<ul> <li>Description<ul> <li>Web scraping application that uses ETL pipeline to evaluate recent stock options data for 2 million historical options</li> <li>Downloads and processes over 200gb of data daily in real time utilizing asynchronous http requests and multiprocessing vectors to compress data and calculate greeks</li> <li>Utilizes industry-standard data compression techniques to store data locally, as well as multiple database write-ahead connections to make data available instantly</li> </ul> </li> <li>Related skills:  Python, REST API, Concurrent networking, Data Engineering, Big Data, Data Compression, Data Availability, Data Profiling, Data Curation</li> <li>Github link:  Not publicly available yet</li> </ul>"},{"location":"projects/projects_page/#cpu-vs-gpu-deep-learning-benchmarks","title":"CPU vs GPU Deep Learning Benchmarks","text":"<ul> <li>Description<ul> <li>Deep Feed Forward Neural Network (DFFNN) created from scratch to explore concepts of neural network and benchmark CPU vs GPU performance at various steps in the machine learning pipeline</li> <li>Concepts include CPU and GPU networks written in Python and C++, hyperparameter optimization, activation functions, and stochastic-gradient-descent with momentum</li> <li>MNIST Data set used for training and testing.</li> </ul> </li> <li>Related skills:  Python, C++, Thrust, Boosting, CUDA, Machine Learning, Hyperparameter optimization, GPU computing, Linear algebra</li> <li>Github link:  https://github.com/mrmattkennedy/feedforward-neural-net-mnist</li> </ul>"},{"location":"projects/projects_page/#db-compressor","title":"DB Compressor","text":"<ul> <li>Description<ul> <li>Implemented algorithms to optimize tables autonomously, reducing query execution time in BigQuery</li> <li>Designed and implemented a data type downcasting mechanism, minimizing storage requirements and improving data retrieval efficiency</li> <li>Created specialized lookup tables to categorize repeated string objects, streamlining data storage and retrieval processes</li> </ul> </li> <li>Related skills:  Data Engineering, Python, Data Compression, SQL, GCP, BigQuery</li> <li>Github link:  Not publicly available yet</li> </ul>"},{"location":"projects/projects_page/#meleehub","title":"MeleeHub","text":"<ul> <li>Description<ul> <li>Website application acting as a file server to share replay files for the game \u201cSuper Smash Bros. Melee\u201d</li> <li>Intelligently handles large volume uploading/downloading using Google Cloud Storage signed URLs</li> <li>Utilizes Flask to handle incoming requests, as well as Firebase/Firebase Authentication to handle authentication </li> </ul> </li> <li>Related skills:  HTML, Frontend, Backend, CSS, Javascript, JQuery, Python, Flask, GCP, GCS, Asynchronous networking, Website security, ETL Pipeline, Data storage</li> <li>Github link:  https://github.com/mrmattkennedy/MeleeHub</li> </ul>"},{"location":"projects/projects_page/#informed-autonomous","title":"Informed Autonomous","text":"<ul> <li>Description<ul> <li>Android Application used as prototype for autonomous shuttles in the city of Grand Rapids </li> <li>Purpose is to provide interface with users to communicate with autonomous shuttles</li> </ul> </li> <li>Related skills:  Android, Mobile development, Networking, Autonomous shuttles, UI/UX</li> <li>Github link:  Not publicly available as the project is owned by a company and not me</li> </ul>"},{"location":"projects/projects_page/#stardew-fisher","title":"Stardew Fisher","text":"<ul> <li>Description<ul> <li>Machine Learning application that utilizes Neural-Networks, Q-learning, and image analysis to automate fishing in the game Stardew Valley</li> <li>Data provided for training and tested extracted, transformed, and loaded through data ETL pipeline</li> </ul> </li> <li>Related skills:  Python, Machine learning, Reinforcement learning, Q-learning, OpenAI gym, Image processing, ETL Pipeline</li> <li>Github link:  https://github.com/mrmattkennedy/Stardew-Fisher</li> </ul>"},{"location":"projects/projects_page/#ssbm-gifs","title":"SSBM Gifs","text":"<ul> <li>Description<ul> <li>Utilizes Discord API for user interaction to download binary replay files for the game \u201cSuper Smash Bros. Melee\u201d</li> <li>Once downloaded, binary replay files are converted to video files and processed with FFMPEG</li> <li>After processing, videos are automatically uploaded to imgur, or giphy if imgur upload fails, and a link is sent back to the user</li> </ul> </li> <li>Related skills:  Python, Discord API, Chat bot development, Image analysis, GCP, GCS, Binary file analysis, FFMPEG, Image analysis</li> <li>Github link:  Not publicly available yet</li> </ul>"},{"location":"projects/projects_page/#no-you-pick","title":"No, you pick!","text":"<ul> <li>Description<ul> <li>Android Application used for assisting groups of people make a single decision</li> <li>Utilizes firebase for lobby creation and maintenance logic, as well as custom voting algorithm</li> <li>Over 20,000 installs, currently in use globally</li> </ul> </li> <li>Related skills:  Android, Mobile development, Asynchronous networking, Network security, Firebase, NoSQL, Voting algorithms</li> <li>Github link:  Not publicly available yet</li> </ul>"},{"location":"projects/projects_page/#euchre-ai","title":"Euchre AI","text":"<ul> <li>Description<ul> <li>Deep Q-Learning AI that can play the card game Euchre</li> <li>Data provided based on game-state modeled to minimize necessary state variables as well as improve AI learning</li> <li>Utilizes cryptography to attempt true random number generation (RNG) as opposed to predictable state-based RNG</li> </ul> </li> <li>Related skills:  Python, Machine learning, Reinforcement learning, Tensorflow, Deep-Q learning, Cryptography</li> <li>Github link:  https://github.com/mrmattkennedy/Euchre-AI</li> </ul>"},{"location":"projects/projects_page/#flavortown","title":"Flavortown","text":"<ul> <li>Description<ul> <li>Interactive map that shows roughly 6,000 different restaurants featured in roughly 50 cooking shows</li> <li>Created using python script to scrape web data and Google Maps API to plot information</li> </ul> </li> <li>Related skills:  HTML, Frontend, Backend, Web design, Google maps API, UI/UX, Data scraping</li> <li>Github link:  https://github.com/mrmattkennedy/Flavortown-Book</li> </ul>"},{"location":"projects/projects_page/#scooter","title":"Scooter","text":"<ul> <li>Description<ul> <li>Social media app that utilizes NoSQL Firebase to perform real-time interactions to scale up to millions of users</li> <li>Includes optimizations to handle large volume for database transactions, image scaling for reduced storage costs, text sentiment analysis, image categorization, and much more</li> </ul> </li> <li>Related skills:  Android, Mobile development, Firebase, NoSQL, Asynchronous networking, Image compression and analysis, Text sentiment and analysis, RESTful API</li> <li>Github link:  Not publicly available</li> </ul>"},{"location":"research/census/","title":"Analyzing census populations","text":"<p>Predicting and segmenting census data to classify individuals income levels as below \\$50,000 or above \\$50,000.</p> <p>Data was provided by Census Income dataset available at the UC Irvine Machine Learning Repository.</p> <p>I will start with a visualization of each feature, then work into preprocessing. Next, I will use several different classification methods, and end with analysis and segmentation.</p> <p>Please note - any time the numbers '0' and '1' are used when referring to classes, classification, or targets, '0' means '&lt;=50k annual salary', and '1' means '&gt;50k annual salary'.</p> <p>The below imports and methods are used throughout the notebook, so they are defined at the start.</p> In\u00a0[1]: Copied! <pre>#Imports used for NN\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\n\n#Used for visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import plot_roc_curve, roc_curve, auc, confusion_matrix\n\n#General use\nfrom collections import defaultdict\nimport math\nimport time\n</pre> #Imports used for NN import pandas as pd import tensorflow as tf import numpy as np  #Used for visualization import seaborn as sns import matplotlib.pyplot as plt from sklearn.metrics import plot_roc_curve, roc_curve, auc, confusion_matrix  #General use from collections import defaultdict import math import time In\u00a0[2]: Copied! <pre>#Methods are used through this notebook\n\n#Loads train/test datasets\ndef load_datasets():\n    \n    #First row is headers, so just simple import on the csv data using pandas\n    train_csv = pd.read_csv(\"au_train.csv\")\n    test_csv = pd.read_csv(\"au_test.csv\")\n\n    #Remove period from last character in class for test cases\n    test_csv['class'] = test_csv['class'].str.replace('.', '')\n    \n    return train_csv, test_csv\n\n#Convert object columns to discrete numerical values\ndef convert_to_discrete(ds):\n    for col in ds:\n        if ds[col].dtype == np.dtype('object'):\n            temp_col = pd.Categorical(ds[col])\n            temp_col = temp_col.codes\n            ds[col] = temp_col\n    \n    return ds\n</pre> #Methods are used through this notebook  #Loads train/test datasets def load_datasets():          #First row is headers, so just simple import on the csv data using pandas     train_csv = pd.read_csv(\"au_train.csv\")     test_csv = pd.read_csv(\"au_test.csv\")      #Remove period from last character in class for test cases     test_csv['class'] = test_csv['class'].str.replace('.', '')          return train_csv, test_csv  #Convert object columns to discrete numerical values def convert_to_discrete(ds):     for col in ds:         if ds[col].dtype == np.dtype('object'):             temp_col = pd.Categorical(ds[col])             temp_col = temp_col.codes             ds[col] = temp_col          return ds <p>I'm starting with an analysis of this dataset's features. This will give us a better understanding of each feature, how it correlates, and later on, data-preprocessing as well as how we should segment.</p> In\u00a0[3]: Copied! <pre>#Load data\ntrain_csv, test_csv = load_datasets()\nds = pd.concat([train_csv, test_csv], axis=0)\n\n\"\"\"\nCreates and returns a bar chart\nds - dataset to evaluate\nclasses - different classes that an item can be categorized as (in this case, &lt;=50k and &gt;50k)\ncol - what column to look at\nx - the x values for the bar chart\nxlbl - the label for the x axis\nylbl - the label for the y axis\nrotate - whether or not to rotate the x axis tickmarks\n\nReturns:\ny0 - Items belong to class &lt;=50k\ny1 - Items belong to class &gt;50k\n\"\"\"\ndef createBarChart(ds, classes, col, x, xlbl, ylbl, rotate=0):\n    y = []\n    for i in x:\n        data = []\n        for cls in classes:\n            temp = ds.loc[(col == i) &amp; (ds['class'] == cls)]\n            data.append(temp.size)\n        y.append(data)\n    \n    y0 = [i[0] for i in y]\n    y1 = [i[1] for i in y]\n    \n    width = 0.5\n    fig = plt.figure(figsize=(12, 10))\n    ax = fig.add_axes([0,0,1,1])\n    ax.bar(x, y0, width, color='g', label='&lt;=50k')\n    ax.bar(x, y1, width,bottom=y0, color='b', label='&gt;50k')\n    if rotate &gt; 0:\n        ax.set_xticklabels(x, rotation=rotate)\n    ax.set_xlabel(xlbl)\n    ax.set_ylabel(ylbl)\n    ax.legend()\n    ax.set_title(\"{} vs annual salary\".format(col.name.capitalize()))\n    plt.show()\n    \n    return y0, y1\n\n\"\"\"\nCreates and returns a bar chart for continuous data using a range (start, stop, step)\nds - dataset to evaluate\nclasses - different classes that an item can be categorized as (in this case, &lt;=50k and &gt;50k)\ncol - what column to look at\nstart - what number to start at\nstop - what number to stop at\nstep - how large should each step be\nxlbl - the label for the x axis\nylbl - the label for the y axis\nwidth - width of each bar on graph (default is 0.5)\n\nReturns:\ny0 - Items belong to class &lt;=50k\ny1 - Items belong to class &gt;50k\n\"\"\"\ndef createBarChartContinuous(ds, classes, col, start, stop, step, xlbl, ylbl, width=0.5):\n    y=[]\n    \n    for i in range(start, stop, step):\n        data = []\n        for cls in classes:\n            temp = ds.loc[(col.between(i, i+step)) &amp; (ds['class'] == cls)]\n            data.append(len(temp.index))\n        y.append(data)\n\n    x = list(range(start, stop, step))\n    y0 = [i[0] for i in y]\n    y1 = [i[1] for i in y]\n\n    width = width\n    fig = plt.figure(figsize=(12, 10))\n    ax = fig.add_axes([0,0,1,1])\n    ax.bar(x, y0, width, color='g', label='&lt;=50k')\n    ax.bar(x, y1, width,bottom=y0, color='b', label='&gt;50k')\n    \n    ax.set_xlabel(xlbl)\n    ax.set_ylabel(ylbl)\n    ax.legend()\n    ax.set_title(\"{} vs annual salary\".format(col.name.capitalize()))\n    plt.show()\n    \n    return y0, y1\n\n\"\"\"\nCreates and returns a table that displays percentages\ntitle - title of the column focused on\nx - x values that were used for bar chart\ny0 - Items belong to class &lt;=50k\ny1 - Items belong to class &gt;50k\nmax - the stopping point for range of splits for continuous feature\n\nReturns:\nx_disp - x values that had corresponding y values (not 0) and were shown\ny0_p - Percent of items belonging to class &lt;=50k\ny1_p - Percent of items belonging to class &gt;50k\n\"\"\"\ndef createTable(title, x, y0, y1, max=0):\n    y0_p = []\n    y1_p  = []\n    x_disp = []\n    for i in range(len(y0)):\n        if y0[i] &gt; 0 or y1[i] &gt; 0:\n            y0_p.append((y0[i]/(y0[i]+y1[i]))*100)\n            y1_p.append((y1[i]/(y0[i]+y1[i]))*100)\n            if i &lt; len(x):\n                x_disp.append(x[i])\n            else:\n                x_disp.append('&gt;= {}'.format(max))\n        else:\n            pass\n            \n    table_data = []\n    for i in range(len(x_disp)):\n        table_data.append([x_disp[i], y0_p[i], y1_p[i]])\n\n    cols = [title, '% &lt;=50k', '% &gt;50k']\n    fig = plt.figure(figsize=(12, 10))\n    ax = fig.add_subplot(1,1,1)\n    table = ax.table(cellText=table_data, colLabels = cols, loc='center')\n    table.set_fontsize(14)\n    table.scale(1,2)\n    ax.axis('off')\n    plt.show()\n    \n    return x_disp, y0_p, y1_p\n\n\"\"\"\nCreates and returns a table that displays percentages\ntitle - title of the column focused on\nx - x values that will be used\ny0_p - Percent of items belonging to class &lt;=50k\ny1_p - Percent of items belonging to class &gt;50k\nxlbl - the label for the x axis\nylbl - the label for the y axis\nrotate - whether or not to rotate the x axis tickmarks\n\nReturns:\nNone\n\"\"\"\ndef showScatter(title, x, y0_p, y1_p, xlbl, rotate=0):\n    fig = plt.figure(figsize=(12, 10))\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(x, y0_p, label='&lt;=50k')\n    ax.scatter(x, y0_p)\n    ax.plot(x, y1_p, label='&gt;50k')\n    ax.scatter(x, y1_p)\n    if rotate &gt; 0:\n        ax.set_xticklabels(x, rotation=rotate)\n    ax.set_title(\"{} vs annual salary\".format(title))\n    ax.set_xlabel(xlbl)\n    ax.set_ylabel('% of category')\n    ax.legend()\n    plt.show()\n</pre> #Load data train_csv, test_csv = load_datasets() ds = pd.concat([train_csv, test_csv], axis=0)  \"\"\" Creates and returns a bar chart ds - dataset to evaluate classes - different classes that an item can be categorized as (in this case, &lt;=50k and &gt;50k) col - what column to look at x - the x values for the bar chart xlbl - the label for the x axis ylbl - the label for the y axis rotate - whether or not to rotate the x axis tickmarks  Returns: y0 - Items belong to class &lt;=50k y1 - Items belong to class &gt;50k \"\"\" def createBarChart(ds, classes, col, x, xlbl, ylbl, rotate=0):     y = []     for i in x:         data = []         for cls in classes:             temp = ds.loc[(col == i) &amp; (ds['class'] == cls)]             data.append(temp.size)         y.append(data)          y0 = [i[0] for i in y]     y1 = [i[1] for i in y]          width = 0.5     fig = plt.figure(figsize=(12, 10))     ax = fig.add_axes([0,0,1,1])     ax.bar(x, y0, width, color='g', label='&lt;=50k')     ax.bar(x, y1, width,bottom=y0, color='b', label='&gt;50k')     if rotate &gt; 0:         ax.set_xticklabels(x, rotation=rotate)     ax.set_xlabel(xlbl)     ax.set_ylabel(ylbl)     ax.legend()     ax.set_title(\"{} vs annual salary\".format(col.name.capitalize()))     plt.show()          return y0, y1  \"\"\" Creates and returns a bar chart for continuous data using a range (start, stop, step) ds - dataset to evaluate classes - different classes that an item can be categorized as (in this case, &lt;=50k and &gt;50k) col - what column to look at start - what number to start at stop - what number to stop at step - how large should each step be xlbl - the label for the x axis ylbl - the label for the y axis width - width of each bar on graph (default is 0.5)  Returns: y0 - Items belong to class &lt;=50k y1 - Items belong to class &gt;50k \"\"\" def createBarChartContinuous(ds, classes, col, start, stop, step, xlbl, ylbl, width=0.5):     y=[]          for i in range(start, stop, step):         data = []         for cls in classes:             temp = ds.loc[(col.between(i, i+step)) &amp; (ds['class'] == cls)]             data.append(len(temp.index))         y.append(data)      x = list(range(start, stop, step))     y0 = [i[0] for i in y]     y1 = [i[1] for i in y]      width = width     fig = plt.figure(figsize=(12, 10))     ax = fig.add_axes([0,0,1,1])     ax.bar(x, y0, width, color='g', label='&lt;=50k')     ax.bar(x, y1, width,bottom=y0, color='b', label='&gt;50k')          ax.set_xlabel(xlbl)     ax.set_ylabel(ylbl)     ax.legend()     ax.set_title(\"{} vs annual salary\".format(col.name.capitalize()))     plt.show()          return y0, y1  \"\"\" Creates and returns a table that displays percentages title - title of the column focused on x - x values that were used for bar chart y0 - Items belong to class &lt;=50k y1 - Items belong to class &gt;50k max - the stopping point for range of splits for continuous feature  Returns: x_disp - x values that had corresponding y values (not 0) and were shown y0_p - Percent of items belonging to class &lt;=50k y1_p - Percent of items belonging to class &gt;50k \"\"\" def createTable(title, x, y0, y1, max=0):     y0_p = []     y1_p  = []     x_disp = []     for i in range(len(y0)):         if y0[i] &gt; 0 or y1[i] &gt; 0:             y0_p.append((y0[i]/(y0[i]+y1[i]))*100)             y1_p.append((y1[i]/(y0[i]+y1[i]))*100)             if i &lt; len(x):                 x_disp.append(x[i])             else:                 x_disp.append('&gt;= {}'.format(max))         else:             pass                  table_data = []     for i in range(len(x_disp)):         table_data.append([x_disp[i], y0_p[i], y1_p[i]])      cols = [title, '% &lt;=50k', '% &gt;50k']     fig = plt.figure(figsize=(12, 10))     ax = fig.add_subplot(1,1,1)     table = ax.table(cellText=table_data, colLabels = cols, loc='center')     table.set_fontsize(14)     table.scale(1,2)     ax.axis('off')     plt.show()          return x_disp, y0_p, y1_p  \"\"\" Creates and returns a table that displays percentages title - title of the column focused on x - x values that will be used y0_p - Percent of items belonging to class &lt;=50k y1_p - Percent of items belonging to class &gt;50k xlbl - the label for the x axis ylbl - the label for the y axis rotate - whether or not to rotate the x axis tickmarks  Returns: None \"\"\" def showScatter(title, x, y0_p, y1_p, xlbl, rotate=0):     fig = plt.figure(figsize=(12, 10))     ax = fig.add_subplot(1,1,1)     ax.plot(x, y0_p, label='&lt;=50k')     ax.scatter(x, y0_p)     ax.plot(x, y1_p, label='&gt;50k')     ax.scatter(x, y1_p)     if rotate &gt; 0:         ax.set_xticklabels(x, rotation=rotate)     ax.set_title(\"{} vs annual salary\".format(title))     ax.set_xlabel(xlbl)     ax.set_ylabel('% of category')     ax.legend()     plt.show() In\u00a0[4]: Copied! <pre>#These will be used for each feature\ntemp_col = pd.Categorical(ds['class'])\ntemp_col = temp_col.codes\nds['class'] = temp_col\nclasses = ds['class'].unique()\nylbl = '# of people'\n</pre> #These will be used for each feature temp_col = pd.Categorical(ds['class']) temp_col = temp_col.codes ds['class'] = temp_col classes = ds['class'].unique() ylbl = '# of people' In\u00a0[5]: Copied! <pre>col = ds['age']\nx = col.unique()\nx.sort()\n\nxlbl = col.name.capitalize()\ny0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl)\nx, y0_p, y1_p = createTable(xlbl, x, y0, y1)\nshowScatter(xlbl, x, y0_p, y1_p, xlbl)\n</pre> col = ds['age'] x = col.unique() x.sort()  xlbl = col.name.capitalize() y0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl) x, y0_p, y1_p = createTable(xlbl, x, y0, y1) showScatter(xlbl, x, y0_p, y1_p, xlbl) In\u00a0[6]: Copied! <pre>col = ds['workclass']\nx = col.unique()\nxlbl = col.name.capitalize()\n\ny0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 45)\nx, y0_p, y1_p = createTable(col.name.capitalize(), x, y0, y1)\n#showScatter(xlbl, x, y0_p, y1_p, xlbl)\n</pre> col = ds['workclass'] x = col.unique() xlbl = col.name.capitalize()  y0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 45) x, y0_p, y1_p = createTable(col.name.capitalize(), x, y0, y1) #showScatter(xlbl, x, y0_p, y1_p, xlbl) <pre>&lt;ipython-input-3-a5b821a2f71f&gt;:37: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x, rotation=rotate)\n</pre> In\u00a0[7]: Copied! <pre>col = ds['fnlwgt']\nstart=10000\nstop=1500000\nstep=20000\nxlbl = col.name.capitalize()\n\ny0, y1 = createBarChartContinuous(ds, classes, col, start, stop, step, xlbl, ylbl, 10000)\n\nvals = list(range(start, stop, step))\nx = []\nfor i in range(len(vals)-1):\n    x.append(\"{}-{}\".format(vals[i], vals[i+1]-1))\n\nx, y0_p, y1_p = createTable(xlbl, x, y0, y1, max=1490000)\nshowScatter(xlbl, x, y0_p, y1_p, xlbl, 90)\n</pre> col = ds['fnlwgt'] start=10000 stop=1500000 step=20000 xlbl = col.name.capitalize()  y0, y1 = createBarChartContinuous(ds, classes, col, start, stop, step, xlbl, ylbl, 10000)  vals = list(range(start, stop, step)) x = [] for i in range(len(vals)-1):     x.append(\"{}-{}\".format(vals[i], vals[i+1]-1))  x, y0_p, y1_p = createTable(xlbl, x, y0, y1, max=1490000) showScatter(xlbl, x, y0_p, y1_p, xlbl, 90) <pre>&lt;ipython-input-3-a5b821a2f71f&gt;:154: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x, rotation=rotate)\n</pre> In\u00a0[8]: Copied! <pre>col = ds['education']\nx = col.unique()\nxlbl = col.name.capitalize()\n\ny0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 45)\nx, y0_p, y1_p = createTable(xlbl, x, y0, y1)\n#showScatter(xlbl, x, y0_p, y1_p, xlbl, 45)\n</pre> col = ds['education'] x = col.unique() xlbl = col.name.capitalize()  y0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 45) x, y0_p, y1_p = createTable(xlbl, x, y0, y1) #showScatter(xlbl, x, y0_p, y1_p, xlbl, 45) <pre>&lt;ipython-input-3-a5b821a2f71f&gt;:37: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x, rotation=rotate)\n</pre> In\u00a0[9]: Copied! <pre>col = ds['marital-status']\nx = col.unique()\nxlbl = col.name.capitalize()\n\ny0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 45)\nx, y0_p, y1_p = createTable(xlbl, x, y0, y1)\n#showScatter(xlbl, x, y0_p, y1_p, xlbl, 45)\n</pre> col = ds['marital-status'] x = col.unique() xlbl = col.name.capitalize()  y0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 45) x, y0_p, y1_p = createTable(xlbl, x, y0, y1) #showScatter(xlbl, x, y0_p, y1_p, xlbl, 45) <pre>&lt;ipython-input-3-a5b821a2f71f&gt;:37: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x, rotation=rotate)\n</pre> In\u00a0[10]: Copied! <pre>col = ds['occupation']\nx = col.unique()\nxlbl = col.name.capitalize()\n\ny0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 45)\nx, y0_p, y1_p = createTable(xlbl, x, y0, y1)\n#showScatter(xlbl, x, y0_p, y1_p, xlbl, 45)\n</pre> col = ds['occupation'] x = col.unique() xlbl = col.name.capitalize()  y0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 45) x, y0_p, y1_p = createTable(xlbl, x, y0, y1) #showScatter(xlbl, x, y0_p, y1_p, xlbl, 45) <pre>&lt;ipython-input-3-a5b821a2f71f&gt;:37: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x, rotation=rotate)\n</pre> In\u00a0[11]: Copied! <pre>col = ds['relationship']\nx = col.unique()\nxlbl = col.name.capitalize()\n\ny0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 45)\nx, y0_p, y1_p = createTable(xlbl, x, y0, y1)\n#showScatter(xlbl, x, y0_p, y1_p, xlbl, 45)\n</pre> col = ds['relationship'] x = col.unique() xlbl = col.name.capitalize()  y0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 45) x, y0_p, y1_p = createTable(xlbl, x, y0, y1) #showScatter(xlbl, x, y0_p, y1_p, xlbl, 45) <pre>&lt;ipython-input-3-a5b821a2f71f&gt;:37: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x, rotation=rotate)\n</pre> In\u00a0[12]: Copied! <pre>col = ds['race']\nx = col.unique()\nxlbl = col.name.capitalize()\n\ny0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 45)\nx, y0_p, y1_p = createTable(xlbl, x, y0, y1)\n#showScatter(xlbl, x, y0_p, y1_p, xlbl, 45)\n</pre> col = ds['race'] x = col.unique() xlbl = col.name.capitalize()  y0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 45) x, y0_p, y1_p = createTable(xlbl, x, y0, y1) #showScatter(xlbl, x, y0_p, y1_p, xlbl, 45) <pre>&lt;ipython-input-3-a5b821a2f71f&gt;:37: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x, rotation=rotate)\n</pre> In\u00a0[13]: Copied! <pre>col = ds['sex']\nx = col.unique()\nxlbl = col.name.capitalize()\n\ny0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 45)\nx, y0_p, y1_p = createTable(xlbl, x, y0, y1)\n</pre> col = ds['sex'] x = col.unique() xlbl = col.name.capitalize()  y0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 45) x, y0_p, y1_p = createTable(xlbl, x, y0, y1) <pre>&lt;ipython-input-3-a5b821a2f71f&gt;:37: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x, rotation=rotate)\n</pre> In\u00a0[14]: Copied! <pre>col = ds['capital-gain']\nxlbl = col.name.capitalize()\n\nstart=0\nstop=100000\nstep=1000\n\ny0, y1 = createBarChartContinuous(ds, classes, col, start, stop, step, xlbl, ylbl, width=600)\n\nvals = list(range(start, stop, step))\nx = []\nfor i in range(len(vals)-1):\n    x.append(\"{}-{}\".format(vals[i], vals[i+1]-1))\nx, y0_p, y1_p = createTable(xlbl, x, y0, y1, 42000)\nshowScatter(xlbl, x, y0_p, y1_p, xlbl, 70)\n</pre> col = ds['capital-gain'] xlbl = col.name.capitalize()  start=0 stop=100000 step=1000  y0, y1 = createBarChartContinuous(ds, classes, col, start, stop, step, xlbl, ylbl, width=600)  vals = list(range(start, stop, step)) x = [] for i in range(len(vals)-1):     x.append(\"{}-{}\".format(vals[i], vals[i+1]-1)) x, y0_p, y1_p = createTable(xlbl, x, y0, y1, 42000) showScatter(xlbl, x, y0_p, y1_p, xlbl, 70)  <pre>&lt;ipython-input-3-a5b821a2f71f&gt;:154: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x, rotation=rotate)\n</pre> In\u00a0[15]: Copied! <pre>col = ds['capital-loss']\nxlbl = col.name.capitalize()\n\nstart=0\nstop=4400\nstep=100\n\ny0, y1 = createBarChartContinuous(ds, classes, col, start, stop, step, xlbl, ylbl, width=75)\n\nvals = list(range(start, stop, step))\nx = []\nfor i in range(len(vals)-1):\n    x.append(\"{}-{}\".format(vals[i], vals[i+1]-1))\nx, y0_p, y1_p = createTable(xlbl, x, y0, y1, 4000)\nshowScatter(xlbl, x, y0_p, y1_p, xlbl, 60)\n</pre> col = ds['capital-loss'] xlbl = col.name.capitalize()  start=0 stop=4400 step=100  y0, y1 = createBarChartContinuous(ds, classes, col, start, stop, step, xlbl, ylbl, width=75)  vals = list(range(start, stop, step)) x = [] for i in range(len(vals)-1):     x.append(\"{}-{}\".format(vals[i], vals[i+1]-1)) x, y0_p, y1_p = createTable(xlbl, x, y0, y1, 4000) showScatter(xlbl, x, y0_p, y1_p, xlbl, 60) <pre>&lt;ipython-input-3-a5b821a2f71f&gt;:154: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x, rotation=rotate)\n</pre> In\u00a0[16]: Copied! <pre>col = ds['hours-per-week']\nx = col.unique()\nx.sort()\nxlbl = col.name.capitalize()\n\ny0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl)\nx, y0_p, y1_p = createTable(xlbl, x, y0, y1)\nshowScatter(xlbl, x, y0_p, y1_p, xlbl)\n</pre> col = ds['hours-per-week'] x = col.unique() x.sort() xlbl = col.name.capitalize()  y0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl) x, y0_p, y1_p = createTable(xlbl, x, y0, y1) showScatter(xlbl, x, y0_p, y1_p, xlbl) In\u00a0[17]: Copied! <pre>col = ds['native-country']\nx = col.unique()\nxlbl = col.name.capitalize()\n\ny0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 90)\nx, y0_p, y1_p = createTable(xlbl, x, y0, y1)\n#showScatter(xlbl, x, y0_p, y1_p, xlbl, 90)\n</pre> col = ds['native-country'] x = col.unique() xlbl = col.name.capitalize()  y0, y1 = createBarChart(ds, classes, col, x, xlbl, ylbl, 90) x, y0_p, y1_p = createTable(xlbl, x, y0, y1) #showScatter(xlbl, x, y0_p, y1_p, xlbl, 90) <pre>&lt;ipython-input-3-a5b821a2f71f&gt;:37: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(x, rotation=rotate)\n</pre> In\u00a0[18]: Copied! <pre>#Gets the counts of each item for a given column\ndef get_object_data(ds):\n    #Get # of rows in dataset\n    total = len(ds)\n    feature_probs = {}\n    \n    for col in ds:\n        \n        #Don't want to count target col\n        if col == 'class':\n            continue\n\n        #If column is object, count each item and divide by # of rows\n        elif ds[col].dtype == np.dtype('object'):\n            feature_probs[col] = ds[col].value_counts().to_dict()\n\n    return feature_probs\n</pre> #Gets the counts of each item for a given column def get_object_data(ds):     #Get # of rows in dataset     total = len(ds)     feature_probs = {}          for col in ds:                  #Don't want to count target col         if col == 'class':             continue          #If column is object, count each item and divide by # of rows         elif ds[col].dtype == np.dtype('object'):             feature_probs[col] = ds[col].value_counts().to_dict()      return feature_probs In\u00a0[19]: Copied! <pre>from scipy.stats import chi2_contingency \ntrain_csv, test_csv = load_datasets()\nds = pd.concat([train_csv, test_csv], axis=0)\n\n#Load counts of each subitem for each column in each class\nfeature_class_counts = {}\nclasses = train_csv['class'].unique()\nfor k in classes:\n    #Just get the rows belonging to the specific class\n    current_class = ds.loc[ds['class'] == k]\n    feature_class_counts[k] = get_object_data(current_class)\n\n#Loop through obj columns for each class, get the chi2 and p values\nobj_cols = list(feature_class_counts[' &lt;=50K'].keys())\nfor col in obj_cols:\n    keys = list(feature_class_counts[' &lt;=50K'][col].keys()) \n    \n    table = defaultdict(list)\n    for k in keys:\n        for cls in classes:\n            try:\n                table[k].append(feature_class_counts[cls][col][k])\n            except KeyError:\n                table[k].append(0)\n\n    obs = np.array(list(table.values()))\n    chi2, p, _, _ = chi2_contingency(obs)\n    print(\"{}:\\tchi2: {},\\tp: {}\".format(col, round(chi2, 2), round(p, 2)))\n</pre> from scipy.stats import chi2_contingency  train_csv, test_csv = load_datasets() ds = pd.concat([train_csv, test_csv], axis=0)  #Load counts of each subitem for each column in each class feature_class_counts = {} classes = train_csv['class'].unique() for k in classes:     #Just get the rows belonging to the specific class     current_class = ds.loc[ds['class'] == k]     feature_class_counts[k] = get_object_data(current_class)  #Loop through obj columns for each class, get the chi2 and p values obj_cols = list(feature_class_counts[' &lt;=50K'].keys()) for col in obj_cols:     keys = list(feature_class_counts[' &lt;=50K'][col].keys())           table = defaultdict(list)     for k in keys:         for cls in classes:             try:                 table[k].append(feature_class_counts[cls][col][k])             except KeyError:                 table[k].append(0)      obs = np.array(list(table.values()))     chi2, p, _, _ = chi2_contingency(obs)     print(\"{}:\\tchi2: {},\\tp: {}\".format(col, round(chi2, 2), round(p, 2)))       <pre>workclass:\tchi2: 1610.75,\tp: 0.0\neducation:\tchi2: 6537.97,\tp: 0.0\nmarital-status:\tchi2: 9816.02,\tp: 0.0\noccupation:\tchi2: 5983.16,\tp: 0.0\nrelationship:\tchi2: 10088.72,\tp: 0.0\nrace:\tchi2: 487.03,\tp: 0.0\nsex:\tchi2: 2248.85,\tp: 0.0\nnative-country:\tchi2: 452.23,\tp: 0.0\n</pre> <p>Every categorical variable not only had  p-values below an alpha of 0.01, they were all 0! Meaning every variable is important, although some had pretty large chi2 scores, and some had extremely large chi2 squares. Let's try classifying, then take a look at the analysis.</p> <p>Now that we've looked at the categorical data, let's look at the numerical data using two different tests:</p> <ul> <li>Pearson's correlation, which measures the linear correlation from -1 to 1, where 0 is no linear correlation, also assumes gaussian distribution</li> <li>Spearman's correlation, which works better for variables that may not have a gaussian distribution and a non-linear correlation</li> </ul> In\u00a0[20]: Copied! <pre>from scipy.stats import pearsonr, spearmanr\n\n#Load datasets, convert and pop target column\ntrain_csv, test_csv = load_datasets()\nds = pd.concat([train_csv, test_csv], axis=0)\nds['class'] = pd.Categorical(ds['class'])\nds['class'] = ds['class'].cat.codes\ny = ds.pop('class').to_numpy()\n\n#Loop through each column, check numerical columns\nfor col in ds:\n    if ds[col].dtype == np.dtype('int64'):\n        x = ds[col].to_numpy()\n        pearson = pearsonr(x, y)\n        spearman = spearmanr(x, y)\n        \n        print(\"{0}\\tPearson: {1:.{5}f}, p: {2:.{5}f}\\tSpearman: {3:.{5}f}, p: {4:.{5}f}\".format(col, pearson[0], pearson[1], spearman[0], spearman[1], 3))\n</pre> from scipy.stats import pearsonr, spearmanr  #Load datasets, convert and pop target column train_csv, test_csv = load_datasets() ds = pd.concat([train_csv, test_csv], axis=0) ds['class'] = pd.Categorical(ds['class']) ds['class'] = ds['class'].cat.codes y = ds.pop('class').to_numpy()  #Loop through each column, check numerical columns for col in ds:     if ds[col].dtype == np.dtype('int64'):         x = ds[col].to_numpy()         pearson = pearsonr(x, y)         spearman = spearmanr(x, y)                  print(\"{0}\\tPearson: {1:.{5}f}, p: {2:.{5}f}\\tSpearman: {3:.{5}f}, p: {4:.{5}f}\".format(col, pearson[0], pearson[1], spearman[0], spearman[1], 3)) <pre>age\tPearson: 0.230, p: 0.000\tSpearman: 0.269, p: 0.000\nfnlwgt\tPearson: -0.006, p: 0.161\tSpearman: -0.006, p: 0.190\neducation-num\tPearson: 0.333, p: 0.000\tSpearman: 0.328, p: 0.000\ncapital-gain\tPearson: 0.223, p: 0.000\tSpearman: 0.278, p: 0.000\ncapital-loss\tPearson: 0.148, p: 0.000\tSpearman: 0.138, p: 0.000\nhours-per-week\tPearson: 0.228, p: 0.000\tSpearman: 0.268, p: 0.000\n</pre> <p>Every variables had similar Pearson and Spearman scores except for one - fnlwgt. We could see less of a trend earlier, but here we can see direct proof from both tests that the fnlwgt column has little to no correlation with the target. The scores from both tests for fnlwgt are nearly 0, which shows this minimal correlation. Interesting, capital loss has a relatively minor impact on target outcome as well, although not as little as fnlwgt.</p> <p>==========================================================</p> In\u00a0[21]: Copied! <pre>def prune(ds):\n    cols = ['fnlwgt', 'education', 'relationship']\n    for col in cols:\n        ds.pop(col)\n    \n    return ds\n</pre> def prune(ds):     cols = ['fnlwgt', 'education', 'relationship']     for col in cols:         ds.pop(col)          return ds <p>==========================================================</p> In\u00a0[22]: Copied! <pre>#Used for each classification method\ndef create_confusion_matrix(preds, target):\n    plt.figure(figsize=(8, 6))\n    cm = confusion_matrix(test_y, preds)\n    print(cm)\n    hm = sns.heatmap(cm, annot=True, xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])\n    hm.xaxis.tick_top() # x axis on top\n    hm.xaxis.set_label_position('top')\n    return hm\n</pre> #Used for each classification method def create_confusion_matrix(preds, target):     plt.figure(figsize=(8, 6))     cm = confusion_matrix(test_y, preds)     print(cm)     hm = sns.heatmap(cm, annot=True, xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])     hm.xaxis.tick_top() # x axis on top     hm.xaxis.set_label_position('top')     return hm In\u00a0[23]: Copied! <pre>#Load and format data\ntrain_csv, test_csv = load_datasets()\n\n#Prune\ntrain_csv = prune(train_csv)\ntest_csv = prune(test_csv)\n\n#Convert object columns to discrete numerical values\ntrain_csv = convert_to_discrete(train_csv)\ntest_csv = convert_to_discrete(test_csv)\n</pre> #Load and format data train_csv, test_csv = load_datasets()  #Prune train_csv = prune(train_csv) test_csv = prune(test_csv)  #Convert object columns to discrete numerical values train_csv = convert_to_discrete(train_csv) test_csv = convert_to_discrete(test_csv) In\u00a0[24]: Copied! <pre>def create_model(opt='adam', k_r=None, b_r=None, a_r=None):\n    #Create model, add 2 hidden layers with relu activation, and compile with adam optimizer\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(250, \n                              activation='relu',\n                              kernel_regularizer=k_r,\n                              bias_regularizer=b_r,\n                              activity_regularizer=a_r),\n        tf.keras.layers.Dense(150, \n                              activation='relu',\n                              kernel_regularizer=k_r,\n                              bias_regularizer=b_r,\n                              activity_regularizer=a_r),\n        tf.keras.layers.Dense(1)\n    ])\n\n    model.compile(optimizer=opt,\n        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n        metrics=['accuracy'])\n    \n    return model\n</pre> def create_model(opt='adam', k_r=None, b_r=None, a_r=None):     #Create model, add 2 hidden layers with relu activation, and compile with adam optimizer     model = tf.keras.Sequential([         tf.keras.layers.Dense(250,                                activation='relu',                               kernel_regularizer=k_r,                               bias_regularizer=b_r,                               activity_regularizer=a_r),         tf.keras.layers.Dense(150,                                activation='relu',                               kernel_regularizer=k_r,                               bias_regularizer=b_r,                               activity_regularizer=a_r),         tf.keras.layers.Dense(1)     ])      model.compile(optimizer=opt,         loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),         metrics=['accuracy'])          return model In\u00a0[25]: Copied! <pre>#Get train/validation split using sklearn\nfrom sklearn.model_selection import train_test_split\n\ntrain_csv, val_csv = train_test_split(train_csv, test_size=0.1)\\\n#Pop the class columns off each dataset to save as targets for each\ntrain_y = train_csv.pop('class')\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_csv.values, train_y.values))\ntrain_dataset = train_dataset.shuffle(len(train_csv)).batch(64)\n\nval_y = val_csv.pop('class')\nval_dataset = tf.data.Dataset.from_tensor_slices((val_csv.values, val_y.values))\nval_dataset = val_dataset.shuffle(len(val_csv)).batch(64)\n\ntest_y = test_csv.pop('class')\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_csv.values, test_y.values))\ntest_dataset = test_dataset.shuffle(len(test_csv)).batch(len(test_csv))\n</pre> #Get train/validation split using sklearn from sklearn.model_selection import train_test_split  train_csv, val_csv = train_test_split(train_csv, test_size=0.1)\\ #Pop the class columns off each dataset to save as targets for each train_y = train_csv.pop('class') train_dataset = tf.data.Dataset.from_tensor_slices((train_csv.values, train_y.values)) train_dataset = train_dataset.shuffle(len(train_csv)).batch(64)  val_y = val_csv.pop('class') val_dataset = tf.data.Dataset.from_tensor_slices((val_csv.values, val_y.values)) val_dataset = val_dataset.shuffle(len(val_csv)).batch(64)  test_y = test_csv.pop('class') test_dataset = tf.data.Dataset.from_tensor_slices((test_csv.values, test_y.values)) test_dataset = test_dataset.shuffle(len(test_csv)).batch(len(test_csv)) In\u00a0[26]: Copied! <pre>import pickle\nepochs = 1000\n\nrerun_cell = False\nif rerun_cell:\n    #Make a list of each optimizer available in tensorflow\n    opts = ['adadelta', 'adagrad', 'adam', 'adamax', 'ftrl', 'nadam', 'rmsprop', 'sgd']\n    results = {}\n\n    #Run each optimizer for results\n    for opt in opts:\n        #Create model\n        model = create_model(opt)\n        epoch_tests = []\n\n        #Create callback\n        epoch_test_callback = tf.keras.callbacks.LambdaCallback(\n            on_epoch_end=lambda epoch,logs: epoch_tests.append(model.evaluate(test_dataset, verbose=0)))\n\n        #Fit model with train dataset using 500 epochs\n        start = time.time()\n        history = model.fit(train_dataset, validation_data=val_dataset, callbacks=[epoch_test_callback], epochs=epochs, verbose=0)\n        end = time.time()\n\n        #Print time\n        hours, rem = divmod(end-start, 3600)\n        minutes, seconds = divmod(rem, 60)\n        print(\"Runtime of NN with {} optimizer: {:0&gt;2}:{:0&gt;2}:{:05.2f} (HH:MM:SS.MS)\".format(opt, int(hours),int(minutes),seconds))\n\n        #Split epoch tests into 2 lists\n        test_losses = [i[0] for i in epoch_tests]\n        test_accs = [i[1] for i in epoch_tests]\n\n        #Save results\n        results[opt] = {'train_loss': history.history['loss'], 'train_accuracy': history.history['accuracy'],\n                        'test_loss': test_losses, 'test_accuracy': test_accs}\n\n    #Save results\n    f = open(\"neural_network_results.pkl\",\"wb\")\n    pickle.dump(results,f)\n    f.close()\n</pre> import pickle epochs = 1000  rerun_cell = False if rerun_cell:     #Make a list of each optimizer available in tensorflow     opts = ['adadelta', 'adagrad', 'adam', 'adamax', 'ftrl', 'nadam', 'rmsprop', 'sgd']     results = {}      #Run each optimizer for results     for opt in opts:         #Create model         model = create_model(opt)         epoch_tests = []          #Create callback         epoch_test_callback = tf.keras.callbacks.LambdaCallback(             on_epoch_end=lambda epoch,logs: epoch_tests.append(model.evaluate(test_dataset, verbose=0)))          #Fit model with train dataset using 500 epochs         start = time.time()         history = model.fit(train_dataset, validation_data=val_dataset, callbacks=[epoch_test_callback], epochs=epochs, verbose=0)         end = time.time()          #Print time         hours, rem = divmod(end-start, 3600)         minutes, seconds = divmod(rem, 60)         print(\"Runtime of NN with {} optimizer: {:0&gt;2}:{:0&gt;2}:{:05.2f} (HH:MM:SS.MS)\".format(opt, int(hours),int(minutes),seconds))          #Split epoch tests into 2 lists         test_losses = [i[0] for i in epoch_tests]         test_accs = [i[1] for i in epoch_tests]          #Save results         results[opt] = {'train_loss': history.history['loss'], 'train_accuracy': history.history['accuracy'],                         'test_loss': test_losses, 'test_accuracy': test_accs}      #Save results     f = open(\"neural_network_results.pkl\",\"wb\")     pickle.dump(results,f)     f.close() In\u00a0[27]: Copied! <pre>#Get results with regularization\nrerun_cell = False\nif rerun_cell:\n    #Make a list of each optimizer available in tensorflow\n    opts = ['adadelta', 'adagrad', 'adam', 'adamax', 'ftrl', 'nadam', 'rmsprop', 'sgd']\n    results = {}\n\n    #Run each optimizer for results\n    for opt in opts:\n        #Create model\n        model = create_model(opt, \n                             k_r=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4), \n                             b_r=tf.keras.regularizers.l2(1e-4),\n                             a_r=tf.keras.regularizers.l2(1e-4))\n        epoch_tests = []\n\n        #Create callback\n        epoch_test_callback = tf.keras.callbacks.LambdaCallback(\n            on_epoch_end=lambda epoch,logs: epoch_tests.append(model.evaluate(test_dataset, verbose=0)))\n\n        #Fit model with train dataset using 500 epochs\n        start = time.time()\n        history = model.fit(train_dataset, validation_data=val_dataset, callbacks=[epoch_test_callback], epochs=epochs, verbose=0)\n        end = time.time()\n\n        #Print time\n        hours, rem = divmod(end-start, 3600)\n        minutes, seconds = divmod(rem, 60)\n        print(\"Runtime of NN with {} optimizer: {:0&gt;2}:{:0&gt;2}:{:05.2f} (HH:MM:SS.MS)\".format(opt, int(hours),int(minutes),seconds))\n\n        #Split epoch tests into 2 lists\n        test_losses = [i[0] for i in epoch_tests]\n        test_accs = [i[1] for i in epoch_tests]\n\n        #Save results\n        results[opt] = {'train_loss': history.history['loss'], 'train_accuracy': history.history['accuracy'],\n                        'test_loss': test_losses, 'test_accuracy': test_accs}\n\n    #Save results\n    f = open(\"neural_network_reg_results.pkl\",\"wb\")\n    pickle.dump(results,f)\n    f.close()\n</pre> #Get results with regularization rerun_cell = False if rerun_cell:     #Make a list of each optimizer available in tensorflow     opts = ['adadelta', 'adagrad', 'adam', 'adamax', 'ftrl', 'nadam', 'rmsprop', 'sgd']     results = {}      #Run each optimizer for results     for opt in opts:         #Create model         model = create_model(opt,                               k_r=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),                               b_r=tf.keras.regularizers.l2(1e-4),                              a_r=tf.keras.regularizers.l2(1e-4))         epoch_tests = []          #Create callback         epoch_test_callback = tf.keras.callbacks.LambdaCallback(             on_epoch_end=lambda epoch,logs: epoch_tests.append(model.evaluate(test_dataset, verbose=0)))          #Fit model with train dataset using 500 epochs         start = time.time()         history = model.fit(train_dataset, validation_data=val_dataset, callbacks=[epoch_test_callback], epochs=epochs, verbose=0)         end = time.time()          #Print time         hours, rem = divmod(end-start, 3600)         minutes, seconds = divmod(rem, 60)         print(\"Runtime of NN with {} optimizer: {:0&gt;2}:{:0&gt;2}:{:05.2f} (HH:MM:SS.MS)\".format(opt, int(hours),int(minutes),seconds))          #Split epoch tests into 2 lists         test_losses = [i[0] for i in epoch_tests]         test_accs = [i[1] for i in epoch_tests]          #Save results         results[opt] = {'train_loss': history.history['loss'], 'train_accuracy': history.history['accuracy'],                         'test_loss': test_losses, 'test_accuracy': test_accs}      #Save results     f = open(\"neural_network_reg_results.pkl\",\"wb\")     pickle.dump(results,f)     f.close() <p>The below function created four different plots - train loss, train accuracy, test loss, and test acurracy. After these four, a horizontal bar chart is shown displaying the maximum test accuracy recorded for each optimizer.</p> In\u00a0[28]: Copied! <pre>def create_nn_plots(step=1):\n    x = list(range(1, epochs+1, step))\n    x_ticks = list(range(0, epochs+1, 50))\n    \n    #Train loss\n    plt.figure(figsize=(10,8))\n    for k in results.keys():\n        #Get every nth item,  n=step\n        loss = results[k]['train_loss'][::step]\n        \n        #Plot\n        plt.plot(x, loss, label='{} loss'.format(k))\n        plt.scatter(x, loss)\n        \n        #Tweak plot\n        plt.xticks(x_ticks)\n        plt.ylim(0, 1)\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title(\"Train loss\")\n        plt.legend()\n    \n    \n    #Train accuracy\n    plt.figure(figsize=(10,8))\n    for k in results.keys():\n        #Get every nth item,  n=step\n        accuracy = [a*100 for a in results[k]['train_accuracy'][::step]]\n        \n        #Plot\n        plt.plot(x, accuracy, label='{} accuracy'.format(k))\n        plt.scatter(x, accuracy)\n        plt.xticks(x_ticks)\n        \n        #Tweak plot\n        plt.xticks(x_ticks)\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy (%)')    \n        plt.title(\"Train accuracy\")\n        plt.legend()\n\n        \n    #Test loss\n    plt.figure(figsize=(10,8))\n    for k in results.keys():\n        #Get every nth item,  n=step\n        loss = results[k]['test_loss'][::step]\n        \n        #Plot\n        plt.plot(x, loss, label='{} loss'.format(k))\n        plt.scatter(x, loss)\n        \n        #Tweak plot\n        plt.xticks(x_ticks)\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.ylim(0, 1)\n        plt.title(\"Test loss\")\n        plt.legend()\n\n        \n    #Test accuracy\n    plt.figure(figsize=(10,8))\n    for k in results.keys():\n        #Get every nth item,  n=step\n        accuracy = [a*100 for a in results[k]['test_accuracy'][::step]]\n        \n        #Plot\n        plt.plot(x, accuracy, label='{} accuracy'.format(k))\n        plt.scatter(x, accuracy)\n        \n        #Tweak plot\n        plt.xticks(x_ticks)\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy (%)')\n        plt.title(\"Test accuracy\")\n        plt.legend()\n\n        \n    #Get values for bar chart\n    labels = []\n    accs = []\n    for k in results.keys():\n        labels.append(k)\n        accs.append(max(results[k]['test_accuracy'])*100)\n\n    #Max test accuracy - shows max value from test dataset for each optimizer\n    y_pos = np.arange(len(accs))\n    fig, ax = plt.subplots(figsize=(10, 8))    \n    ax.barh(y_pos, accs, align='center')\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(labels)\n    ax.invert_yaxis()  # labels read top-to-bottom\n    ax.set_xlabel('Accuracy (%)')\n    ax.set_title('Max accuracy on test dataset vs optimizer for neural network')\n    for i, v in enumerate(accs):\n        ax.text(v-10, i, str(round(v,2)))\n</pre> def create_nn_plots(step=1):     x = list(range(1, epochs+1, step))     x_ticks = list(range(0, epochs+1, 50))          #Train loss     plt.figure(figsize=(10,8))     for k in results.keys():         #Get every nth item,  n=step         loss = results[k]['train_loss'][::step]                  #Plot         plt.plot(x, loss, label='{} loss'.format(k))         plt.scatter(x, loss)                  #Tweak plot         plt.xticks(x_ticks)         plt.ylim(0, 1)         plt.xlabel('Epoch')         plt.ylabel('Loss')         plt.title(\"Train loss\")         plt.legend()               #Train accuracy     plt.figure(figsize=(10,8))     for k in results.keys():         #Get every nth item,  n=step         accuracy = [a*100 for a in results[k]['train_accuracy'][::step]]                  #Plot         plt.plot(x, accuracy, label='{} accuracy'.format(k))         plt.scatter(x, accuracy)         plt.xticks(x_ticks)                  #Tweak plot         plt.xticks(x_ticks)         plt.xlabel('Epoch')         plt.ylabel('Accuracy (%)')             plt.title(\"Train accuracy\")         plt.legend()               #Test loss     plt.figure(figsize=(10,8))     for k in results.keys():         #Get every nth item,  n=step         loss = results[k]['test_loss'][::step]                  #Plot         plt.plot(x, loss, label='{} loss'.format(k))         plt.scatter(x, loss)                  #Tweak plot         plt.xticks(x_ticks)         plt.xlabel('Epoch')         plt.ylabel('Loss')         plt.ylim(0, 1)         plt.title(\"Test loss\")         plt.legend()               #Test accuracy     plt.figure(figsize=(10,8))     for k in results.keys():         #Get every nth item,  n=step         accuracy = [a*100 for a in results[k]['test_accuracy'][::step]]                  #Plot         plt.plot(x, accuracy, label='{} accuracy'.format(k))         plt.scatter(x, accuracy)                  #Tweak plot         plt.xticks(x_ticks)         plt.xlabel('Epoch')         plt.ylabel('Accuracy (%)')         plt.title(\"Test accuracy\")         plt.legend()               #Get values for bar chart     labels = []     accs = []     for k in results.keys():         labels.append(k)         accs.append(max(results[k]['test_accuracy'])*100)      #Max test accuracy - shows max value from test dataset for each optimizer     y_pos = np.arange(len(accs))     fig, ax = plt.subplots(figsize=(10, 8))         ax.barh(y_pos, accs, align='center')     ax.set_yticks(y_pos)     ax.set_yticklabels(labels)     ax.invert_yaxis()  # labels read top-to-bottom     ax.set_xlabel('Accuracy (%)')     ax.set_title('Max accuracy on test dataset vs optimizer for neural network')     for i, v in enumerate(accs):         ax.text(v-10, i, str(round(v,2))) In\u00a0[29]: Copied! <pre>#Show non-regularized results results\nresults = pickle.load(open(\"neural_network_results.pkl\",\"rb\" ))\ncreate_nn_plots(20)\n</pre> #Show non-regularized results results results = pickle.load(open(\"neural_network_results.pkl\",\"rb\" )) create_nn_plots(20) In\u00a0[30]: Copied! <pre>#Show regularized results results\nresults = pickle.load(open(\"neural_network_reg_results.pkl\",\"rb\" ))\ncreate_nn_plots(20)\n</pre> #Show regularized results results results = pickle.load(open(\"neural_network_reg_results.pkl\",\"rb\" )) create_nn_plots(20) <p>From the above graphs, we can see each optimizer behaves extremely differently - currently, we only see every 20th item from each dataset for sake of minimizing data points. From the bar chart at the bottom, we see most algorithms performing extremely well at &gt;=84% accuracy.</p> <p>An interesting note is how some optimizers approach 90% accuracy on the training set, perhaps a sign of overfitting - we see models in the regularized group that don't approach this 90% marker as much as the models from the non-regularized group.</p> In\u00a0[31]: Copied! <pre>model = create_model('adam')\nmodel.fit(train_dataset, validation_data=val_dataset, epochs=100, verbose=1)\n</pre> model = create_model('adam') model.fit(train_dataset, validation_data=val_dataset, epochs=100, verbose=1) <pre>Epoch 1/100\n458/458 [==============================] - 1s 1ms/step - loss: 5.5631 - accuracy: 0.7802 - val_loss: 1.3140 - val_accuracy: 0.7900\nEpoch 2/100\n458/458 [==============================] - 0s 986us/step - loss: 1.7393 - accuracy: 0.7937 - val_loss: 2.3510 - val_accuracy: 0.7986\nEpoch 3/100\n458/458 [==============================] - 0s 954us/step - loss: 1.5514 - accuracy: 0.8038 - val_loss: 1.7722 - val_accuracy: 0.8032\nEpoch 4/100\n458/458 [==============================] - 0s 958us/step - loss: 1.3410 - accuracy: 0.8119 - val_loss: 0.6998 - val_accuracy: 0.8216\nEpoch 5/100\n458/458 [==============================] - 0s 940us/step - loss: 1.2342 - accuracy: 0.8145 - val_loss: 0.4706 - val_accuracy: 0.8305\nEpoch 6/100\n458/458 [==============================] - 0s 950us/step - loss: 0.7319 - accuracy: 0.8176 - val_loss: 0.6941 - val_accuracy: 0.8133\nEpoch 7/100\n458/458 [==============================] - 0s 962us/step - loss: 0.8759 - accuracy: 0.8207 - val_loss: 0.5458 - val_accuracy: 0.8207\nEpoch 8/100\n458/458 [==============================] - 0s 926us/step - loss: 0.6617 - accuracy: 0.8215 - val_loss: 0.7905 - val_accuracy: 0.8099\nEpoch 9/100\n458/458 [==============================] - 0s 936us/step - loss: 0.6993 - accuracy: 0.8227 - val_loss: 2.7504 - val_accuracy: 0.8050\nEpoch 10/100\n458/458 [==============================] - 0s 950us/step - loss: 0.6498 - accuracy: 0.8216 - val_loss: 0.5760 - val_accuracy: 0.8207\nEpoch 11/100\n458/458 [==============================] - 0s 1ms/step - loss: 0.5804 - accuracy: 0.8246 - val_loss: 0.9961 - val_accuracy: 0.8020\nEpoch 12/100\n458/458 [==============================] - 0s 997us/step - loss: 0.4921 - accuracy: 0.8267 - val_loss: 0.3675 - val_accuracy: 0.8416\nEpoch 13/100\n458/458 [==============================] - 0s 963us/step - loss: 0.4408 - accuracy: 0.8276 - val_loss: 0.3480 - val_accuracy: 0.8382\nEpoch 14/100\n458/458 [==============================] - 0s 972us/step - loss: 0.3908 - accuracy: 0.8280 - val_loss: 0.3483 - val_accuracy: 0.8281\nEpoch 15/100\n458/458 [==============================] - 0s 981us/step - loss: 0.3965 - accuracy: 0.8293 - val_loss: 0.3489 - val_accuracy: 0.8397\nEpoch 16/100\n458/458 [==============================] - 0s 949us/step - loss: 0.3769 - accuracy: 0.8310 - val_loss: 0.3693 - val_accuracy: 0.8109\nEpoch 17/100\n458/458 [==============================] - 0s 931us/step - loss: 0.3619 - accuracy: 0.8294 - val_loss: 0.3642 - val_accuracy: 0.8176\nEpoch 18/100\n458/458 [==============================] - 0s 930us/step - loss: 0.3790 - accuracy: 0.8293 - val_loss: 0.3383 - val_accuracy: 0.8327\nEpoch 19/100\n458/458 [==============================] - 0s 932us/step - loss: 0.3566 - accuracy: 0.8316 - val_loss: 0.3476 - val_accuracy: 0.8453\nEpoch 20/100\n458/458 [==============================] - 0s 935us/step - loss: 0.3499 - accuracy: 0.8321 - val_loss: 0.3428 - val_accuracy: 0.8284\nEpoch 21/100\n458/458 [==============================] - 0s 925us/step - loss: 0.3473 - accuracy: 0.8347 - val_loss: 0.3369 - val_accuracy: 0.8345\nEpoch 22/100\n458/458 [==============================] - 0s 953us/step - loss: 0.3405 - accuracy: 0.8347 - val_loss: 0.3789 - val_accuracy: 0.8164\nEpoch 23/100\n458/458 [==============================] - 0s 931us/step - loss: 0.3411 - accuracy: 0.8342 - val_loss: 0.3436 - val_accuracy: 0.8345\nEpoch 24/100\n458/458 [==============================] - 0s 937us/step - loss: 0.3397 - accuracy: 0.8337 - val_loss: 0.3361 - val_accuracy: 0.8360\nEpoch 25/100\n458/458 [==============================] - 0s 929us/step - loss: 0.3456 - accuracy: 0.8336 - val_loss: 0.3461 - val_accuracy: 0.8253\nEpoch 26/100\n458/458 [==============================] - 0s 941us/step - loss: 0.3367 - accuracy: 0.8340 - val_loss: 0.3376 - val_accuracy: 0.8419\nEpoch 27/100\n458/458 [==============================] - 0s 934us/step - loss: 0.3360 - accuracy: 0.8352 - val_loss: 0.3421 - val_accuracy: 0.8321\nEpoch 28/100\n458/458 [==============================] - 0s 929us/step - loss: 0.3351 - accuracy: 0.8372 - val_loss: 0.3406 - val_accuracy: 0.8434\nEpoch 29/100\n458/458 [==============================] - 0s 923us/step - loss: 0.3354 - accuracy: 0.8354 - val_loss: 0.3380 - val_accuracy: 0.8416\nEpoch 30/100\n458/458 [==============================] - 0s 965us/step - loss: 0.3360 - accuracy: 0.8343 - val_loss: 0.3384 - val_accuracy: 0.8382\nEpoch 31/100\n458/458 [==============================] - 0s 1ms/step - loss: 0.3357 - accuracy: 0.8353 - val_loss: 0.3456 - val_accuracy: 0.8317\nEpoch 32/100\n458/458 [==============================] - 0s 978us/step - loss: 0.3383 - accuracy: 0.8356 - val_loss: 0.3535 - val_accuracy: 0.8241\nEpoch 33/100\n458/458 [==============================] - 0s 993us/step - loss: 0.3352 - accuracy: 0.8364 - val_loss: 0.3368 - val_accuracy: 0.8388\nEpoch 34/100\n458/458 [==============================] - 0s 969us/step - loss: 0.3323 - accuracy: 0.8382 - val_loss: 0.3341 - val_accuracy: 0.8247\nEpoch 35/100\n458/458 [==============================] - 0s 986us/step - loss: 0.3344 - accuracy: 0.8365 - val_loss: 0.3368 - val_accuracy: 0.8345\nEpoch 36/100\n458/458 [==============================] - 0s 938us/step - loss: 0.3332 - accuracy: 0.8380 - val_loss: 0.3420 - val_accuracy: 0.8302\nEpoch 37/100\n458/458 [==============================] - 0s 995us/step - loss: 0.3326 - accuracy: 0.8377 - val_loss: 0.3379 - val_accuracy: 0.8443\nEpoch 38/100\n458/458 [==============================] - 0s 954us/step - loss: 0.3339 - accuracy: 0.8366 - val_loss: 0.3569 - val_accuracy: 0.8370\nEpoch 39/100\n458/458 [==============================] - 0s 926us/step - loss: 0.3341 - accuracy: 0.8346 - val_loss: 0.3339 - val_accuracy: 0.8388\nEpoch 40/100\n458/458 [==============================] - 0s 983us/step - loss: 0.3318 - accuracy: 0.8384 - val_loss: 0.3355 - val_accuracy: 0.8314\nEpoch 41/100\n458/458 [==============================] - 0s 989us/step - loss: 0.3310 - accuracy: 0.8383 - val_loss: 0.3391 - val_accuracy: 0.8416\nEpoch 42/100\n458/458 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8391 - val_loss: 0.3402 - val_accuracy: 0.8281\nEpoch 43/100\n458/458 [==============================] - 0s 996us/step - loss: 0.3299 - accuracy: 0.8396 - val_loss: 0.3373 - val_accuracy: 0.8324\nEpoch 44/100\n458/458 [==============================] - 0s 999us/step - loss: 0.3296 - accuracy: 0.8402 - val_loss: 0.3400 - val_accuracy: 0.8281\nEpoch 45/100\n458/458 [==============================] - 0s 985us/step - loss: 0.3294 - accuracy: 0.8381 - val_loss: 0.3335 - val_accuracy: 0.8311\nEpoch 46/100\n458/458 [==============================] - 0s 1ms/step - loss: 0.3295 - accuracy: 0.8385 - val_loss: 0.3365 - val_accuracy: 0.8360\nEpoch 47/100\n458/458 [==============================] - 0s 966us/step - loss: 0.3275 - accuracy: 0.8401 - val_loss: 0.3336 - val_accuracy: 0.8382\nEpoch 48/100\n458/458 [==============================] - 0s 964us/step - loss: 0.3279 - accuracy: 0.8396 - val_loss: 0.3348 - val_accuracy: 0.8373\nEpoch 49/100\n458/458 [==============================] - 0s 981us/step - loss: 0.3288 - accuracy: 0.8397 - val_loss: 0.3346 - val_accuracy: 0.8400\nEpoch 50/100\n458/458 [==============================] - 0s 991us/step - loss: 0.3275 - accuracy: 0.8396 - val_loss: 0.3355 - val_accuracy: 0.8440\nEpoch 51/100\n458/458 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8383 - val_loss: 0.3350 - val_accuracy: 0.8407\nEpoch 52/100\n458/458 [==============================] - 0s 1ms/step - loss: 0.3269 - accuracy: 0.8404 - val_loss: 0.3338 - val_accuracy: 0.8314\nEpoch 53/100\n458/458 [==============================] - 0s 963us/step - loss: 0.3269 - accuracy: 0.8386 - val_loss: 0.3414 - val_accuracy: 0.8357\nEpoch 54/100\n458/458 [==============================] - 0s 1ms/step - loss: 0.3270 - accuracy: 0.8397 - val_loss: 0.3522 - val_accuracy: 0.8308\nEpoch 55/100\n458/458 [==============================] - 0s 1ms/step - loss: 0.3263 - accuracy: 0.8428 - val_loss: 0.3351 - val_accuracy: 0.8339\nEpoch 56/100\n458/458 [==============================] - 0s 999us/step - loss: 0.3255 - accuracy: 0.8416 - val_loss: 0.3360 - val_accuracy: 0.8400\nEpoch 57/100\n</pre> <pre>458/458 [==============================] - 0s 1ms/step - loss: 0.3250 - accuracy: 0.8400 - val_loss: 0.3394 - val_accuracy: 0.8410\nEpoch 58/100\n458/458 [==============================] - 0s 1ms/step - loss: 0.3260 - accuracy: 0.8399 - val_loss: 0.3398 - val_accuracy: 0.8425\nEpoch 59/100\n458/458 [==============================] - 0s 983us/step - loss: 0.3255 - accuracy: 0.8397 - val_loss: 0.3338 - val_accuracy: 0.8428\nEpoch 60/100\n458/458 [==============================] - 0s 1ms/step - loss: 0.3255 - accuracy: 0.8403 - val_loss: 0.3410 - val_accuracy: 0.8403\nEpoch 61/100\n458/458 [==============================] - 0s 970us/step - loss: 0.3240 - accuracy: 0.8419 - val_loss: 0.3345 - val_accuracy: 0.8422\nEpoch 62/100\n458/458 [==============================] - 0s 978us/step - loss: 0.3238 - accuracy: 0.8407 - val_loss: 0.3392 - val_accuracy: 0.8373\nEpoch 63/100\n458/458 [==============================] - 0s 969us/step - loss: 0.3225 - accuracy: 0.8428 - val_loss: 0.3363 - val_accuracy: 0.8431\nEpoch 64/100\n458/458 [==============================] - 0s 966us/step - loss: 0.3227 - accuracy: 0.8418 - val_loss: 0.3420 - val_accuracy: 0.8357\nEpoch 65/100\n458/458 [==============================] - 0s 951us/step - loss: 0.3224 - accuracy: 0.8421 - val_loss: 0.3344 - val_accuracy: 0.8462\nEpoch 66/100\n458/458 [==============================] - 0s 956us/step - loss: 0.3219 - accuracy: 0.8422 - val_loss: 0.3347 - val_accuracy: 0.8449\nEpoch 67/100\n458/458 [==============================] - 0s 954us/step - loss: 0.3222 - accuracy: 0.8436 - val_loss: 0.3330 - val_accuracy: 0.8437\nEpoch 68/100\n458/458 [==============================] - 0s 951us/step - loss: 0.3220 - accuracy: 0.8402 - val_loss: 0.3316 - val_accuracy: 0.8416\nEpoch 69/100\n458/458 [==============================] - 0s 956us/step - loss: 0.3209 - accuracy: 0.8426 - val_loss: 0.3389 - val_accuracy: 0.8321\nEpoch 70/100\n458/458 [==============================] - 0s 956us/step - loss: 0.3206 - accuracy: 0.8426 - val_loss: 0.3371 - val_accuracy: 0.8397\nEpoch 71/100\n458/458 [==============================] - 0s 955us/step - loss: 0.3200 - accuracy: 0.8423 - val_loss: 0.3353 - val_accuracy: 0.8449\nEpoch 72/100\n458/458 [==============================] - 0s 942us/step - loss: 0.3194 - accuracy: 0.8421 - val_loss: 0.3350 - val_accuracy: 0.8453\nEpoch 73/100\n458/458 [==============================] - 0s 956us/step - loss: 0.3228 - accuracy: 0.8406 - val_loss: 0.3409 - val_accuracy: 0.8354\nEpoch 74/100\n458/458 [==============================] - 0s 947us/step - loss: 0.3200 - accuracy: 0.8431 - val_loss: 0.3365 - val_accuracy: 0.8376\nEpoch 75/100\n458/458 [==============================] - 0s 936us/step - loss: 0.3199 - accuracy: 0.8428 - val_loss: 0.3439 - val_accuracy: 0.8382\nEpoch 76/100\n458/458 [==============================] - 0s 948us/step - loss: 0.3192 - accuracy: 0.8428 - val_loss: 0.3344 - val_accuracy: 0.8443\nEpoch 77/100\n458/458 [==============================] - 0s 939us/step - loss: 0.3195 - accuracy: 0.8428 - val_loss: 0.3372 - val_accuracy: 0.8489\nEpoch 78/100\n458/458 [==============================] - 0s 956us/step - loss: 0.3171 - accuracy: 0.8442 - val_loss: 0.3363 - val_accuracy: 0.8446\nEpoch 79/100\n458/458 [==============================] - 0s 949us/step - loss: 0.3185 - accuracy: 0.8414 - val_loss: 0.3348 - val_accuracy: 0.8465\nEpoch 80/100\n458/458 [==============================] - 0s 961us/step - loss: 0.3179 - accuracy: 0.8446 - val_loss: 0.3316 - val_accuracy: 0.8449\nEpoch 81/100\n458/458 [==============================] - 0s 966us/step - loss: 0.3237 - accuracy: 0.8428 - val_loss: 0.3329 - val_accuracy: 0.8468\nEpoch 82/100\n458/458 [==============================] - 0s 963us/step - loss: 0.3156 - accuracy: 0.8436 - val_loss: 0.3366 - val_accuracy: 0.8410\nEpoch 83/100\n458/458 [==============================] - 0s 963us/step - loss: 0.3161 - accuracy: 0.8442 - val_loss: 0.3366 - val_accuracy: 0.8403\nEpoch 84/100\n458/458 [==============================] - 0s 960us/step - loss: 0.3249 - accuracy: 0.8433 - val_loss: 0.3417 - val_accuracy: 0.8422\nEpoch 85/100\n458/458 [==============================] - 0s 965us/step - loss: 0.3186 - accuracy: 0.8428 - val_loss: 0.3359 - val_accuracy: 0.8348\nEpoch 86/100\n458/458 [==============================] - 0s 955us/step - loss: 0.3152 - accuracy: 0.8429 - val_loss: 0.3368 - val_accuracy: 0.8443\nEpoch 87/100\n458/458 [==============================] - 0s 957us/step - loss: 0.3159 - accuracy: 0.8408 - val_loss: 0.3335 - val_accuracy: 0.8446\nEpoch 88/100\n458/458 [==============================] - 0s 959us/step - loss: 0.3139 - accuracy: 0.8433 - val_loss: 0.3337 - val_accuracy: 0.8462\nEpoch 89/100\n458/458 [==============================] - 0s 956us/step - loss: 0.3133 - accuracy: 0.8446 - val_loss: 0.3398 - val_accuracy: 0.8382\nEpoch 90/100\n458/458 [==============================] - 0s 936us/step - loss: 0.3139 - accuracy: 0.8445 - val_loss: 0.3413 - val_accuracy: 0.8367\nEpoch 91/100\n458/458 [==============================] - 0s 970us/step - loss: 0.3135 - accuracy: 0.8428 - val_loss: 0.3329 - val_accuracy: 0.8456\nEpoch 92/100\n458/458 [==============================] - 0s 960us/step - loss: 0.3118 - accuracy: 0.8452 - val_loss: 0.3388 - val_accuracy: 0.8394\nEpoch 93/100\n458/458 [==============================] - 0s 954us/step - loss: 0.3125 - accuracy: 0.8445 - val_loss: 0.3370 - val_accuracy: 0.8373\nEpoch 94/100\n458/458 [==============================] - 0s 948us/step - loss: 0.3127 - accuracy: 0.8456 - val_loss: 0.3416 - val_accuracy: 0.8434\nEpoch 95/100\n458/458 [==============================] - 0s 944us/step - loss: 0.3124 - accuracy: 0.8459 - val_loss: 0.3364 - val_accuracy: 0.8410\nEpoch 96/100\n458/458 [==============================] - 0s 960us/step - loss: 0.3110 - accuracy: 0.8457 - val_loss: 0.3333 - val_accuracy: 0.8471\nEpoch 97/100\n458/458 [==============================] - 0s 950us/step - loss: 0.3091 - accuracy: 0.8452 - val_loss: 0.3391 - val_accuracy: 0.8443\nEpoch 98/100\n458/458 [==============================] - 0s 950us/step - loss: 0.3101 - accuracy: 0.8442 - val_loss: 0.3366 - val_accuracy: 0.8471\nEpoch 99/100\n458/458 [==============================] - 0s 950us/step - loss: 0.3102 - accuracy: 0.8454 - val_loss: 0.3367 - val_accuracy: 0.8443\nEpoch 100/100\n458/458 [==============================] - 0s 959us/step - loss: 0.3096 - accuracy: 0.8459 - val_loss: 0.3375 - val_accuracy: 0.8486\n</pre> Out[31]: <pre>&lt;tensorflow.python.keras.callbacks.History at 0x16d5cf7ed30&gt;</pre> In\u00a0[32]: Copied! <pre>model.evaluate(test_dataset)\n</pre> model.evaluate(test_dataset) <pre>1/1 [==============================] - 0s 1ms/step - loss: 0.3327 - accuracy: 0.8454\n</pre> Out[32]: <pre>[0.3326649069786072, 0.8454025983810425]</pre> In\u00a0[33]: Copied! <pre>#Get numbers for AUROC curve\npreds = model.predict(test_csv.to_numpy()).ravel()\nnn_fpr, nn_tpr, _ = roc_curve(test_y, preds)\nnn_roc_auc = auc(nn_fpr, nn_tpr)\n</pre> #Get numbers for AUROC curve preds = model.predict(test_csv.to_numpy()).ravel() nn_fpr, nn_tpr, _ = roc_curve(test_y, preds) nn_roc_auc = auc(nn_fpr, nn_tpr) In\u00a0[34]: Copied! <pre>#Creating confusion matrix for proportion\npreds = np.where(preds &lt; 0.5, 0, 1)\ncreate_confusion_matrix(preds, test_y)\n</pre> #Creating confusion matrix for proportion preds = np.where(preds &lt; 0.5, 0, 1) create_confusion_matrix(preds, test_y) <pre>[[11931   504]\n [ 2013  1833]]\n</pre> Out[34]: <pre>&lt;AxesSubplot:&gt;</pre> <p>==========================================================</p> In\u00a0[35]: Copied! <pre>#Load\ntrain_csv, test_csv = load_datasets()\n\n#Prune\ntrain_csv = prune(train_csv)\ntest_csv = prune(test_csv)\n\n#Convert object columns to discrete numerical values\ntrain_csv = convert_to_discrete(train_csv)\ntest_csv = convert_to_discrete(test_csv)\n</pre> #Load train_csv, test_csv = load_datasets()  #Prune train_csv = prune(train_csv) test_csv = prune(test_csv)  #Convert object columns to discrete numerical values train_csv = convert_to_discrete(train_csv) test_csv = convert_to_discrete(test_csv) In\u00a0[36]: Copied! <pre>from sklearn.naive_bayes import GaussianNB\n\n#Sort into train/test sets\ntrain_y = train_csv.pop('class').to_numpy()\ntrain_x = train_csv.to_numpy()\ntest_y = test_csv.pop('class').to_numpy()\ntest_x = test_csv.to_numpy()\n\n#Create Gaussian Naive Bayues, fit, get the y-scores, and show ROC curve\ngnb = GaussianNB()\nstart = time.time()\ngnb.fit(train_x, train_y)\nend = time.time()\n\nhours, rem = divmod(end-start, 3600)\nminutes, seconds = divmod(rem, 60)\nprint(\"Runtime of NB: {:0&gt;2}:{:0&gt;2}:{:05.2f} (HH:MM:SS.MS)\".format(int(hours),int(minutes),seconds))\n</pre> from sklearn.naive_bayes import GaussianNB  #Sort into train/test sets train_y = train_csv.pop('class').to_numpy() train_x = train_csv.to_numpy() test_y = test_csv.pop('class').to_numpy() test_x = test_csv.to_numpy()  #Create Gaussian Naive Bayues, fit, get the y-scores, and show ROC curve gnb = GaussianNB() start = time.time() gnb.fit(train_x, train_y) end = time.time()  hours, rem = divmod(end-start, 3600) minutes, seconds = divmod(rem, 60) print(\"Runtime of NB: {:0&gt;2}:{:0&gt;2}:{:05.2f} (HH:MM:SS.MS)\".format(int(hours),int(minutes),seconds)) <pre>Runtime of NB: 00:00:00.01 (HH:MM:SS.MS)\n</pre> In\u00a0[37]: Copied! <pre>preds = gnb.predict(test_x)\nprint('Accuracy: {0:.{1}f}'.format((preds == test_y).sum() / len(test_y), 4))\n</pre> preds = gnb.predict(test_x) print('Accuracy: {0:.{1}f}'.format((preds == test_y).sum() / len(test_y), 4)) <pre>Accuracy: 0.7988\n</pre> In\u00a0[38]: Copied! <pre>#Get ROC curve variables\nnb_score = gnb.predict_proba(test_x)\nnb_fpr, nb_tpr, _ = roc_curve(test_y, nb_score[:, 1])\nnb_roc_auc = auc(nb_fpr, nb_tpr)\n</pre> #Get ROC curve variables nb_score = gnb.predict_proba(test_x) nb_fpr, nb_tpr, _ = roc_curve(test_y, nb_score[:, 1]) nb_roc_auc = auc(nb_fpr, nb_tpr) In\u00a0[39]: Copied! <pre>create_confusion_matrix(preds, test_y)\n</pre> create_confusion_matrix(preds, test_y) <pre>[[11825   610]\n [ 2665  1181]]\n</pre> Out[39]: <pre>&lt;AxesSubplot:&gt;</pre> <p>==========================================================</p> In\u00a0[40]: Copied! <pre>#Load and format data\ntrain_csv, test_csv = load_datasets()\n\n#Prune\ntrain_csv = prune(train_csv)\ntest_csv = prune(test_csv)\n\n#Convert object columns to discrete numerical values\ntrain_csv = convert_to_discrete(train_csv)\ntest_csv = convert_to_discrete(test_csv)\n</pre> #Load and format data train_csv, test_csv = load_datasets()  #Prune train_csv = prune(train_csv) test_csv = prune(test_csv)  #Convert object columns to discrete numerical values train_csv = convert_to_discrete(train_csv) test_csv = convert_to_discrete(test_csv) In\u00a0[41]: Copied! <pre>from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n\n#Get random sample of 20000\ntrain_csv = train_csv.sample(n=20000)\ntrain_y = train_csv.pop('class').to_list()\ntrain_x = train_csv.values.tolist()\n\ntest_y = test_csv.pop('class').to_list()\ntest_x = test_csv.values.tolist()\n\n#Fit SVM\nstart = time.time()\nsvm = SVC(kernel='poly', probability=True)\nsvm.fit(train_x,train_y)\nend = time.time()\n\nhours, rem = divmod(end-start, 3600)\nminutes, seconds = divmod(rem, 60)\nprint(\"Runtime of SVM: {:0&gt;2}:{:0&gt;2}:{:05.2f} (HH:MM:SS.MS)\".format(int(hours),int(minutes),seconds))\n</pre> from sklearn.svm import SVC from sklearn.metrics import accuracy_score   #Get random sample of 20000 train_csv = train_csv.sample(n=20000) train_y = train_csv.pop('class').to_list() train_x = train_csv.values.tolist()  test_y = test_csv.pop('class').to_list() test_x = test_csv.values.tolist()  #Fit SVM start = time.time() svm = SVC(kernel='poly', probability=True) svm.fit(train_x,train_y) end = time.time()  hours, rem = divmod(end-start, 3600) minutes, seconds = divmod(rem, 60) print(\"Runtime of SVM: {:0&gt;2}:{:0&gt;2}:{:05.2f} (HH:MM:SS.MS)\".format(int(hours),int(minutes),seconds)) <pre>Runtime of SVM: 00:00:15.76 (HH:MM:SS.MS)\n</pre> In\u00a0[42]: Copied! <pre>#Show accuracy\npreds = svm.predict(test_x)\nprint('Accuracy: {0:.{1}f}'.format(accuracy_score(test_y, preds), 4))\n</pre> #Show accuracy preds = svm.predict(test_x) print('Accuracy: {0:.{1}f}'.format(accuracy_score(test_y, preds), 4)) <pre>Accuracy: 0.7850\n</pre> In\u00a0[43]: Copied! <pre>#Create AUROC\nsvm_score = svm.predict_proba(test_x)\nsvm_fpr, svm_tpr, _ = roc_curve(test_y, svm_score[:, 1])\nsvm_roc_auc = auc(svm_fpr, svm_tpr)\n</pre> #Create AUROC svm_score = svm.predict_proba(test_x) svm_fpr, svm_tpr, _ = roc_curve(test_y, svm_score[:, 1]) svm_roc_auc = auc(svm_fpr, svm_tpr) In\u00a0[44]: Copied! <pre>#Show confusion matrix\ncreate_confusion_matrix(preds, test_y)\n</pre> #Show confusion matrix create_confusion_matrix(preds, test_y) <pre>[[12432     3]\n [ 3497   349]]\n</pre> Out[44]: <pre>&lt;AxesSubplot:&gt;</pre> <p>==========================================================</p> In\u00a0[45]: Copied! <pre>def fit_and_score(name, model):\n    #Fit model\n    start = time.time()\n    model.fit(train_x, train_y)\n    end = time.time()\n    \n    #Print time\n    hours, rem = divmod(end-start, 3600)\n    minutes, seconds = divmod(rem, 60)\n    print(\"Runtime of {}: {:0&gt;2}:{:0&gt;2}:{:05.2f} (HH:MM:SS.MS)\".format(name, int(hours),int(minutes),seconds))\n    \n    #Get accuracy\n    preds = model.predict(test_x)\n    acc = accuracy_score(test_y, preds)\n    print('Accuracy of {}: {:.{}f}'.format(name, acc, 4))\n    \n    #Get numbers for auroc curve\n    score = model.predict_proba(test_x)\n    fpr, tpr, _ = roc_curve(test_y, score[:, 1])\n    roc_auc = auc(fpr, tpr)\n\n    return acc, fpr, tpr, roc_auc\n    \n#Load and format data\ntrain_csv, test_csv = load_datasets()\n\n#Prune\ntrain_csv = prune(train_csv)\ntest_csv = prune(test_csv)\n\n#Convert object columns to discrete numerical values\ntrain_csv = convert_to_discrete(train_csv)\ntest_csv = convert_to_discrete(test_csv)\n</pre> def fit_and_score(name, model):     #Fit model     start = time.time()     model.fit(train_x, train_y)     end = time.time()          #Print time     hours, rem = divmod(end-start, 3600)     minutes, seconds = divmod(rem, 60)     print(\"Runtime of {}: {:0&gt;2}:{:0&gt;2}:{:05.2f} (HH:MM:SS.MS)\".format(name, int(hours),int(minutes),seconds))          #Get accuracy     preds = model.predict(test_x)     acc = accuracy_score(test_y, preds)     print('Accuracy of {}: {:.{}f}'.format(name, acc, 4))          #Get numbers for auroc curve     score = model.predict_proba(test_x)     fpr, tpr, _ = roc_curve(test_y, score[:, 1])     roc_auc = auc(fpr, tpr)      return acc, fpr, tpr, roc_auc      #Load and format data train_csv, test_csv = load_datasets()  #Prune train_csv = prune(train_csv) test_csv = prune(test_csv)  #Convert object columns to discrete numerical values train_csv = convert_to_discrete(train_csv) test_csv = convert_to_discrete(test_csv) In\u00a0[46]: Copied! <pre>from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting \nfrom sklearn.ensemble import HistGradientBoostingClassifier\n</pre> from sklearn.ensemble import AdaBoostClassifier from sklearn.ensemble import BaggingClassifier from sklearn.ensemble import ExtraTreesClassifier from sklearn.ensemble import GradientBoostingClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.experimental import enable_hist_gradient_boosting  from sklearn.ensemble import HistGradientBoostingClassifier In\u00a0[47]: Copied! <pre>#Create models\nadaBoost = AdaBoostClassifier(n_estimators=1000)\nbagging = BaggingClassifier(n_estimators=1000, n_jobs=-1, random_state=0)\nextraTrees = ExtraTreesClassifier(n_estimators=1000, n_jobs=-1, criterion='entropy', random_state=0)\ngradBoost = GradientBoostingClassifier(n_estimators=150, random_state=0)\nrandForest = RandomForestClassifier(criterion=\"entropy\", n_estimators=150, n_jobs=-1, random_state=0)\nhistGrad = HistGradientBoostingClassifier(random_state=0)\n\n#Add to list for convenience\nmodels = []\nmodels.append(['AdaBoost', adaBoost])\nmodels.append(['Bagging', bagging])\nmodels.append(['ExtraTrees', extraTrees])\nmodels.append(['GradientBoosting', gradBoost])\nmodels.append(['RandomForest', randForest])\nmodels.append(['HistogramGradientBoosting', histGrad])\n</pre> #Create models adaBoost = AdaBoostClassifier(n_estimators=1000) bagging = BaggingClassifier(n_estimators=1000, n_jobs=-1, random_state=0) extraTrees = ExtraTreesClassifier(n_estimators=1000, n_jobs=-1, criterion='entropy', random_state=0) gradBoost = GradientBoostingClassifier(n_estimators=150, random_state=0) randForest = RandomForestClassifier(criterion=\"entropy\", n_estimators=150, n_jobs=-1, random_state=0) histGrad = HistGradientBoostingClassifier(random_state=0)  #Add to list for convenience models = [] models.append(['AdaBoost', adaBoost]) models.append(['Bagging', bagging]) models.append(['ExtraTrees', extraTrees]) models.append(['GradientBoosting', gradBoost]) models.append(['RandomForest', randForest]) models.append(['HistogramGradientBoosting', histGrad]) In\u00a0[48]: Copied! <pre>#Loop through each model, display accuracy and save for bar chart\ndata = []\nfor n, m in models:\n    acc, fpr, tpr, roc_auc = fit_and_score(n, m)\n    data.append([n, acc, fpr, tpr, roc_auc])\n    print()\n</pre> #Loop through each model, display accuracy and save for bar chart data = [] for n, m in models:     acc, fpr, tpr, roc_auc = fit_and_score(n, m)     data.append([n, acc, fpr, tpr, roc_auc])     print() <pre>Runtime of AdaBoost: 00:00:06.85 (HH:MM:SS.MS)\nAccuracy of AdaBoost: 0.8695\n\nRuntime of Bagging: 00:00:03.82 (HH:MM:SS.MS)\nAccuracy of Bagging: 0.8441\n\nRuntime of ExtraTrees: 00:00:02.28 (HH:MM:SS.MS)\nAccuracy of ExtraTrees: 0.8390\n\nRuntime of GradientBoosting: 00:00:01.65 (HH:MM:SS.MS)\nAccuracy of GradientBoosting: 0.8689\n\nRuntime of RandomForest: 00:00:00.45 (HH:MM:SS.MS)\nAccuracy of RandomForest: 0.8501\n\nRuntime of HistogramGradientBoosting: 00:00:00.80 (HH:MM:SS.MS)\nAccuracy of HistogramGradientBoosting: 0.8715\n\n</pre> In\u00a0[49]: Copied! <pre>#Get values for bar chart\nlabels = [i[0] for i in data]\naccs = [i[1] for i in data]\n\n#Show accuracy for each model\ny_pos = np.arange(len(accs))\nfig, ax = plt.subplots(figsize=(10, 8))    \nax.barh(y_pos, accs, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(labels)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Accuracy (%)')\nax.set_title('Varying ensemble methods accuracy')\nfor i, v in enumerate(accs):\n    ax.text(v-0.07, i, str(round(v,4)))\n</pre> #Get values for bar chart labels = [i[0] for i in data] accs = [i[1] for i in data]  #Show accuracy for each model y_pos = np.arange(len(accs)) fig, ax = plt.subplots(figsize=(10, 8))     ax.barh(y_pos, accs, align='center') ax.set_yticks(y_pos) ax.set_yticklabels(labels) ax.invert_yaxis()  # labels read top-to-bottom ax.set_xlabel('Accuracy (%)') ax.set_title('Varying ensemble methods accuracy') for i, v in enumerate(accs):     ax.text(v-0.07, i, str(round(v,4))) In\u00a0[50]: Copied! <pre>#Plot ROC curves\nplt.figure(figsize=(10,8))\nlw = 2\n\nfor i in data:\n    plt.plot(i[2], i[3], lw=lw, label='AUROC ({}) (area = {})'.format(i[0], round(i[4], 2)))\n\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()\n</pre> #Plot ROC curves plt.figure(figsize=(10,8)) lw = 2  for i in data:     plt.plot(i[2], i[3], lw=lw, label='AUROC ({}) (area = {})'.format(i[0], round(i[4], 2)))  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--') plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('ROC') plt.legend(loc=\"lower right\") plt.show() <p>From the above bar chart and ROC curves, we see that there are several ensemble classifiers that perform better than the random forest - namely, AdaBoost, GradientBoosting, and HistogramGradientBoosting. An interesting note is that the RandomForest classifier uses bagging, whereas the three that outperformed all use boosting. Additionally, almost all of the ensembles scored better on the ROC chart than the neural network did, which is shown below</p> In\u00a0[51]: Copied! <pre>#Load and format data\ntrain_csv, test_csv = load_datasets()\n\n#Prune\ntrain_csv = prune(train_csv)\ntest_csv = prune(test_csv)\n\n#Convert object columns to discrete numerical values\ntrain_csv = convert_to_discrete(train_csv)\ntest_csv = convert_to_discrete(test_csv)\n</pre> #Load and format data train_csv, test_csv = load_datasets()  #Prune train_csv = prune(train_csv) test_csv = prune(test_csv)  #Convert object columns to discrete numerical values train_csv = convert_to_discrete(train_csv) test_csv = convert_to_discrete(test_csv) In\u00a0[52]: Copied! <pre>train_y = train_csv.pop('class').to_list()\ntrain_x = train_csv.values.tolist()\n\ntest_y = test_csv.pop('class').to_list()\ntest_x = test_csv.values.tolist()\n\nrf = RandomForestClassifier(criterion=\"entropy\", random_state=0)\n\nstart = time.time()\nrf.fit(train_x, train_y)\nend = time.time()\n\nhours, rem = divmod(end-start, 3600)\nminutes, seconds = divmod(rem, 60)\nprint(\"Runtime of RF: {:0&gt;2}:{:0&gt;2}:{:05.2f} (HH:MM:SS.MS)\".format(int(hours),int(minutes),seconds))\n</pre> train_y = train_csv.pop('class').to_list() train_x = train_csv.values.tolist()  test_y = test_csv.pop('class').to_list() test_x = test_csv.values.tolist()  rf = RandomForestClassifier(criterion=\"entropy\", random_state=0)  start = time.time() rf.fit(train_x, train_y) end = time.time()  hours, rem = divmod(end-start, 3600) minutes, seconds = divmod(rem, 60) print(\"Runtime of RF: {:0&gt;2}:{:0&gt;2}:{:05.2f} (HH:MM:SS.MS)\".format(int(hours),int(minutes),seconds)) <pre>Runtime of RF: 00:00:02.40 (HH:MM:SS.MS)\n</pre> In\u00a0[53]: Copied! <pre>preds = rf.predict(test_x)\nprint('Accuracy: {0:.{1}f}'.format(accuracy_score(test_y, preds), 4))\n</pre> preds = rf.predict(test_x) print('Accuracy: {0:.{1}f}'.format(accuracy_score(test_y, preds), 4)) <pre>Accuracy: 0.8517\n</pre> In\u00a0[54]: Copied! <pre>#Create AUROC\nrf_score = rf.predict_proba(test_x)\nrf_fpr, rf_tpr, _ = roc_curve(test_y, rf_score[:, 1])\nrf_roc_auc = auc(rf_fpr, rf_tpr)\n</pre> #Create AUROC rf_score = rf.predict_proba(test_x) rf_fpr, rf_tpr, _ = roc_curve(test_y, rf_score[:, 1]) rf_roc_auc = auc(rf_fpr, rf_tpr) In\u00a0[55]: Copied! <pre>#Show confusion matrix\ncreate_confusion_matrix(preds, test_y)\n</pre> #Show confusion matrix create_confusion_matrix(preds, test_y) <pre>[[11540   895]\n [ 1520  2326]]\n</pre> Out[55]: <pre>&lt;AxesSubplot:&gt;</pre> In\u00a0[56]: Copied! <pre>plt.figure(figsize=(10,8))\nlw = 2\n\n#Plot neural network\nplt.plot(nn_fpr, nn_tpr, color='r',\n         lw=lw, label='AUROC (NN) (area = %0.2f)' % nn_roc_auc)\n#Plot naive bayes\nplt.plot(nb_fpr, nb_tpr, color='g',\n         lw=lw, label='AUROC (NB) (area = %0.2f)' % nb_roc_auc)\n#Plot SVM\nplt.plot(svm_fpr, svm_tpr, color='b',\n         lw=lw, label='AUROC (SVM) (area = %0.2f)' % svm_roc_auc)\n#Plot random forest\nplt.plot(rf_fpr, rf_tpr, color='y',\n         lw=lw, label='AUROC (RF) (area = %0.2f)' % rf_roc_auc)\n\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()\n</pre> plt.figure(figsize=(10,8)) lw = 2  #Plot neural network plt.plot(nn_fpr, nn_tpr, color='r',          lw=lw, label='AUROC (NN) (area = %0.2f)' % nn_roc_auc) #Plot naive bayes plt.plot(nb_fpr, nb_tpr, color='g',          lw=lw, label='AUROC (NB) (area = %0.2f)' % nb_roc_auc) #Plot SVM plt.plot(svm_fpr, svm_tpr, color='b',          lw=lw, label='AUROC (SVM) (area = %0.2f)' % svm_roc_auc) #Plot random forest plt.plot(rf_fpr, rf_tpr, color='y',          lw=lw, label='AUROC (RF) (area = %0.2f)' % rf_roc_auc)  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--') plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('ROC') plt.legend(loc=\"lower right\") plt.show() <p>Above, we can see how each algorithm did, with the random forest having the greatest area under the ROC curve, followed by the neural network, the naive-bayes algorithm, and lastly with the SVM.</p> <ul> <li>I anticipated the NN and random forest doing extremely well, as finding the common segments and patterns here would be a fitting task for both algorithms.</li> <li>I am surprised by the naive bayes a bit - later, I will discuss how these features relate, and naive bayes assumes feature independence.</li> <li>It's not too surprising to see the SVM not doing as well, given the time constraint - again, more time would have likely shown a better fit with the linear kernel.</li> </ul> <p>==========================================================</p> <p>One way to segment would be to try and group people based on analysis of the above attributes</p> <ul> <li>Age - Seems to be a trend present, middle ages have higher percentages of people making &gt;50k</li> <li>Workclass - Certain classes, such as self-empl-inc, have higher percentages of people making &gt;50k</li> <li>Education - Higher education correlates to higher percentages of people making &gt;50k</li> <li>Marital Status - Married people have higher percentages of people making &gt;50k compared to data</li> <li>Occupation - Some jobs, such as managers/executives, have higher percentages of people making &gt;50k</li> <li>Relationship - Married people have higher percentages of people making &gt;50k</li> <li>Race - Certain races have lower percentages of people making &gt;50k</li> <li>Sex - Men tend to have higher percentages of people making &gt;50k</li> <li>Capital Gain - Certain splits, such as 7,000-20,000, higher percentages of people making &gt;50k</li> <li>Native Country - Some countries have higher or lower percentages of people making &gt;50k</li> </ul> <p>Looking further at these features, several of these are related: Age, Workclass, Education, Marital Status, Occupation, Relationship, and possibly Capital Gain.</p> <ul> <li>Generally, it takes a person many years to finish a Masters or Professional Schoool, and even longer to finish a Doctoral degree (linked to higher percentages)</li> <li>Additionally, people don't often start at the manager/executive level - this takes several years</li> <li>While many younger people do get married, it is often something that takes time</li> </ul> <p>Because of this, a person's age is a factor in their occupation, education, marital status, and most of what affects their income.</p> <p>Given this information, we could segment certain demographics, such as married men, or executives. Let's look at a few segments from the above information.</p> In\u00a0[57]: Copied! <pre>#Load data\ntrain_csv, test_csv = load_datasets()\n\n#Convert class to numerical\ntrain_csv['class'] = pd.Categorical(train_csv['class'])\ntrain_csv['class'] = train_csv['class'].cat.codes\nds = pd.concat([train_csv, test_csv], axis=0)\nsegments = []\n\n#First segment - married men who are executives\ntemp = ds.loc[(ds['relationship'] == ' Husband') &amp; (ds['occupation'] == ' Exec-managerial')]\nlte50k = len(temp.loc[temp['class'] == 0].index)\ngt50k = len(temp.loc[temp['class'] == 1].index)\nsegments.append([lte50k, gt50k])\n\n#Second segment - married women who are executives\ntemp = ds.loc[(ds['relationship'] == ' Wife') &amp; (ds['occupation'] == ' Exec-managerial')]\nlte50k = len(temp.loc[temp['class'] == 0].index)\ngt50k = len(temp.loc[temp['class'] == 1].index)\nsegments.append([lte50k, gt50k])\n\n#Third segment - non-white/pacific islanders who have little education\nlower_educations = [' 12th', ' 10th', ' 5th-6th', ' 1st-4th', ' Preschool' ' 7th-8th', ' 9th', ' 11th']\ntemp = ds.loc[(ds['race'] != ' White') &amp; (ds['race'] != ' Asian-Pac-Islander') &amp; (ds['education'].isin(lower_educations))]                                                                         \nlte50k = len(temp.loc[temp['class'] == 0].index)\ngt50k = len(temp.loc[temp['class'] == 1].index)\nsegments.append([lte50k, gt50k])\n\n#Fourth segment - married people who have at least a bachelors degree\nhigher_educations =[' Prof-school', 'Doctorate', ' Masters', ' Bachelors']\ntemp = ds.loc[((ds['relationship'] == ' Husband') | (ds['relationship'] == ' Wife')) &amp; (ds['education'].isin(higher_educations))]                                                                       \nlte50k = len(temp.loc[temp['class'] == 0].index)\ngt50k = len(temp.loc[temp['class'] == 1].index)\nsegments.append([lte50k, gt50k])\n\n#Fifth segment - self-empl-inc who work at least 40 hours per week\ntemp = ds.loc[(ds['workclass'] == ' Self-emp-inc') &amp; (ds['hours-per-week'] &gt;= 40)]\nlte50k = len(temp.loc[temp['class'] == 0].index)\ngt50k = len(temp.loc[temp['class'] == 1].index)\nsegments.append([lte50k, gt50k])\n\n#Create bargraph\ny0 = [i[0] for i in segments]\ny1 = [i[1] for i in segments]\nx = np.arange(len(y0))\nwidth = 0.5\nfig = plt.figure(1, figsize=(12, 10))\nax = fig.add_axes([0,0,1,1])\nax.bar(x, y0, width, color='g', label='&lt;=50k')\nax.bar(x, y1, width,bottom=y0, color='b', label='&gt;50k')\nx = ['0: Married men who are executives', \n'1: Married women who are executives',\n'2: Non-White/Pacific Islanders who have less-than high-school diploma for education',\n'3: Married people who have at least a bachelor\\'s degree',\n'4: Self employed (inc) people who work at least 40 hours a week']\n\nprops = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\nax.text(0.05, 0.95, '\\n'.join(x), transform=ax.transAxes, fontsize=11.5,\n        verticalalignment='top',horizontalalignment='left', bbox=props)\nax.legend()\nplt.show()\n\n#Create table\ny0_p = []\ny1_p  = []\nfor i in range(len(y0)):\n    y0_p.append((y0[i]/(y0[i]+y1[i]))*100)\n    y1_p.append((y1[i]/(y0[i]+y1[i]))*100)\n\nx = np.arange(len(x))\ntable_data = []\nfor i in range(len(y0)):\n    table_data.append([x[i], y0_p[i], y1_p[i]])\n\ncols = ['', '% &lt;=50k', '% &gt;50k']\ng = plt.figure(2, figsize=(12, 10))\nax = g.add_subplot(1,1,1)\ntable = ax.table(cellText=table_data, colLabels = cols, loc='center')\ntable.set_fontsize(14)\ntable.scale(1,2)\nax.axis('off')\nplt.show()\n</pre> #Load data train_csv, test_csv = load_datasets()  #Convert class to numerical train_csv['class'] = pd.Categorical(train_csv['class']) train_csv['class'] = train_csv['class'].cat.codes ds = pd.concat([train_csv, test_csv], axis=0) segments = []  #First segment - married men who are executives temp = ds.loc[(ds['relationship'] == ' Husband') &amp; (ds['occupation'] == ' Exec-managerial')] lte50k = len(temp.loc[temp['class'] == 0].index) gt50k = len(temp.loc[temp['class'] == 1].index) segments.append([lte50k, gt50k])  #Second segment - married women who are executives temp = ds.loc[(ds['relationship'] == ' Wife') &amp; (ds['occupation'] == ' Exec-managerial')] lte50k = len(temp.loc[temp['class'] == 0].index) gt50k = len(temp.loc[temp['class'] == 1].index) segments.append([lte50k, gt50k])  #Third segment - non-white/pacific islanders who have little education lower_educations = [' 12th', ' 10th', ' 5th-6th', ' 1st-4th', ' Preschool' ' 7th-8th', ' 9th', ' 11th'] temp = ds.loc[(ds['race'] != ' White') &amp; (ds['race'] != ' Asian-Pac-Islander') &amp; (ds['education'].isin(lower_educations))]                                                                          lte50k = len(temp.loc[temp['class'] == 0].index) gt50k = len(temp.loc[temp['class'] == 1].index) segments.append([lte50k, gt50k])  #Fourth segment - married people who have at least a bachelors degree higher_educations =[' Prof-school', 'Doctorate', ' Masters', ' Bachelors'] temp = ds.loc[((ds['relationship'] == ' Husband') | (ds['relationship'] == ' Wife')) &amp; (ds['education'].isin(higher_educations))]                                                                        lte50k = len(temp.loc[temp['class'] == 0].index) gt50k = len(temp.loc[temp['class'] == 1].index) segments.append([lte50k, gt50k])  #Fifth segment - self-empl-inc who work at least 40 hours per week temp = ds.loc[(ds['workclass'] == ' Self-emp-inc') &amp; (ds['hours-per-week'] &gt;= 40)] lte50k = len(temp.loc[temp['class'] == 0].index) gt50k = len(temp.loc[temp['class'] == 1].index) segments.append([lte50k, gt50k])  #Create bargraph y0 = [i[0] for i in segments] y1 = [i[1] for i in segments] x = np.arange(len(y0)) width = 0.5 fig = plt.figure(1, figsize=(12, 10)) ax = fig.add_axes([0,0,1,1]) ax.bar(x, y0, width, color='g', label='&lt;=50k') ax.bar(x, y1, width,bottom=y0, color='b', label='&gt;50k') x = ['0: Married men who are executives',  '1: Married women who are executives', '2: Non-White/Pacific Islanders who have less-than high-school diploma for education', '3: Married people who have at least a bachelor\\'s degree', '4: Self employed (inc) people who work at least 40 hours a week']  props = dict(boxstyle='round', facecolor='wheat', alpha=0.5) ax.text(0.05, 0.95, '\\n'.join(x), transform=ax.transAxes, fontsize=11.5,         verticalalignment='top',horizontalalignment='left', bbox=props) ax.legend() plt.show()  #Create table y0_p = [] y1_p  = [] for i in range(len(y0)):     y0_p.append((y0[i]/(y0[i]+y1[i]))*100)     y1_p.append((y1[i]/(y0[i]+y1[i]))*100)  x = np.arange(len(x)) table_data = [] for i in range(len(y0)):     table_data.append([x[i], y0_p[i], y1_p[i]])  cols = ['', '% &lt;=50k', '% &gt;50k'] g = plt.figure(2, figsize=(12, 10)) ax = g.add_subplot(1,1,1) table = ax.table(cellText=table_data, colLabels = cols, loc='center') table.set_fontsize(14) table.scale(1,2) ax.axis('off') plt.show() <p>Above, we see several segments that could be made, both signficantly greater percent in the &gt;50k class, and one for significantly less. There are more segments that could be made by hand that will give both above and below average percentages, but I will be doing something different.</p> <p>One last approach that I will showcase for segmenting - we could instead use a brute force method, analyzing certain datasets that perform better in our training set. A problem with a brute force method is trying to analyze all features - unfortunately, if we did that, there are 8,628,360,700,000,000 possible categories in the set of cartesian products for all features.</p> <p>If we remove certain features and focus only on a few, we get a more manageable number of categories - 6,350,400, to be exact.</p> <p>Features to remove:</p> <ul> <li>Ages - as discussed above, age is reflected in many other features, which will be on this list</li> <li>Capital Gain/Capital Loss - too many items to include for this example</li> <li>Hours per week - too many items to include for this example</li> </ul> <p>Below, we will save notable results - those that have &gt;=15 matching cases, into a text file for future use. Additionally, we will only store results with either &lt;=20% or &gt;=30% of people in the &gt;50k salary category.</p> In\u00a0[58]: Copied! <pre>train_csv, test_csv = load_datasets()\ntrain_csv.pop('education-num')\ntest_csv.pop('education-num')\nfeat_atts = defaultdict(list)\n\n\n#Workclasses\nfeat_atts['workclass'] = train_csv['workclass'].unique()\n\n#Education\nfeat_atts['education'] = train_csv['education'].unique()\n\n#Marital-status\nfeat_atts['marital-status'] = train_csv['marital-status'].unique()\n\n#Occupation\nfeat_atts['occupation'] = train_csv['occupation'].unique()\n\n#Race\nfeat_atts['race'] = train_csv['race'].unique()\n\n#Sex\nfeat_atts['sex'] = train_csv['sex'].unique()\n\n\n#Native country\nfeat_atts['native-country'] = train_csv['native-country'].unique()\n</pre> train_csv, test_csv = load_datasets() train_csv.pop('education-num') test_csv.pop('education-num') feat_atts = defaultdict(list)   #Workclasses feat_atts['workclass'] = train_csv['workclass'].unique()  #Education feat_atts['education'] = train_csv['education'].unique()  #Marital-status feat_atts['marital-status'] = train_csv['marital-status'].unique()  #Occupation feat_atts['occupation'] = train_csv['occupation'].unique()  #Race feat_atts['race'] = train_csv['race'].unique()  #Sex feat_atts['sex'] = train_csv['sex'].unique()   #Native country feat_atts['native-country'] = train_csv['native-country'].unique()  In\u00a0[59]: Copied! <pre>import itertools\nfrom collections import Counter\nfrom random import shuffle\n\n#Create cartesian product list of all possible combinations for each sublist\ncart_prod = list(itertools.product(*list(feat_atts.values())))\n#Shuffle (used for testing purposes, no impact on production)\nshuffle(cart_prod)\n#Create defaultdict which defaults to counter\ncart_prod_dict = defaultdict(Counter)\n#Columns used in brute force method\ncols_of_importance = ['workclass', 'education', 'marital-status', 'occupation', 'race', 'sex', 'native-country']\n</pre> import itertools from collections import Counter from random import shuffle  #Create cartesian product list of all possible combinations for each sublist cart_prod = list(itertools.product(*list(feat_atts.values()))) #Shuffle (used for testing purposes, no impact on production) shuffle(cart_prod) #Create defaultdict which defaults to counter cart_prod_dict = defaultdict(Counter) #Columns used in brute force method cols_of_importance = ['workclass', 'education', 'marital-status', 'occupation', 'race', 'sex', 'native-country'] In\u00a0[60]: Copied! <pre>#Convert train_csv to 0's and 1's\ntrain_csv['class'] = pd.Categorical(train_csv['class'])\ntrain_csv['class'] = train_csv['class'].cat.codes\n\ndef check_combinations(comb_set):\n    #Count which combination currently checking\n    for count, comb in enumerate(comb_set):\n        #Create temporary copy of train_csv dataset\n        temp = train_csv.copy(True)\n        #Loop through every column specified, and narrow down the temp dataset based on current combination and specified column\n        for i, col in enumerate(cols_of_importance):\n            temp = temp.loc[temp[col] == comb[i]]\n        \n        #Count rows with class &lt;=50k and &gt;50k\n        lte50k = len(temp.loc[temp['class'] == 0].index)\n        gt50k = len(temp.loc[temp['class'] == 1].index)\n        \n        #Add counts to counter at specified combination\n        cart_prod_dict[comb][0] += lte50k\n        cart_prod_dict[comb][1] += gt50k\n\n#This takes over 11 hours, set this boolean to true to re-store data\nrun_this_cell = False\nif run_this_cell:\n    start = time.time()\n    check_combinations(cart_prod)\n    end = time.time()\n\n    with open('brute_force_results.data', 'w') as f:\n        #End timer and display results in HH:MM:SS.MS\n        hours, rem = divmod(end-start, 3600)\n        minutes, seconds = divmod(rem, 60)\n        f.write(\"Runtime of brute force method: {:0&gt;2}:{:0&gt;2}:{:05.2f} (HH:MM:SS.MS)\\n\\n\".format(int(hours),int(minutes),seconds))\n\n        #Go through each result, store if significant (&gt;= 15 cases)\n        notable_results = {}\n        for k, v in cart_prod_dict.items():\n            if v[0] + v[1] &gt;= 15:\n                perc = v[1] / (v[0] + v[1])\n                if perc &lt;= 0.2 or perc &gt;= 0.3:\n                    notable_results[k] = [v[0], v[1], (v[1] / (v[0] + v[1])) * 100]\n                    \n        #Sort from highest to lowest percent\n        notable_results = sorted(notable_results.items(), key=lambda x: x[1][2], reverse=True)\n\n        #Save results of significance to file for future use\n        f.write('Results sorted by descending % &gt;50k\\nKeywords are from the below columns, in order:\\nWorkclass, Education, Marital-status, Occupation, Race, Sex, Country-of-origin')\n        f.write('='*75)\n        f.write('\\n')\n\n        for i in notable_results:\n            #print(\"{}\\n&lt;=50k: {}, &gt;50k: {}, % &gt;50k: {:.2f}%\\n\".format(i[0], i[1][0], i[1][1], i[1][2]))\n            f.write(\"{}\\n&lt;=50k: {}, &gt;50k: {}, % &gt;50k: {:.2f}%\\n\".format(i[0], i[1][0], i[1][1], i[1][2]))\n</pre> #Convert train_csv to 0's and 1's train_csv['class'] = pd.Categorical(train_csv['class']) train_csv['class'] = train_csv['class'].cat.codes  def check_combinations(comb_set):     #Count which combination currently checking     for count, comb in enumerate(comb_set):         #Create temporary copy of train_csv dataset         temp = train_csv.copy(True)         #Loop through every column specified, and narrow down the temp dataset based on current combination and specified column         for i, col in enumerate(cols_of_importance):             temp = temp.loc[temp[col] == comb[i]]                  #Count rows with class &lt;=50k and &gt;50k         lte50k = len(temp.loc[temp['class'] == 0].index)         gt50k = len(temp.loc[temp['class'] == 1].index)                  #Add counts to counter at specified combination         cart_prod_dict[comb][0] += lte50k         cart_prod_dict[comb][1] += gt50k  #This takes over 11 hours, set this boolean to true to re-store data run_this_cell = False if run_this_cell:     start = time.time()     check_combinations(cart_prod)     end = time.time()      with open('brute_force_results.data', 'w') as f:         #End timer and display results in HH:MM:SS.MS         hours, rem = divmod(end-start, 3600)         minutes, seconds = divmod(rem, 60)         f.write(\"Runtime of brute force method: {:0&gt;2}:{:0&gt;2}:{:05.2f} (HH:MM:SS.MS)\\n\\n\".format(int(hours),int(minutes),seconds))          #Go through each result, store if significant (&gt;= 15 cases)         notable_results = {}         for k, v in cart_prod_dict.items():             if v[0] + v[1] &gt;= 15:                 perc = v[1] / (v[0] + v[1])                 if perc &lt;= 0.2 or perc &gt;= 0.3:                     notable_results[k] = [v[0], v[1], (v[1] / (v[0] + v[1])) * 100]                              #Sort from highest to lowest percent         notable_results = sorted(notable_results.items(), key=lambda x: x[1][2], reverse=True)          #Save results of significance to file for future use         f.write('Results sorted by descending % &gt;50k\\nKeywords are from the below columns, in order:\\nWorkclass, Education, Marital-status, Occupation, Race, Sex, Country-of-origin')         f.write('='*75)         f.write('\\n')          for i in notable_results:             #print(\"{}\\n&lt;=50k: {}, &gt;50k: {}, % &gt;50k: {:.2f}%\\n\".format(i[0], i[1][0], i[1][1], i[1][2]))             f.write(\"{}\\n&lt;=50k: {}, &gt;50k: {}, % &gt;50k: {:.2f}%\\n\".format(i[0], i[1][0], i[1][1], i[1][2]))   In\u00a0[61]: Copied! <pre>data = defaultdict(str)\n#Load data\nwith open('brute_force_results.data', 'r') as f:\n    for x, line in enumerate(f):\n        #First 8 lines are just header lines\n        if x &gt; 7:\n            #Data works in groups of 3:\n            #Line 0: the data\n            #Line 1: newline\n            #Line 2: the category, or the key\n            if line == '\\n':\n                continue\n            if x % 3 == 2:\n                key = line.strip(' \\t\\n\\r')\n            elif x % 3 == 0:\n                data[key] = line\n        else:\n            print(line)\n\n#Convert key'd data from str to lists\nsorted_data = {}               \nfor k in data.keys():\n    #Load in data, replace classes as 0's and 1's\n    temp = data[k].replace('\\n', '').replace('&lt;=50k', '0').replace('&gt;50k', '1').replace('% 1', '%')[:-1].split(', ')\n    #Create dictionary with keys 0, 1, and %\n    temp_dict = {0: int(temp[0][temp[0].find(' ')+1:]),\n                1: int(temp[1][temp[1].find(' ')+1:]),\n                '%': float(temp[2][temp[2].find(' ')+1:])}\n    sorted_data[k] = temp_dict\n\n#Sort data by % &gt;50k\nsorted_data = sorted(sorted_data.items(), key=lambda x: x[1]['%'], reverse=True)\n\nfor i in sorted_data:\n    print(i[0], '\\n', i[1], '\\n')\n</pre> data = defaultdict(str) #Load data with open('brute_force_results.data', 'r') as f:     for x, line in enumerate(f):         #First 8 lines are just header lines         if x &gt; 7:             #Data works in groups of 3:             #Line 0: the data             #Line 1: newline             #Line 2: the category, or the key             if line == '\\n':                 continue             if x % 3 == 2:                 key = line.strip(' \\t\\n\\r')             elif x % 3 == 0:                 data[key] = line         else:             print(line)  #Convert key'd data from str to lists sorted_data = {}                for k in data.keys():     #Load in data, replace classes as 0's and 1's     temp = data[k].replace('\\n', '').replace('&lt;=50k', '0').replace('&gt;50k', '1').replace('% 1', '%')[:-1].split(', ')     #Create dictionary with keys 0, 1, and %     temp_dict = {0: int(temp[0][temp[0].find(' ')+1:]),                 1: int(temp[1][temp[1].find(' ')+1:]),                 '%': float(temp[2][temp[2].find(' ')+1:])}     sorted_data[k] = temp_dict  #Sort data by % &gt;50k sorted_data = sorted(sorted_data.items(), key=lambda x: x[1]['%'], reverse=True)  for i in sorted_data:     print(i[0], '\\n', i[1], '\\n')   <pre>Runtime of brute force method: 12:43:33.08 (HH:MM:SS.MS)\n\n\n\nResults sorted by descending % &gt;50k\n\nKeywords are from the below columns, in order:\n\nWorkclass, Education, Marital-status, Occupation, Race, Sex, Country-of-origin\n\n===========================================================================\n\n\n\n\n\n(' Self-emp-inc', ' Prof-school', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 1, 1: 56, '%': 98.25} \n\n(' Private', ' Doctorate', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 1, 1: 15, '%': 93.75} \n\n(' Federal-gov', ' Some-college', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 1, 1: 14, '%': 93.33} \n\n(' Private', ' Masters', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 12, 1: 159, '%': 92.98} \n\n(' Local-gov', ' Masters', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 3, 1: 24, '%': 88.89} \n\n(' State-gov', ' Doctorate', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 4, 1: 31, '%': 88.57} \n\n(' Private', ' Bachelors', ' Married-civ-spouse', ' Exec-managerial', ' Black', ' Male', ' United-States') \n {0: 2, 1: 15, '%': 88.24} \n\n(' Private', ' Doctorate', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 8, 1: 58, '%': 87.88} \n\n(' Self-emp-inc', ' Doctorate', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 2, 1: 14, '%': 87.5} \n\n(' Self-emp-not-inc', ' Prof-school', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 10, 1: 70, '%': 87.5} \n\n(' Private', ' Prof-school', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 12, 1: 83, '%': 87.37} \n\n(' Private', ' Masters', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 3, 1: 19, '%': 86.36} \n\n(' Self-emp-inc', ' Masters', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 3, 1: 18, '%': 85.71} \n\n(' Self-emp-inc', ' Masters', ' Married-civ-spouse', ' Sales', ' White', ' Male', ' United-States') \n {0: 3, 1: 16, '%': 84.21} \n\n(' Local-gov', ' Masters', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 6, 1: 30, '%': 83.33} \n\n(' Self-emp-inc', ' Bachelors', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 14, 1: 69, '%': 83.13} \n\n(' Private', ' Bachelors', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 82, 1: 370, '%': 81.86} \n\n(' Federal-gov', ' Bachelors', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 5, 1: 21, '%': 80.77} \n\n(' Private', ' Masters', ' Divorced', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 3, 1: 12, '%': 80.0} \n\n(' Private', ' Masters', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 30, 1: 116, '%': 79.45} \n\n(' Private', ' Assoc-acdm', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 8, 1: 29, '%': 78.38} \n\n(' Private', ' Bachelors', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Female', ' United-States') \n {0: 9, 1: 32, '%': 78.05} \n\n(' Self-emp-inc', ' Bachelors', ' Married-civ-spouse', ' Sales', ' White', ' Male', ' United-States') \n {0: 12, 1: 42, '%': 77.78} \n\n(' Self-emp-inc', ' Some-college', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 15, 1: 50, '%': 76.92} \n\n(' State-gov', ' Bachelors', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 8, 1: 26, '%': 76.47} \n\n(' State-gov', ' Masters', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 4, 1: 13, '%': 76.47} \n\n(' Self-emp-not-inc', ' Masters', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 4, 1: 13, '%': 76.47} \n\n(' Private', ' Masters', ' Married-civ-spouse', ' Sales', ' White', ' Male', ' United-States') \n {0: 9, 1: 29, '%': 76.32} \n\n(' Federal-gov', ' Bachelors', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 6, 1: 19, '%': 76.0} \n\n(' State-gov', ' Masters', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 10, 1: 30, '%': 75.0} \n\n(' Local-gov', ' Bachelors', ' Married-civ-spouse', ' Protective-serv', ' White', ' Male', ' United-States') \n {0: 7, 1: 20, '%': 74.07} \n\n(' Private', ' Prof-school', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 5, 1: 14, '%': 73.68} \n\n(' Local-gov', ' Masters', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 24, 1: 64, '%': 72.73} \n\n(' Private', ' Bachelors', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 13, 1: 34, '%': 72.34} \n\n(' Private', ' Bachelors', ' Married-civ-spouse', ' Sales', ' White', ' Male', ' United-States') \n {0: 77, 1: 199, '%': 72.1} \n\n(' Local-gov', ' Bachelors', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 11, 1: 28, '%': 71.79} \n\n(' Private', ' Bachelors', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 89, 1: 223, '%': 71.47} \n\n(' Private', ' Bachelors', ' Married-civ-spouse', ' Tech-support', ' White', ' Male', ' United-States') \n {0: 19, 1: 45, '%': 70.31} \n\n(' Private', ' Assoc-voc', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 14, 1: 29, '%': 67.44} \n\n(' Private', ' Assoc-voc', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 5, 1: 10, '%': 66.67} \n\n(' Self-emp-inc', ' HS-grad', ' Married-civ-spouse', ' Sales', ' White', ' Male', ' United-States') \n {0: 12, 1: 24, '%': 66.67} \n\n(' Federal-gov', ' Some-college', ' Married-civ-spouse', ' Adm-clerical', ' White', ' Male', ' United-States') \n {0: 8, 1: 16, '%': 66.67} \n\n(' Self-emp-inc', ' Bachelors', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 6, 1: 12, '%': 66.67} \n\n(' Self-emp-not-inc', ' Doctorate', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 7, 1: 14, '%': 66.67} \n\n(' ?', ' Bachelors', ' Married-civ-spouse', ' ?', ' White', ' Female', ' United-States') \n {0: 6, 1: 11, '%': 64.71} \n\n(' Private', ' Assoc-voc', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 12, 1: 22, '%': 64.71} \n\n(' Self-emp-inc', ' Some-college', ' Married-civ-spouse', ' Sales', ' White', ' Male', ' United-States') \n {0: 17, 1: 31, '%': 64.58} \n\n(' Private', ' Bachelors', ' Married-civ-spouse', ' Adm-clerical', ' White', ' Male', ' United-States') \n {0: 21, 1: 38, '%': 64.41} \n\n(' Local-gov', ' Some-college', ' Married-civ-spouse', ' Protective-serv', ' White', ' Male', ' United-States') \n {0: 22, 1: 38, '%': 63.33} \n\n(' Private', ' Assoc-voc', ' Married-civ-spouse', ' Tech-support', ' White', ' Male', ' United-States') \n {0: 12, 1: 20, '%': 62.5} \n\n(' Private', ' Bachelors', ' Married-civ-spouse', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 32, 1: 50, '%': 60.98} \n\n(' Private', ' Assoc-acdm', ' Married-civ-spouse', ' Tech-support', ' White', ' Male', ' United-States') \n {0: 9, 1: 14, '%': 60.87} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 6, 1: 9, '%': 60.0} \n\n(' Self-emp-not-inc', ' Bachelors', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 21, 1: 31, '%': 59.62} \n\n(' Local-gov', ' Bachelors', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 7, 1: 10, '%': 58.82} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 92, 1: 129, '%': 58.37} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Female', ' United-States') \n {0: 13, 1: 18, '%': 58.06} \n\n(' State-gov', ' Bachelors', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 13, 1: 18, '%': 58.06} \n\n(' Self-emp-inc', ' HS-grad', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 29, 1: 40, '%': 57.97} \n\n(' Federal-gov', ' HS-grad', ' Married-civ-spouse', ' Adm-clerical', ' White', ' Male', ' United-States') \n {0: 16, 1: 22, '%': 57.89} \n\n(' Private', ' Bachelors', ' Married-civ-spouse', ' Machine-op-inspct', ' White', ' Male', ' United-States') \n {0: 9, 1: 12, '%': 57.14} \n\n(' Self-emp-not-inc', ' Bachelors', ' Married-civ-spouse', ' Sales', ' White', ' Male', ' United-States') \n {0: 23, 1: 30, '%': 56.6} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 26, 1: 33, '%': 55.93} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Tech-support', ' White', ' Male', ' United-States') \n {0: 30, 1: 38, '%': 55.88} \n\n(' Private', ' Masters', ' Divorced', ' Exec-managerial', ' White', ' Female', ' United-States') \n {0: 8, 1: 10, '%': 55.56} \n\n(' Private', ' HS-grad', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 88, 1: 109, '%': 55.33} \n\n(' Self-emp-not-inc', ' Bachelors', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 19, 1: 23, '%': 54.76} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 47, 1: 56, '%': 54.37} \n\n(' Private', ' Assoc-acdm', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 13, 1: 15, '%': 53.57} \n\n(' Local-gov', ' Bachelors', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 35, 1: 40, '%': 53.33} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Craft-repair', ' Black', ' Male', ' United-States') \n {0: 7, 1: 8, '%': 53.33} \n\n(' Private', ' Assoc-acdm', ' Married-civ-spouse', ' Adm-clerical', ' White', ' Male', ' United-States') \n {0: 7, 1: 8, '%': 53.33} \n\n(' Private', ' HS-grad', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Female', ' United-States') \n {0: 16, 1: 18, '%': 52.94} \n\n(' Private', ' Bachelors', ' Divorced', ' Sales', ' White', ' Male', ' United-States') \n {0: 13, 1: 14, '%': 51.85} \n\n(' Private', ' Assoc-acdm', ' Married-civ-spouse', ' Sales', ' White', ' Male', ' United-States') \n {0: 18, 1: 19, '%': 51.35} \n\n(' Private', ' HS-grad', ' Married-civ-spouse', ' Tech-support', ' White', ' Male', ' United-States') \n {0: 19, 1: 20, '%': 51.28} \n\n(' Local-gov', ' HS-grad', ' Married-civ-spouse', ' Protective-serv', ' White', ' Male', ' United-States') \n {0: 23, 1: 24, '%': 51.06} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Sales', ' White', ' Male', ' United-States') \n {0: 119, 1: 119, '%': 50.0} \n\n(' Private', ' Bachelors', ' Divorced', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 21, 1: 20, '%': 48.78} \n\n(' Local-gov', ' Some-college', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 11, 1: 10, '%': 47.62} \n\n(' Private', ' Bachelors', ' Married-civ-spouse', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 10, 1: 9, '%': 47.37} \n\n(' Private', ' HS-grad', ' Married-civ-spouse', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 29, 1: 26, '%': 47.27} \n\n(' Self-emp-inc', ' HS-grad', ' Married-civ-spouse', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 18, 1: 16, '%': 47.06} \n\n(' ?', ' Masters', ' Married-civ-spouse', ' ?', ' White', ' Male', ' United-States') \n {0: 13, 1: 11, '%': 45.83} \n\n(' Private', ' Masters', ' Never-married', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 12, 1: 10, '%': 45.45} \n\n(' Private', ' Bachelors', ' Married-civ-spouse', ' Transport-moving', ' White', ' Male', ' United-States') \n {0: 14, 1: 11, '%': 44.0} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Transport-moving', ' White', ' Male', ' United-States') \n {0: 65, 1: 51, '%': 43.97} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 195, 1: 151, '%': 43.64} \n\n(' Private', ' HS-grad', ' Married-civ-spouse', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 59, 1: 45, '%': 43.27} \n\n(' Local-gov', ' HS-grad', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 12, 1: 9, '%': 42.86} \n\n(' Self-emp-not-inc', ' Some-college', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 27, 1: 20, '%': 42.55} \n\n(' Private', ' Assoc-voc', ' Married-civ-spouse', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 65, 1: 48, '%': 42.48} \n\n(' Private', ' Assoc-acdm', ' Married-civ-spouse', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 24, 1: 17, '%': 41.46} \n\n(' ?', ' Bachelors', ' Married-civ-spouse', ' ?', ' White', ' Male', ' United-States') \n {0: 38, 1: 25, '%': 39.68} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Sales', ' White', ' Female', ' United-States') \n {0: 13, 1: 8, '%': 38.1} \n\n(' Private', ' Prof-school', ' Never-married', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 13, 1: 8, '%': 38.1} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Farming-fishing', ' White', ' Male', ' United-States') \n {0: 13, 1: 8, '%': 38.1} \n\n(' Private', ' Assoc-voc', ' Married-civ-spouse', ' Machine-op-inspct', ' White', ' Male', ' United-States') \n {0: 18, 1: 11, '%': 37.93} \n\n(' Private', ' HS-grad', ' Married-civ-spouse', ' Sales', ' White', ' Male', ' United-States') \n {0: 172, 1: 105, '%': 37.91} \n\n(' Private', ' Doctorate', ' Never-married', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 10, 1: 6, '%': 37.5} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Adm-clerical', ' White', ' Male', ' United-States') \n {0: 52, 1: 31, '%': 37.35} \n\n(' Local-gov', ' Some-college', ' Married-civ-spouse', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 12, 1: 7, '%': 36.84} \n\n(' Private', ' Assoc-voc', ' Married-civ-spouse', ' Sales', ' White', ' Male', ' United-States') \n {0: 24, 1: 14, '%': 36.84} \n\n(' Private', ' Bachelors', ' Divorced', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 11, 1: 6, '%': 35.29} \n\n(' State-gov', ' HS-grad', ' Married-civ-spouse', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 13, 1: 7, '%': 35.0} \n\n(' Private', ' Assoc-voc', ' Married-civ-spouse', ' Transport-moving', ' White', ' Male', ' United-States') \n {0: 13, 1: 7, '%': 35.0} \n\n(' Private', ' Bachelors', ' Divorced', ' Exec-managerial', ' White', ' Female', ' United-States') \n {0: 28, 1: 15, '%': 34.88} \n\n(' Self-emp-not-inc', ' Bachelors', ' Married-civ-spouse', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 15, 1: 8, '%': 34.78} \n\n(' Private', ' Prof-school', ' Never-married', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 10, 1: 5, '%': 33.33} \n\n(' Private', ' Some-college', ' Divorced', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 10, 1: 5, '%': 33.33} \n\n(' Private', ' Bachelors', ' Married-civ-spouse', ' Other-service', ' White', ' Male', ' United-States') \n {0: 12, 1: 6, '%': 33.33} \n\n(' State-gov', ' Some-college', ' Married-civ-spouse', ' Protective-serv', ' White', ' Male', ' United-States') \n {0: 12, 1: 6, '%': 33.33} \n\n(' Private', ' Masters', ' Never-married', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 28, 1: 14, '%': 33.33} \n\n(' Private', ' HS-grad', ' Married-civ-spouse', ' Transport-moving', ' White', ' Male', ' United-States') \n {0: 224, 1: 111, '%': 33.13} \n\n(' Self-emp-not-inc', ' HS-grad', ' Married-civ-spouse', ' Sales', ' White', ' Male', ' United-States') \n {0: 47, 1: 23, '%': 32.86} \n\n(' Private', ' HS-grad', ' Married-civ-spouse', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 552, 1: 255, '%': 31.6} \n\n(' ?', ' Some-college', ' Married-civ-spouse', ' ?', ' White', ' Female', ' United-States') \n {0: 18, 1: 8, '%': 30.77} \n\n(' Private', ' Masters', ' Never-married', ' Exec-managerial', ' White', ' Female', ' United-States') \n {0: 18, 1: 8, '%': 30.77} \n\n(' Local-gov', ' HS-grad', ' Married-civ-spouse', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 34, 1: 15, '%': 30.61} \n\n(' Self-emp-not-inc', ' HS-grad', ' Married-civ-spouse', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 53, 1: 23, '%': 30.26} \n\n(' ?', ' Some-college', ' Married-civ-spouse', ' ?', ' White', ' Male', ' United-States') \n {0: 42, 1: 18, '%': 30.0} \n\n(' Local-gov', ' Masters', ' Never-married', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 16, 1: 4, '%': 20.0} \n\n(' Private', ' 11th', ' Married-civ-spouse', ' Sales', ' White', ' Male', ' United-States') \n {0: 12, 1: 3, '%': 20.0} \n\n(' Private', ' HS-grad', ' Married-civ-spouse', ' Sales', ' White', ' Female', ' United-States') \n {0: 33, 1: 8, '%': 19.51} \n\n(' Self-emp-not-inc', ' Assoc-voc', ' Married-civ-spouse', ' Farming-fishing', ' White', ' Male', ' United-States') \n {0: 21, 1: 5, '%': 19.23} \n\n(' Private', ' Assoc-voc', ' Divorced', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 21, 1: 5, '%': 19.23} \n\n(' Private', ' Bachelors', ' Never-married', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 21, 1: 5, '%': 19.23} \n\n(' ?', ' HS-grad', ' Married-civ-spouse', ' ?', ' White', ' Male', ' United-States') \n {0: 122, 1: 29, '%': 19.21} \n\n(' Private', ' HS-grad', ' Married-civ-spouse', ' Other-service', ' Black', ' Male', ' United-States') \n {0: 17, 1: 4, '%': 19.05} \n\n(' Private', ' Some-college', ' Divorced', ' Transport-moving', ' White', ' Male', ' United-States') \n {0: 13, 1: 3, '%': 18.75} \n\n(' Private', ' Assoc-voc', ' Married-civ-spouse', ' Other-service', ' White', ' Male', ' United-States') \n {0: 13, 1: 3, '%': 18.75} \n\n(' Private', ' Some-college', ' Divorced', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 48, 1: 11, '%': 18.64} \n\n(' Private', ' Bachelors', ' Never-married', ' Sales', ' White', ' Male', ' United-States') \n {0: 98, 1: 22, '%': 18.33} \n\n(' Private', ' Masters', ' Never-married', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 48, 1: 10, '%': 17.24} \n\n(' State-gov', ' HS-grad', ' Married-civ-spouse', ' Protective-serv', ' White', ' Male', ' United-States') \n {0: 24, 1: 5, '%': 17.24} \n\n(' Local-gov', ' Masters', ' Divorced', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 35, 1: 7, '%': 16.67} \n\n(' Self-emp-not-inc', ' HS-grad', ' Divorced', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 20, 1: 4, '%': 16.67} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Other-service', ' White', ' Female', ' United-States') \n {0: 15, 1: 3, '%': 16.67} \n\n(' Local-gov', ' HS-grad', ' Married-civ-spouse', ' Transport-moving', ' White', ' Male', ' United-States') \n {0: 25, 1: 5, '%': 16.67} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Other-service', ' White', ' Male', ' United-States') \n {0: 36, 1: 7, '%': 16.28} \n\n(' Private', ' Bachelors', ' Never-married', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 109, 1: 21, '%': 16.15} \n\n(' Private', ' Some-college', ' Divorced', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 21, 1: 4, '%': 16.0} \n\n(' Private', ' 11th', ' Married-civ-spouse', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 59, 1: 11, '%': 15.71} \n\n(' Private', ' HS-grad', ' Married-civ-spouse', ' Transport-moving', ' Black', ' Male', ' United-States') \n {0: 22, 1: 4, '%': 15.38} \n\n(' Private', ' HS-grad', ' Married-civ-spouse', ' Handlers-cleaners', ' Black', ' Male', ' United-States') \n {0: 18, 1: 3, '%': 14.29} \n\n(' Private', ' Some-college', ' Married-civ-spouse', ' Handlers-cleaners', ' White', ' Male', ' United-States') \n {0: 49, 1: 8, '%': 14.04} \n\n(' Private', ' 10th', ' Married-civ-spouse', ' Machine-op-inspct', ' White', ' Male', ' United-States') \n {0: 31, 1: 5, '%': 13.89} \n\n(' Private', ' Some-college', ' Never-married', ' Craft-repair', ' White', ' Female', ' United-States') \n {0: 13, 1: 2, '%': 13.33} \n\n(' Private', ' 7th-8th', ' Married-civ-spouse', ' Transport-moving', ' White', ' Male', ' United-States') \n {0: 20, 1: 3, '%': 13.04} \n\n(' Self-emp-not-inc', ' 7th-8th', ' Married-civ-spouse', ' Farming-fishing', ' White', ' Male', ' United-States') \n {0: 20, 1: 3, '%': 13.04} \n\n(' Private', ' HS-grad', ' Widowed', ' Sales', ' White', ' Female', ' United-States') \n {0: 29, 1: 4, '%': 12.12} \n\n(' Private', ' Bachelors', ' Never-married', ' Exec-managerial', ' White', ' Female', ' United-States') \n {0: 102, 1: 14, '%': 12.07} \n\n(' Private', ' HS-grad', ' Never-married', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 15, 1: 2, '%': 11.76} \n\n(' Private', ' HS-grad', ' Married-civ-spouse', ' Other-service', ' White', ' Male', ' United-States') \n {0: 83, 1: 11, '%': 11.7} \n\n(' Private', ' HS-grad', ' Married-civ-spouse', ' Farming-fishing', ' White', ' Male', ' United-States') \n {0: 53, 1: 7, '%': 11.67} \n\n(' Self-emp-not-inc', ' Some-college', ' Married-civ-spouse', ' Farming-fishing', ' White', ' Male', ' United-States') \n {0: 46, 1: 6, '%': 11.54} \n\n(' Private', ' HS-grad', ' Divorced', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 16, 1: 2, '%': 11.11} \n\n(' Local-gov', ' Masters', ' Never-married', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 33, 1: 4, '%': 10.81} \n\n(' Private', ' HS-grad', ' Divorced', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 25, 1: 3, '%': 10.71} \n\n(' Private', ' Some-college', ' Never-married', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 34, 1: 4, '%': 10.53} \n\n(' Private', ' Bachelors', ' Never-married', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 126, 1: 14, '%': 10.0} \n\n(' Private', ' Assoc-acdm', ' Divorced', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 18, 1: 2, '%': 10.0} \n\n(' Private', ' Bachelors', ' Never-married', ' Tech-support', ' White', ' Female', ' United-States') \n {0: 27, 1: 3, '%': 10.0} \n\n(' Local-gov', ' HS-grad', ' Married-civ-spouse', ' Other-service', ' White', ' Male', ' United-States') \n {0: 18, 1: 2, '%': 10.0} \n\n(' Private', ' Bachelors', ' Divorced', ' Sales', ' White', ' Female', ' United-States') \n {0: 20, 1: 2, '%': 9.09} \n\n(' Private', ' 11th', ' Married-civ-spouse', ' Machine-op-inspct', ' White', ' Male', ' United-States') \n {0: 31, 1: 3, '%': 8.82} \n\n(' Private', ' HS-grad', ' Divorced', ' Exec-managerial', ' White', ' Female', ' United-States') \n {0: 63, 1: 6, '%': 8.7} \n\n(' Private', ' Some-college', ' Divorced', ' Sales', ' White', ' Male', ' United-States') \n {0: 21, 1: 2, '%': 8.7} \n\n(' Private', ' Some-college', ' Divorced', ' Exec-managerial', ' White', ' Female', ' United-States') \n {0: 63, 1: 6, '%': 8.7} \n\n(' Local-gov', ' Bachelors', ' Divorced', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 32, 1: 3, '%': 8.57} \n\n(' Private', ' Bachelors', ' Divorced', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 22, 1: 2, '%': 8.33} \n\n(' ?', ' Some-college', ' Divorced', ' ?', ' White', ' Female', ' United-States') \n {0: 22, 1: 2, '%': 8.33} \n\n(' Private', ' Some-college', ' Divorced', ' Sales', ' White', ' Female', ' United-States') \n {0: 47, 1: 4, '%': 7.84} \n\n(' Private', ' Bachelors', ' Never-married', ' Sales', ' White', ' Female', ' United-States') \n {0: 71, 1: 6, '%': 7.79} \n\n(' Local-gov', ' Bachelors', ' Never-married', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 25, 1: 2, '%': 7.41} \n\n(' ?', ' 7th-8th', ' Married-civ-spouse', ' ?', ' White', ' Male', ' United-States') \n {0: 25, 1: 2, '%': 7.41} \n\n(' Self-emp-not-inc', ' HS-grad', ' Never-married', ' Farming-fishing', ' White', ' Male', ' United-States') \n {0: 26, 1: 2, '%': 7.14} \n\n(' Private', ' Bachelors', ' Never-married', ' Adm-clerical', ' White', ' Male', ' United-States') \n {0: 53, 1: 4, '%': 7.02} \n\n(' Local-gov', ' Bachelors', ' Never-married', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 54, 1: 4, '%': 6.9} \n\n(' State-gov', ' Bachelors', ' Never-married', ' Prof-specialty', ' White', ' Male', ' United-States') \n {0: 27, 1: 2, '%': 6.9} \n\n(' Private', ' HS-grad', ' Divorced', ' Transport-moving', ' White', ' Male', ' United-States') \n {0: 68, 1: 5, '%': 6.85} \n\n(' Private', ' Some-college', ' Divorced', ' Handlers-cleaners', ' White', ' Male', ' United-States') \n {0: 14, 1: 1, '%': 6.67} \n\n(' Private', ' 9th', ' Married-civ-spouse', ' Transport-moving', ' White', ' Male', ' United-States') \n {0: 14, 1: 1, '%': 6.67} \n\n(' Private', ' Assoc-acdm', ' Never-married', ' Other-service', ' White', ' Female', ' United-States') \n {0: 14, 1: 1, '%': 6.67} \n\n(' Private', ' 11th', ' Married-civ-spouse', ' Other-service', ' White', ' Male', ' United-States') \n {0: 14, 1: 1, '%': 6.67} \n\n(' Self-emp-not-inc', ' 7th-8th', ' Married-civ-spouse', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 14, 1: 1, '%': 6.67} \n\n(' Private', ' Assoc-acdm', ' Never-married', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 14, 1: 1, '%': 6.67} \n\n(' Private', ' 11th', ' Married-civ-spouse', ' Transport-moving', ' White', ' Male', ' United-States') \n {0: 29, 1: 2, '%': 6.45} \n\n(' State-gov', ' HS-grad', ' Divorced', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 15, 1: 1, '%': 6.25} \n\n(' Private', ' Some-college', ' Never-married', ' Exec-managerial', ' White', ' Female', ' United-States') \n {0: 45, 1: 3, '%': 6.25} \n\n(' Private', ' Assoc-acdm', ' Never-married', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 15, 1: 1, '%': 6.25} \n\n(' Private', ' 10th', ' Married-civ-spouse', ' Handlers-cleaners', ' White', ' Male', ' United-States') \n {0: 15, 1: 1, '%': 6.25} \n\n(' Private', ' HS-grad', ' Divorced', ' Tech-support', ' White', ' Female', ' United-States') \n {0: 15, 1: 1, '%': 6.25} \n\n(' Private', ' HS-grad', ' Divorced', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 135, 1: 9, '%': 6.25} \n\n(' Private', ' Assoc-voc', ' Never-married', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 31, 1: 2, '%': 6.06} \n\n(' Private', ' HS-grad', ' Divorced', ' Adm-clerical', ' White', ' Male', ' United-States') \n {0: 16, 1: 1, '%': 5.88} \n\n(' Private', ' 9th', ' Married-civ-spouse', ' Machine-op-inspct', ' White', ' Male', ' United-States') \n {0: 16, 1: 1, '%': 5.88} \n\n(' Private', ' Bachelors', ' Never-married', ' Tech-support', ' White', ' Male', ' United-States') \n {0: 35, 1: 2, '%': 5.41} \n\n(' Private', ' Some-college', ' Never-married', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 54, 1: 3, '%': 5.26} \n\n(' Private', ' 11th', ' Never-married', ' Transport-moving', ' White', ' Male', ' United-States') \n {0: 19, 1: 1, '%': 5.0} \n\n(' ?', ' Bachelors', ' Never-married', ' ?', ' White', ' Male', ' United-States') \n {0: 19, 1: 1, '%': 5.0} \n\n(' Private', ' 7th-8th', ' Married-civ-spouse', ' Machine-op-inspct', ' White', ' Male', ' United-States') \n {0: 19, 1: 1, '%': 5.0} \n\n(' Private', ' Some-college', ' Separated', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 19, 1: 1, '%': 5.0} \n\n(' Private', ' Some-college', ' Never-married', ' Tech-support', ' White', ' Female', ' United-States') \n {0: 39, 1: 2, '%': 4.88} \n\n(' Self-emp-not-inc', ' Some-college', ' Never-married', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 20, 1: 1, '%': 4.76} \n\n(' Private', ' HS-grad', ' Never-married', ' Sales', ' White', ' Male', ' United-States') \n {0: 101, 1: 5, '%': 4.72} \n\n(' Private', ' Some-college', ' Divorced', ' Machine-op-inspct', ' White', ' Male', ' United-States') \n {0: 24, 1: 1, '%': 4.0} \n\n(' Private', ' Assoc-voc', ' Divorced', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 24, 1: 1, '%': 4.0} \n\n(' Private', ' Some-college', ' Divorced', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 25, 1: 1, '%': 3.85} \n\n(' ?', ' HS-grad', ' Divorced', ' ?', ' White', ' Female', ' United-States') \n {0: 25, 1: 1, '%': 3.85} \n\n(' Private', ' HS-grad', ' Divorced', ' Craft-repair', ' White', ' Female', ' United-States') \n {0: 25, 1: 1, '%': 3.85} \n\n(' Private', ' HS-grad', ' Divorced', ' Other-service', ' White', ' Male', ' United-States') \n {0: 26, 1: 1, '%': 3.7} \n\n(' Private', ' HS-grad', ' Widowed', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 54, 1: 2, '%': 3.57} \n\n(' Private', ' Some-college', ' Widowed', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 27, 1: 1, '%': 3.57} \n\n(' Private', ' Some-college', ' Divorced', ' Adm-clerical', ' Black', ' Female', ' United-States') \n {0: 27, 1: 1, '%': 3.57} \n\n(' Private', ' 9th', ' Married-civ-spouse', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 28, 1: 1, '%': 3.45} \n\n(' Private', ' Some-college', ' Never-married', ' Tech-support', ' White', ' Male', ' United-States') \n {0: 28, 1: 1, '%': 3.45} \n\n(' Local-gov', ' HS-grad', ' Divorced', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 28, 1: 1, '%': 3.45} \n\n(' Private', ' HS-grad', ' Divorced', ' Sales', ' White', ' Male', ' United-States') \n {0: 29, 1: 1, '%': 3.33} \n\n(' Private', ' Bachelors', ' Never-married', ' Other-service', ' White', ' Male', ' United-States') \n {0: 31, 1: 1, '%': 3.12} \n\n(' Private', ' HS-grad', ' Never-married', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 242, 1: 7, '%': 2.81} \n\n(' Private', ' Assoc-acdm', ' Never-married', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 35, 1: 1, '%': 2.78} \n\n(' Private', ' HS-grad', ' Never-married', ' Exec-managerial', ' White', ' Male', ' United-States') \n {0: 35, 1: 1, '%': 2.78} \n\n(' Private', ' Assoc-voc', ' Never-married', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 38, 1: 1, '%': 2.56} \n\n(' Private', ' HS-grad', ' Divorced', ' Sales', ' White', ' Female', ' United-States') \n {0: 78, 1: 2, '%': 2.5} \n\n(' Private', ' Some-college', ' Divorced', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 156, 1: 4, '%': 2.5} \n\n(' Private', ' Some-college', ' Never-married', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 42, 1: 1, '%': 2.33} \n\n(' Private', ' Some-college', ' Divorced', ' Other-service', ' White', ' Female', ' United-States') \n {0: 44, 1: 1, '%': 2.22} \n\n(' Private', ' Some-college', ' Never-married', ' Transport-moving', ' White', ' Male', ' United-States') \n {0: 45, 1: 1, '%': 2.17} \n\n(' ?', ' HS-grad', ' Never-married', ' ?', ' White', ' Male', ' United-States') \n {0: 46, 1: 1, '%': 2.13} \n\n(' Private', ' HS-grad', ' Never-married', ' Transport-moving', ' White', ' Male', ' United-States') \n {0: 93, 1: 2, '%': 2.11} \n\n(' Private', ' HS-grad', ' Divorced', ' Machine-op-inspct', ' White', ' Female', ' United-States') \n {0: 47, 1: 1, '%': 2.08} \n\n(' ?', ' HS-grad', ' Widowed', ' ?', ' White', ' Female', ' United-States') \n {0: 48, 1: 1, '%': 2.04} \n\n(' Private', ' Some-college', ' Never-married', ' Sales', ' White', ' Male', ' United-States') \n {0: 144, 1: 3, '%': 2.04} \n\n(' Private', ' Bachelors', ' Never-married', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 102, 1: 2, '%': 1.92} \n\n(' Private', ' HS-grad', ' Divorced', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 179, 1: 3, '%': 1.65} \n\n(' Private', ' Some-college', ' Never-married', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 240, 1: 4, '%': 1.64} \n\n(' Private', ' HS-grad', ' Divorced', ' Machine-op-inspct', ' White', ' Male', ' United-States') \n {0: 60, 1: 1, '%': 1.64} \n\n(' Private', ' HS-grad', ' Never-married', ' Adm-clerical', ' White', ' Male', ' United-States') \n {0: 95, 1: 1, '%': 1.04} \n\n(' Private', ' Some-college', ' Never-married', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 102, 1: 1, '%': 0.97} \n\n(' Private', ' Some-college', ' Never-married', ' Adm-clerical', ' White', ' Male', ' United-States') \n {0: 112, 1: 1, '%': 0.88} \n\n(' Private', ' HS-grad', ' Never-married', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 240, 1: 2, '%': 0.83} \n\n(' Private', ' HS-grad', ' Divorced', ' Other-service', ' White', ' Female', ' United-States') \n {0: 120, 1: 1, '%': 0.83} \n\n(' ?', ' Some-college', ' Never-married', ' ?', ' White', ' Male', ' United-States') \n {0: 138, 1: 1, '%': 0.72} \n\n(' Private', ' Some-college', ' Never-married', ' Other-service', ' White', ' Male', ' United-States') \n {0: 149, 1: 1, '%': 0.67} \n\n(' Private', ' HS-grad', ' Never-married', ' Other-service', ' White', ' Female', ' United-States') \n {0: 178, 1: 1, '%': 0.56} \n\n(' Private', ' Some-college', ' Never-married', ' Other-service', ' White', ' Female', ' United-States') \n {0: 201, 1: 1, '%': 0.5} \n\n(' Private', ' HS-grad', ' Never-married', ' Handlers-cleaners', ' White', ' Male', ' United-States') \n {0: 206, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Sales', ' White', ' Female', ' United-States') \n {0: 130, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Separated', ' Other-service', ' White', ' Female', ' United-States') \n {0: 28, 1: 0, '%': 0.0} \n\n(' Private', ' 10th', ' Never-married', ' Other-service', ' White', ' Male', ' United-States') \n {0: 38, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Divorced', ' Other-service', ' Black', ' Female', ' United-States') \n {0: 23, 1: 0, '%': 0.0} \n\n(' Private', ' Some-college', ' Divorced', ' Tech-support', ' White', ' Female', ' United-States') \n {0: 19, 1: 0, '%': 0.0} \n\n(' Private', ' 11th', ' Never-married', ' Other-service', ' White', ' Female', ' United-States') \n {0: 47, 1: 0, '%': 0.0} \n\n(' ?', ' 10th', ' Never-married', ' ?', ' White', ' Female', ' United-States') \n {0: 18, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Transport-moving', ' Black', ' Male', ' United-States') \n {0: 18, 1: 0, '%': 0.0} \n\n(' Private', ' 12th', ' Never-married', ' Handlers-cleaners', ' White', ' Male', ' United-States') \n {0: 17, 1: 0, '%': 0.0} \n\n(' ?', ' 11th', ' Never-married', ' ?', ' White', ' Male', ' United-States') \n {0: 26, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Tech-support', ' White', ' Male', ' United-States') \n {0: 18, 1: 0, '%': 0.0} \n\n(' Private', ' Assoc-voc', ' Never-married', ' Other-service', ' White', ' Female', ' United-States') \n {0: 19, 1: 0, '%': 0.0} \n\n(' Private', ' 11th', ' Divorced', ' Other-service', ' White', ' Female', ' United-States') \n {0: 18, 1: 0, '%': 0.0} \n\n(' Private', ' 11th', ' Never-married', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 40, 1: 0, '%': 0.0} \n\n(' Private', ' 11th', ' Never-married', ' Handlers-cleaners', ' White', ' Male', ' United-States') \n {0: 56, 1: 0, '%': 0.0} \n\n(' Private', ' 10th', ' Never-married', ' Sales', ' White', ' Female', ' United-States') \n {0: 21, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 19, 1: 0, '%': 0.0} \n\n(' Private', ' Assoc-acdm', ' Divorced', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 23, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Separated', ' Other-service', ' Black', ' Female', ' United-States') \n {0: 19, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Other-service', ' Black', ' Male', ' United-States') \n {0: 38, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Handlers-cleaners', ' Black', ' Male', ' United-States') \n {0: 30, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Divorced', ' Adm-clerical', ' Black', ' Female', ' United-States') \n {0: 16, 1: 0, '%': 0.0} \n\n(' Private', ' 11th', ' Never-married', ' Sales', ' White', ' Female', ' United-States') \n {0: 52, 1: 0, '%': 0.0} \n\n(' Private', ' 10th', ' Divorced', ' Other-service', ' White', ' Female', ' United-States') \n {0: 15, 1: 0, '%': 0.0} \n\n(' State-gov', ' Bachelors', ' Never-married', ' Prof-specialty', ' White', ' Female', ' United-States') \n {0: 23, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Other-service', ' White', ' Male', ' United-States') \n {0: 151, 1: 0, '%': 0.0} \n\n(' Private', ' Some-college', ' Never-married', ' Adm-clerical', ' Black', ' Female', ' United-States') \n {0: 42, 1: 0, '%': 0.0} \n\n(' Private', ' Some-college', ' Never-married', ' Sales', ' White', ' Female', ' United-States') \n {0: 183, 1: 0, '%': 0.0} \n\n(' Private', ' 12th', ' Never-married', ' Other-service', ' White', ' Male', ' United-States') \n {0: 25, 1: 0, '%': 0.0} \n\n(' Private', ' 12th', ' Never-married', ' Sales', ' White', ' Female', ' United-States') \n {0: 19, 1: 0, '%': 0.0} \n\n(' Private', ' 11th', ' Never-married', ' Other-service', ' White', ' Male', ' United-States') \n {0: 44, 1: 0, '%': 0.0} \n\n(' Private', ' 12th', ' Never-married', ' Other-service', ' White', ' Female', ' United-States') \n {0: 22, 1: 0, '%': 0.0} \n\n(' Private', ' 10th', ' Never-married', ' Handlers-cleaners', ' White', ' Male', ' United-States') \n {0: 26, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Separated', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 18, 1: 0, '%': 0.0} \n\n(' Private', ' 11th', ' Never-married', ' Other-service', ' Black', ' Male', ' United-States') \n {0: 15, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Machine-op-inspct', ' Black', ' Female', ' United-States') \n {0: 30, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Machine-op-inspct', ' White', ' Female', ' United-States') \n {0: 56, 1: 0, '%': 0.0} \n\n(' Private', ' Some-college', ' Never-married', ' Handlers-cleaners', ' White', ' Male', ' United-States') \n {0: 96, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Machine-op-inspct', ' White', ' Male', ' United-States') \n {0: 143, 1: 0, '%': 0.0} \n\n(' ?', ' HS-grad', ' Never-married', ' ?', ' White', ' Female', ' United-States') \n {0: 48, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Craft-repair', ' White', ' Female', ' United-States') \n {0: 18, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Craft-repair', ' Black', ' Male', ' United-States') \n {0: 28, 1: 0, '%': 0.0} \n\n(' Private', ' Some-college', ' Never-married', ' Farming-fishing', ' White', ' Male', ' United-States') \n {0: 26, 1: 0, '%': 0.0} \n\n(' Private', ' Bachelors', ' Never-married', ' Other-service', ' White', ' Female', ' United-States') \n {0: 27, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Farming-fishing', ' White', ' Male', ' United-States') \n {0: 49, 1: 0, '%': 0.0} \n\n(' Private', ' 11th', ' Never-married', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 17, 1: 0, '%': 0.0} \n\n(' Private', ' 10th', ' Never-married', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 23, 1: 0, '%': 0.0} \n\n(' Private', ' Some-college', ' Never-married', ' Machine-op-inspct', ' White', ' Female', ' United-States') \n {0: 23, 1: 0, '%': 0.0} \n\n(' Private', ' Bachelors', ' Never-married', ' Handlers-cleaners', ' White', ' Male', ' United-States') \n {0: 15, 1: 0, '%': 0.0} \n\n(' Private', ' Some-college', ' Never-married', ' Sales', ' Black', ' Male', ' United-States') \n {0: 18, 1: 0, '%': 0.0} \n\n(' Private', ' 7th-8th', ' Married-civ-spouse', ' Craft-repair', ' White', ' Male', ' United-States') \n {0: 45, 1: 0, '%': 0.0} \n\n(' Private', ' Some-college', ' Divorced', ' Machine-op-inspct', ' White', ' Female', ' United-States') \n {0: 17, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Adm-clerical', ' Black', ' Female', ' United-States') \n {0: 35, 1: 0, '%': 0.0} \n\n(' Local-gov', ' Some-college', ' Divorced', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 24, 1: 0, '%': 0.0} \n\n(' ?', ' HS-grad', ' Never-married', ' ?', ' Black', ' Female', ' United-States') \n {0: 25, 1: 0, '%': 0.0} \n\n(' Private', ' Some-college', ' Never-married', ' Machine-op-inspct', ' White', ' Male', ' United-States') \n {0: 42, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Separated', ' Sales', ' White', ' Female', ' United-States') \n {0: 21, 1: 0, '%': 0.0} \n\n(' ?', ' Some-college', ' Never-married', ' ?', ' Black', ' Female', ' United-States') \n {0: 16, 1: 0, '%': 0.0} \n\n(' ?', ' 10th', ' Never-married', ' ?', ' White', ' Male', ' United-States') \n {0: 26, 1: 0, '%': 0.0} \n\n(' Private', ' Some-college', ' Never-married', ' Other-service', ' Black', ' Male', ' United-States') \n {0: 19, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Other-service', ' Black', ' Female', ' United-States') \n {0: 53, 1: 0, '%': 0.0} \n\n(' ?', ' Some-college', ' Never-married', ' ?', ' White', ' Female', ' United-States') \n {0: 146, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Machine-op-inspct', ' Black', ' Male', ' United-States') \n {0: 19, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Handlers-cleaners', ' White', ' Female', ' United-States') \n {0: 30, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Sales', ' Black', ' Female', ' United-States') \n {0: 28, 1: 0, '%': 0.0} \n\n(' State-gov', ' Some-college', ' Divorced', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 21, 1: 0, '%': 0.0} \n\n(' Private', ' Assoc-acdm', ' Never-married', ' Sales', ' White', ' Male', ' United-States') \n {0: 17, 1: 0, '%': 0.0} \n\n(' ?', ' 11th', ' Never-married', ' ?', ' White', ' Female', ' United-States') \n {0: 25, 1: 0, '%': 0.0} \n\n(' State-gov', ' Some-college', ' Never-married', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 25, 1: 0, '%': 0.0} \n\n(' Private', ' 10th', ' Never-married', ' Other-service', ' White', ' Female', ' United-States') \n {0: 32, 1: 0, '%': 0.0} \n\n(' Private', ' Some-college', ' Never-married', ' Sales', ' Black', ' Female', ' United-States') \n {0: 26, 1: 0, '%': 0.0} \n\n(' ?', ' 11th', ' Married-civ-spouse', ' ?', ' White', ' Male', ' United-States') \n {0: 15, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Widowed', ' Machine-op-inspct', ' White', ' Female', ' United-States') \n {0: 16, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Never-married', ' Exec-managerial', ' White', ' Female', ' United-States') \n {0: 46, 1: 0, '%': 0.0} \n\n(' Private', ' Some-college', ' Divorced', ' Other-service', ' White', ' Male', ' United-States') \n {0: 17, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Divorced', ' Handlers-cleaners', ' White', ' Male', ' United-States') \n {0: 31, 1: 0, '%': 0.0} \n\n(' Federal-gov', ' HS-grad', ' Divorced', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 18, 1: 0, '%': 0.0} \n\n(' Private', ' Some-college', ' Never-married', ' Other-service', ' Black', ' Female', ' United-States') \n {0: 19, 1: 0, '%': 0.0} \n\n(' Private', ' 11th', ' Never-married', ' Sales', ' White', ' Male', ' United-States') \n {0: 25, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Widowed', ' Other-service', ' White', ' Female', ' United-States') \n {0: 40, 1: 0, '%': 0.0} \n\n(' Private', ' HS-grad', ' Separated', ' Adm-clerical', ' White', ' Female', ' United-States') \n {0: 36, 1: 0, '%': 0.0} \n\n</pre> In\u00a0[62]: Copied! <pre>#Get category counts\ncat_counts = [0, 0]\nfor i in sorted_data:\n    if i[1]['%'] &lt; 20:\n        cat_counts[0]+=1\n    else:\n        cat_counts[1]+=1\n\nlt25p = round((cat_counts[0] / (cat_counts[0]+cat_counts[1])*100), 2)\ngte25p = round((cat_counts[1] / (cat_counts[0]+cat_counts[1])*100), 2)\nprint('{} categories, or {}% of categories with &lt;25% of people making &gt;50k'.format(cat_counts[0], lt25p))\nprint('{} categories, or {}% of categories with over &gt;=25% of people making &gt;50k'.format(cat_counts[1], gte25p))\nprint()\n\n#Get people counts\npeople_counts = [[0, 0], [0, 0]]\nfor i in sorted_data:\n    if i[1]['%'] &lt; 25:\n        people_counts[0][0]+=i[1][0]\n        people_counts[0][1]+=i[1][1]\n    else:\n        people_counts[1][0]+=i[1][0]\n        people_counts[1][1]+=i[1][1]\n\nlt25p = round((people_counts[0][1]/(people_counts[0][0]+people_counts[0][1])*100), 2)\ngte25p = round((people_counts[1][1]/(people_counts[1][0]+people_counts[1][1])*100), 2)\nprint('{} people are in the \"&lt;25% making &gt;50k\" categories. Of those people, there are {}% making &gt;50k'.format(people_counts[0][0]+people_counts[0][1], lt25p))\nprint('{} people are in the \"&gt;=25% making &gt;50k\" categories. Of those people, there are {}% making &gt;50k'.format(people_counts[1][0]+people_counts[0][1], gte25p))\n</pre> #Get category counts cat_counts = [0, 0] for i in sorted_data:     if i[1]['%'] &lt; 20:         cat_counts[0]+=1     else:         cat_counts[1]+=1  lt25p = round((cat_counts[0] / (cat_counts[0]+cat_counts[1])*100), 2) gte25p = round((cat_counts[1] / (cat_counts[0]+cat_counts[1])*100), 2) print('{} categories, or {}% of categories with &lt;25% of people making &gt;50k'.format(cat_counts[0], lt25p)) print('{} categories, or {}% of categories with over &gt;=25% of people making &gt;50k'.format(cat_counts[1], gte25p)) print()  #Get people counts people_counts = [[0, 0], [0, 0]] for i in sorted_data:     if i[1]['%'] &lt; 25:         people_counts[0][0]+=i[1][0]         people_counts[0][1]+=i[1][1]     else:         people_counts[1][0]+=i[1][0]         people_counts[1][1]+=i[1][1]  lt25p = round((people_counts[0][1]/(people_counts[0][0]+people_counts[0][1])*100), 2) gte25p = round((people_counts[1][1]/(people_counts[1][0]+people_counts[1][1])*100), 2) print('{} people are in the \"&lt;25% making &gt;50k\" categories. Of those people, there are {}% making &gt;50k'.format(people_counts[0][0]+people_counts[0][1], lt25p)) print('{} people are in the \"&gt;=25% making &gt;50k\" categories. Of those people, there are {}% making &gt;50k'.format(people_counts[1][0]+people_counts[0][1], gte25p)) <pre>206 categories, or 62.61% of categories with &lt;25% of people making &gt;50k\n123 categories, or 37.39% of categories with over &gt;=25% of people making &gt;50k\n\n9989 people are in the \"&lt;25% making &gt;50k\" categories. Of those people, there are 4.49% making &gt;50k\n3964 people are in the \"&gt;=25% making &gt;50k\" categories. Of those people, there are 56.36% making &gt;50k\n</pre> <p>From the above results, we can see some interesting trends based on the categorical data alone.</p> <ul> <li>The best performing category, with 56 out of 57 samples making &gt;50k, are self-emp-inc, have prof-school education, are married-civ-spouse, have prof-specialty occupation, are white males, and live in the US.</li> <li>Almost every category above 40% people making &gt;50k has 'Married-civ-spouse' for marital status.</li> <li>Workclass alone doesn't seem to be as correlated to success - the top 6 performing categories all have different workclasses. However, many poor performing categories seem to have 'Private' for the workclass. This could also be due to the workclass 'Private' being the largest reported for the 'Occupation' feature.</li> <li>Although 'White' is seen as race for most high-performers, it is also seen frequently in the average and low performers. This shows that 'White' was the largest race involved in this study.</li> <li>There doesn't appear to be many, if any, native countries other than 'United-States'. This could be due to numerical features not included in this selection, or it shows less of a correlation for notable categories.</li> <li>2/3 of notable categories have &lt;25% making &gt;50k, while the last third has &gt;=25%.</li> </ul> <p>Additional statistics at the bottom of the output:</p> <ul> <li>The percent of people in the \"&gt;=25% making &gt;50k\" category is over 1200% more than those in the \"&lt;25% making 50k\" category.</li> <li>The total percents for each split are extremely far from the 25/75 split, with only 4.49% in the \"&lt;25% making &gt;50k\" category making &gt;50k, and 56.36% in the \"&gt;=25% making &gt;50k\" making &gt;50k.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"research/census/#visualization","title":"Visualization\u00b6","text":""},{"location":"research/census/#age","title":"Age\u00b6","text":""},{"location":"research/census/#bar-chart","title":"Bar Chart\u00b6","text":"<p>From the bar chart, we can see there is a trend of ages where the number of people in the &gt;50k class grows, from what looks like the ages of 28-65. This would suggest a correlation between age and salary</p>"},{"location":"research/census/#table","title":"Table\u00b6","text":"<p>The table gives more evidence to this correlation, showing that as age increases, so does likelihood of being in &gt;50k class. After age 27, almost no ages drop to less than 10% making &gt;50k. Ages 31-69 have &gt;=20% that are in that class, and ages 37-61 have all but one where &gt;=30% make &gt;50k.</p>"},{"location":"research/census/#scatterplot","title":"Scatterplot\u00b6","text":"<p>The scatterplot of each percentage shows a quadratic trend to age vs salary, displaying the ages discussed in the table section and how middle-aged people are more likely to be in the &gt;50k category.</p>"},{"location":"research/census/#workclass","title":"Workclass\u00b6","text":""},{"location":"research/census/#bar-chart","title":"Bar Chart\u00b6","text":"<p>The bar chart shows us that  almost all categories have noticable portions in the &gt;50k class, all except unknown (?), without-pay, and never worked. Without pay had less than 20 people fitting this category, and it would make sense that most people without pay would make &gt;50k annually.</p>"},{"location":"research/census/#table","title":"Table\u00b6","text":"<p>The table does confirm that most categories do well; however, there are some workclasses that do better than others. self-emp-inc are people who work for thesmelves in corporate entities. This workclass has an actual majority making &gt;50k, with 55% fitting this.</p>"},{"location":"research/census/#fnlwgt","title":"Fnlwgt\u00b6","text":""},{"location":"research/census/#bar-chart","title":"Bar Chart\u00b6","text":"<p>Fnlwgt appears to have a similar quadratic trend to age, suggesting that fnlwgt is a noticable factor for determining class. The dropoff for # of people for each fnlwgt category drops off rather quickly, so this may be a reason why this quadratic curve exists - rather than correlation, this could be just causation.</p>"},{"location":"research/census/#table","title":"Table\u00b6","text":"<p>The table shows that, rather than a quadratic fit for percentages, there is actually a relatively linear fit with some noise. Most splits for this column have a 75-25 split for each class, which is similar to what the actual data is (roughly 75% are in the &lt;=50k class, and roughly 25% are in the &gt;50k class).</p>"},{"location":"research/census/#scatterplot","title":"Scatterplot\u00b6","text":"<p>The scatterplot visualizes this linear trend for us, showing a general updward curve in percent of people in the &gt;50k class. However, in the first few splits, each value is relatively close to what it's neighbors are at, but as the splits increase, there is more deviation - the greater splits follow the 75-25 curve less, some with more and some with less in the &gt;50k class.</p>"},{"location":"research/census/#education","title":"Education\u00b6","text":""},{"location":"research/census/#bar-chart","title":"Bar Chart\u00b6","text":"<p>The graph shows clear differences in proportions for each education category, with some having very few people in the &gt;50k category, and some educations having a majority. It appears that the higher the education, the higher the percent making &gt;50k annually.</p>"},{"location":"research/census/#table","title":"Table\u00b6","text":"<p>The percentages reinforce this theory that higher education correlates to greater salary - All levels below high-school grad have &lt;10% making &gt;50k. However, HS grad has nearly 16%, bachelors have a great increase at 41%, 3x as many as high school grad. Moving upward, Masters have 54%, and Doctorates have over 70% each. Furthermore, Assoc-acdm and Assoc-voc have ~25% each, and Prof school matches the doctorate at 70% making &gt;50k.</p>"},{"location":"research/census/#marital-status","title":"Marital-status\u00b6","text":""},{"location":"research/census/#bar-chart","title":"Bar Chart\u00b6","text":"<p>Most categories for marital status are dominantly &lt;=50k. There are only 2 categories that do exceptionally well - married-civ-spouse, and married-af-spouse. However, married-af-spouse has very few samples, so this may be innaccurate.</p>"},{"location":"research/census/#table","title":"Table\u00b6","text":"<p>The table for marital status shows that, indeed, married individuals do exceptionally better than those who aren't. Married civilian spouses have just under 45% making &gt;50k annually, and armed-forces spouses are at ~37% in the same category.</p>"},{"location":"research/census/#occupation","title":"Occupation\u00b6","text":""},{"location":"research/census/#bar-chart","title":"Bar Chart\u00b6","text":"<p>Again, we see several categories that are mostly &lt;=50k, but some that are a more even split. This would be expected of occupation, the source of income. Executive/managerial, professional/specialty, sales, and craft repair all seem to have noticable chunks in the &gt;50k class.</p>"},{"location":"research/census/#table","title":"Table\u00b6","text":"<p>Exec-manageral performs the best with nearly 48% making &gt;50k annually. Prof-speciality does nearly the same at 47%. Armed forces comes in 3rd at 33%, but again, this may be inaccurate as the small sample size. Protective-services follows up with 31%, which has a significantly large sample than armed forces.</p>"},{"location":"research/census/#relationship","title":"Relationship\u00b6","text":""},{"location":"research/census/#bar-chart","title":"Bar Chart\u00b6","text":"<p>This chart is closely linked to the marital-status column - only the husband and wife categories have majorities making &gt;50k annually. The rest have significantly fewer.</p>"},{"location":"research/census/#table","title":"Table\u00b6","text":"<p>Husband and wife, as shown, display the best results - however, wife actually has 46%, whereas husband has 44%. This is different than what the \"sex\" column will show later on, with women having a significantly smaller percentage making &gt;50k.</p>"},{"location":"research/census/#race","title":"Race\u00b6","text":""},{"location":"research/census/#bar-chart","title":"Bar Chart\u00b6","text":"<p>Here we can actually see a negative correlation - only white and pacific highlander seem to have a percentage of standard size making &gt;50k. The rest have smaller percentages, showing that race could lead to a smaller salary.</p>"},{"location":"research/census/#table","title":"Table\u00b6","text":"<p>As discussed, only white and pacific highlander have reasonable percentages. In fact, they follow the trend of the data presented, with 75% making &lt;=50k, and 25% making &gt;50k. The other races have smaller than 25% making &gt;50k, in fact by half as much - black, amer-indian-eskimo, and other all have 11-12% in the &gt;50k class.</p>"},{"location":"research/census/#sex","title":"Sex\u00b6","text":""},{"location":"research/census/#bar-chart","title":"Bar Chart\u00b6","text":"<p>From the earlier relationship column, we see husbands and wives with nearly the same percentage making &gt;50k. Here, however, we see that men have a significantly higher percentage in this category.</p>"},{"location":"research/census/#table","title":"Table\u00b6","text":"<p>Again, men do, in fact, have a greater percentage making &gt;50k - 20% more than women. In fact, men even have a greater percentage than the ~75-25 trend the data follows, while women have significantly less.</p>"},{"location":"research/census/#capital-gain","title":"Capital-gain\u00b6","text":""},{"location":"research/census/#bar-chart","title":"Bar Chart\u00b6","text":"<p>Almost everyone in this study had &lt;20,000 for capital gain. From everyone between 0 and 20,000, almost everyone there had 0 capital gain. However, there were a few people making nearly 100,000 in capital gain, and everyone in this category is in the &gt;50k class.</p>"},{"location":"research/census/#table","title":"Table\u00b6","text":"<p>Looking at each split under 11,000, there are some splits that extremely favor those who make &gt;50k. Given more data, we could assume a certain salary looking at these splits. Furthermore, looking at the few who have a capital-gain of nearly 100,000 and how it is 100% people in the &gt;50k class, we could make connections with their salaries given more information there as well.</p>"},{"location":"research/census/#scatterplot","title":"Scatterplot\u00b6","text":"<p>The scatterplot paints the story told in the table - for captain gain under 5000, this is mostly people making &lt;=50k. From 5,000-6,000, this is a fairly even split, then 6,000-7,000, this again favors &lt;=50k salaries. However, capital gain from 7,000 to 20,000 is almost always 100% salaries &gt;50k. There are some splits after 20,000 that switch back and forth, but the highest capital gain is, again, &gt;50k.</p>"},{"location":"research/census/#capital-loss","title":"Capital-loss\u00b6","text":""},{"location":"research/census/#bar-chart","title":"Bar Chart\u00b6","text":"<p>The bar chart shows, similar to capital gain, most people had 0 capital loss. After 0, there are some samples spread out, but the numbers at 0 capital loss follow the 75-25 trend of the data.</p>"},{"location":"research/census/#table","title":"Table\u00b6","text":"<p>The data doesn't show us any obvious trends or correlations, but it does suggest that certain splits favor a group more than other splits. While correlation may not be obvious, it does show it is related in some way.</p>"},{"location":"research/census/#scatterplot","title":"Scatterplot\u00b6","text":"<p>The scatterplot reinforces the table, not necessarily showing any obvious correlations - the percentages seem to flip-flop in favoritism toward each class.</p>"},{"location":"research/census/#hours-per-week","title":"Hours-per-week\u00b6","text":""},{"location":"research/census/#bar-chart","title":"Bar Chart\u00b6","text":"<p>The graph here shows that most people work an average of 40 hours per week. There are some larger samples at multiples of 5, such as 20, 30, 35, 45, 50, and 60. There is an interesting trend - under 35 hours per week, a very small portion of each group is in the &gt;50k class. This is likely due to working few hours per week, likely as part-time.</p>"},{"location":"research/census/#table","title":"Table\u00b6","text":"<p>The table here shows a steady increase in percent of each group making &gt;50k, with 41 hours/week holding 25% in the &gt;50k class. From there, the trend continues steadily upward to greater percentages making &gt;50k, although the more hours worked per week, the more variance and noise there is - higher hours doesn't guarantee making &gt;50k.</p>"},{"location":"research/census/#scatterplot","title":"Scatterplot\u00b6","text":"<p>Here we can see that general trend upward for increase in percentage of people making &gt;50k, and we can also see that variance. Percent of people per split making &gt;50k increases quickly from 15 to ~45, then from there it appears to not be as correlated, with the scatterplot showing less of a trend.</p>"},{"location":"research/census/#native-country","title":"Native country\u00b6","text":""},{"location":"research/census/#bar-chart","title":"Bar Chart\u00b6","text":"<p>For the united states, there appears to be no correlation. However, for some native countries such as Mexico and India, the percentage of each class is askew, with Mexico having significantly less than what the data shows overall, and India having a higher percentage. There are significantly fewer samples for these native countries, so this could be a biased sample.</p>"},{"location":"research/census/#table","title":"Table\u00b6","text":"<p>The table shows many countries having a variance for percentages from what the data holds. Most countries other than the United States have very few samples overall. Because of this, these results could be misleading.</p>"},{"location":"research/census/#chi-square-test","title":"Chi-square-test\u00b6","text":"<p>Now that we've seen the visualizations of each feature, let's look at how strongly the categorical data is linked to classification using a chi square test.</p>"},{"location":"research/census/#pearsons-and-spearmans-correlations","title":"Pearson's-and-Spearman's-Correlations\u00b6","text":""},{"location":"research/census/#data-preprocessing","title":"Data-preprocessing\u00b6","text":"<p>It's rarely the case where a dataset is ready to be analyzed - normally, there is some data preprocessing that needs to happen. This can include converting columns (as shown already), or pruning (removing columns). There are many steps that can be helpful to take, but here, pruning should show us great results. Given more time, further preprocessing could occur with a deeper analysis of the above visuals, but for now, pruning alone will help.</p>"},{"location":"research/census/#pruning","title":"Pruning\u00b6","text":"<p>From what was discussed in the visualization section, some columns are dependent on others or simply have no correlation. While it's not so easy as to say how \"important\" each column is, there are some we can remove with confidence:</p> <ul> <li>Education - There is already an \"education-num\" column</li> <li>Relationship - Marital status covers this</li> <li>Fnlwgt - This came after I researched what exactly fnlwgt was. From the description online, and from what was visualized above with minimal correlation, I decided to remove it to see if results improved (they did).</li> </ul>"},{"location":"research/census/#classification","title":"Classification\u00b6","text":"<p>For this assessment, I decided to use 4 different approaches for classification:</p> <ul> <li>Neural Network</li> <li>Naive Bayes</li> <li>SVM</li> <li>Random forest</li> </ul> <p>Before each approach, I will discuss why I chose that approach, the pros and cons to that algorithm, improvements that could be made, and I will discuss results and visualizations after each algorithm.</p>"},{"location":"research/census/#neural-network","title":"Neural-network\u00b6","text":""},{"location":"research/census/#why-i-chose-this-algorithm","title":"Why I chose this algorithm\u00b6","text":"<p>Neural networks are increasingly popular in the world of data science and machine learning. They are excellent at recognizing patterns, and can work with vague or even incomplete data. For this dataset, with a high amount of variance with each feature, I believe a neural network would be great at identifying relationships</p>"},{"location":"research/census/#pros","title":"Pros\u00b6","text":"<ul> <li>Great pattern recognition capabilities</li> <li>Hyperparameters can be customized to allow for flexibility</li> <li>Datasets can be less complete than what is needed for other algorithms</li> </ul>"},{"location":"research/census/#cons","title":"Cons\u00b6","text":"<ul> <li>Computationally intensive</li> <li>\"Black box\" method - not always clear what is going on in the neural network</li> <li>Due to hyperparameters offering so much customization, hard to know exactly what is best with limited time</li> </ul>"},{"location":"research/census/#improvements-that-could-be-made","title":"Improvements that could be made\u00b6","text":"<ul> <li>Again, hyperparameters - performing hyperparameter optimization would be ideal, but with limited time, this solution works well enough.</li> <li>Defining what type of neural network is best for this type of classification. There are many different types of networks, so spending time to find which is best could show improvements.</li> </ul>"},{"location":"research/census/#all-optimizers","title":"All-optimizers\u00b6","text":"<p>In a continuous effort to improve, let's take a look at one hyperparameter - the optimizer. We'll loop through each optimizer, save the results, and plot these results to see how varying default optimizers impact test accuracy. Then, we will add L1/L2 regularization to each hidden layer and compare those results.</p>"},{"location":"research/census/#get-non-regularized-data","title":"Get-non-regularized-data\u00b6","text":""},{"location":"research/census/#get-regularized-data","title":"Get-regularized-data\u00b6","text":""},{"location":"research/census/#non-regularized-neural-network-optimizers","title":"Non-regularized-neural-network-optimizers\u00b6","text":""},{"location":"research/census/#regularized-neural-network-optimizers","title":"Regularized-neural-network-optimizers\u00b6","text":""},{"location":"research/census/#adam-optimizer","title":"Adam-optimizer\u00b6","text":"<p>Now, let's look at the results from using just the 'adam' optimizer to see other metrics.</p>"},{"location":"research/census/#neural-network-accuracy","title":"Neural-network-accuracy\u00b6","text":""},{"location":"research/census/#neural-network-cm","title":"Neural-network-CM\u00b6","text":""},{"location":"research/census/#neural-network-results","title":"Neural-network-results\u00b6","text":""},{"location":"research/census/#evaluation","title":"Evaluation\u00b6","text":"<p>From the above cells, running the evaluate function on the model with the test dataset shows an accuracy of nearly 0.85, which is excellent. Noticable improvement was seen when feature pruning was introduced (was ~0.78 accuracy).</p>"},{"location":"research/census/#confusion-matrix","title":"Confusion Matrix\u00b6","text":"<p>The confusion matrix shows the greatest improvement with feature pruning - before, there were &lt;100 guesses for &gt;50k classifications total. Now, there is a large chunk of true positives (and false positives).</p>"},{"location":"research/census/#naive-bayes","title":"Naive-bayes\u00b6","text":""},{"location":"research/census/#why-i-chose-this-algorithm","title":"Why I chose this algorithm\u00b6","text":"<p>Naive bayes are great for classifying large datasets with many features. Computationally, they are significantly faster than most other algorithms. Additionally, this takes a different approach - the assumption with \"naive\" bayes is that all features are independent. Most other algorithms, such as the neural network, look for patterns among features and classes, but this assumes total independence of each feature from one another.</p>"},{"location":"research/census/#pros","title":"Pros\u00b6","text":"<ul> <li>Fast</li> <li>Simple to implement, fewer hyperparameters to tweak for best results</li> <li>When assumption of independence holds true, classifies better than most models</li> </ul>"},{"location":"research/census/#cons","title":"Cons\u00b6","text":"<ul> <li>Due to less hyperparameters, more rigid to results - what you get is what you get</li> <li>Anything in test data not in training data will be set to probability 0, and model will be unable to make a prediction for that class</li> <li>Assumption of indendence is rarely true</li> </ul>"},{"location":"research/census/#improvements-that-could-be-made","title":"Improvements that could be made\u00b6","text":"<ul> <li>Instead of a naive bayes approach, spending time to implement a bayesian network instead where features are related to each other</li> <li>Create a generative model for missing data, so model doesn't have to guess 0 if the data is missing</li> </ul>"},{"location":"research/census/#nb-accuracy","title":"NB-accuracy\u00b6","text":""},{"location":"research/census/#nb-cm","title":"NB-CM\u00b6","text":""},{"location":"research/census/#nb-results","title":"NB-results\u00b6","text":""},{"location":"research/census/#evaluation","title":"Evaluation\u00b6","text":"<p>From the above cells, running the evaluate function on the model with the test dataset shows an accuracy of nearly 0.80.</p>"},{"location":"research/census/#confusion-matrix","title":"Confusion Matrix\u00b6","text":"<p>Again, significant improvement thanks to feature pruning. The largest error seems to be with false negatives. While the neural network trains and updates, the naive bayes algorithm relies on probabilities from the training set, which don't update over time. This dataset has roughly a 75/25 split for both classes, where 75% make &lt;=50k, and 25% make &gt;50k. Because of this, the model is more likely to predict 0 (&lt;=50k) instead of 1 (&gt;50k).</p>"},{"location":"research/census/#support-vector-machines","title":"Support-vector-machines\u00b6","text":""},{"location":"research/census/#why-i-chose-this-algorithm","title":"Why I chose this algorithm\u00b6","text":"<p>Support Vector Machiones work well when there are clear separations between classes. Additionally, SVM's are more effective in high-dimensional spaces.</p>"},{"location":"research/census/#pros","title":"Pros\u00b6","text":"<ul> <li>Effective in higher dimensions</li> <li>Great when classes are separable</li> <li>Outliers have less impact</li> </ul>"},{"location":"research/census/#cons","title":"Cons\u00b6","text":"<ul> <li>Very slow</li> <li>The more overlap classes have, the worse the performance</li> <li>Selecting hyperparameters and kernel functions can be difficult</li> </ul>"},{"location":"research/census/#improvements-that-could-be-made","title":"Improvements that could be made\u00b6","text":"<ul> <li>Due to time constraints, I went with scikit's built in SVM. Given time, I'd be able to fiddle around with this more and pick the best options for this SVM</li> <li>This one is just a time constraint, but due to the time SVM's take to train, include more samples in the fitting method/using a linear kernel.</li> </ul>"},{"location":"research/census/#svm-accuracy","title":"SVM-accuracy\u00b6","text":""},{"location":"research/census/#svm-cm","title":"SVM-CM\u00b6","text":""},{"location":"research/census/#svm-results","title":"SVM-results\u00b6","text":""},{"location":"research/census/#evaluation","title":"Evaluation\u00b6","text":"<p>From the above cells, running the evaluate function on the model with the test dataset shows an accuracy of almost 0.79. Currently, only 20,000 samples are used for training due to the time it takes for this algorithm to run. I attempted to use a liner kernel on 2,000 samples overnight, and that process either froze or never finished.</p>"},{"location":"research/census/#confusion-matrix","title":"Confusion Matrix\u00b6","text":"<p>This confusion matrix shows most classifications leaning away from classifying as &gt;50k. Interestingly, there are very few false positives predicted. I would like to see how this algorithm performs with a linear kernel on the entire dataset, as soon as I get my hands on a space-age computer fast enough to do that training in my lifetime.</p>"},{"location":"research/census/#general-ensemble-classifiers","title":"General-ensemble-classifiers\u00b6","text":"<p>Instead of just using random forests, I wanted to take advantage of the available classifiers in scikit's ensemble library. Below, I will use the follow models:</p> <ul> <li>AdaBoost</li> <li>Bagging</li> <li>ExtraTrees</li> <li>GradientBoosting</li> <li>RandomForest</li> <li>HistGradientBoosting (experimental)</li> </ul>"},{"location":"research/census/#ensembles-accuracies","title":"Ensembles-accuracies\u00b6","text":""},{"location":"research/census/#ensembles-bar-chart","title":"Ensembles-bar-chart\u00b6","text":""},{"location":"research/census/#ensembles-roc-curves","title":"Ensembles-ROC-curves\u00b6","text":""},{"location":"research/census/#random-forest","title":"Random-forest\u00b6","text":""},{"location":"research/census/#why-i-chose-this-algorithm","title":"Why I chose this algorithm\u00b6","text":"<p>Single decision trees work well when there are clear delineations among the data. Random forests, or ensembles, work extremely well where a single decision tree would work well. Using an ensemble to find the splits among large datasets is not only fast, but excellent at fitting.</p>"},{"location":"research/census/#pros","title":"Pros\u00b6","text":"<ul> <li>Fast</li> <li>Elegant and powerful solution when working in high-dimensional data spaces</li> <li>Works well with missing/incomplete/error-prone datasets</li> </ul>"},{"location":"research/census/#cons","title":"Cons\u00b6","text":"<ul> <li>Similar to neural networks, more of a \"black box\" approach - not as much control over exactly what goes on inside the random forest ensemble</li> <li>Works poorly when features aren't correlated</li> <li>Multitudes of hyperparameters can wildly affect performance</li> </ul>"},{"location":"research/census/#improvements-that-could-be-made","title":"Improvements that could be made\u00b6","text":"<ul> <li>Tweak hyperparameters delicately for best results</li> </ul>"},{"location":"research/census/#rf-accuracy","title":"RF-accuracy\u00b6","text":""},{"location":"research/census/#rf-cm","title":"RF-CM\u00b6","text":""},{"location":"research/census/#rf-results","title":"RF-results\u00b6","text":""},{"location":"research/census/#evaluation","title":"Evaluation\u00b6","text":"<p>Random forest accuracy seems to be the best of these 4 models, coming in at over 0.85. Additionally, this is one of the fastest models as well, performing great among highly-correlated data.</p>"},{"location":"research/census/#confusion-matrix","title":"Confusion Matrix\u00b6","text":"<p>This confusion matrix shows the most accurate classifications of true positives (&gt;50k), as well as the fewest classifications of false positives/negatives.</p>"},{"location":"research/census/#roc-curves","title":"ROC-curves\u00b6","text":"<p>Displayed here are the ROC curves for each classification algorithm used above.</p>"},{"location":"research/census/#segmentation","title":"Segmentation\u00b6","text":""},{"location":"research/census/#manual-segmentation","title":"Manual-segmentation\u00b6","text":""},{"location":"research/census/#automated-segmentation","title":"Automated-segmentation\u00b6","text":""},{"location":"research/cs447/","title":"Working with semantics and parts-of-speech for non-canonical data","text":""},{"location":"research/dlh_study/","title":"Short term mortality prediction in ICU patients with sepsis-3","text":"In\u00a0[1]: Copied! <pre>!pip install google-cloud-bigquery\n!pip install pandas\n!pip install db-dtypes\n!pip install scikit-learn\n!pip install xgboost\n!pip install opencv-python\n!pip install matplotlib\n</pre> !pip install google-cloud-bigquery !pip install pandas !pip install db-dtypes !pip install scikit-learn !pip install xgboost !pip install opencv-python !pip install matplotlib <pre>Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.10/dist-packages (3.21.0)\nRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0dev,&gt;=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.11.1)\nRequirement already satisfied: google-auth&lt;3.0.0dev,&gt;=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.27.0)\nRequirement already satisfied: google-cloud-core&lt;3.0.0dev,&gt;=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.3.3)\nRequirement already satisfied: google-resumable-media&lt;3.0dev,&gt;=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.7.0)\nRequirement already satisfied: packaging&gt;=20.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (24.0)\nRequirement already satisfied: python-dateutil&lt;3.0dev,&gt;=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.8.2)\nRequirement already satisfied: requests&lt;3.0.0dev,&gt;=2.21.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.31.0)\nRequirement already satisfied: googleapis-common-protos&lt;2.0.dev0,&gt;=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0dev,&gt;=1.34.1-&gt;google-cloud-bigquery) (1.63.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0.dev0,&gt;=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0dev,&gt;=1.34.1-&gt;google-cloud-bigquery) (3.20.3)\nRequirement already satisfied: grpcio&lt;2.0dev,&gt;=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0dev,&gt;=1.34.1-&gt;google-cloud-bigquery) (1.63.0)\nRequirement already satisfied: grpcio-status&lt;2.0.dev0,&gt;=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0dev,&gt;=1.34.1-&gt;google-cloud-bigquery) (1.48.2)\nRequirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth&lt;3.0.0dev,&gt;=2.14.1-&gt;google-cloud-bigquery) (5.3.3)\nRequirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth&lt;3.0.0dev,&gt;=2.14.1-&gt;google-cloud-bigquery) (0.4.0)\nRequirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth&lt;3.0.0dev,&gt;=2.14.1-&gt;google-cloud-bigquery) (4.9)\nRequirement already satisfied: google-crc32c&lt;2.0dev,&gt;=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media&lt;3.0dev,&gt;=0.6.0-&gt;google-cloud-bigquery) (1.5.0)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&lt;3.0dev,&gt;=2.7.2-&gt;google-cloud-bigquery) (1.16.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (3.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (2024.2.2)\nRequirement already satisfied: pyasn1&lt;0.7.0,&gt;=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3.0.0dev,&gt;=2.14.1-&gt;google-cloud-bigquery) (0.6.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\nRequirement already satisfied: tzdata&gt;=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\nRequirement already satisfied: numpy&gt;=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.16.0)\nRequirement already satisfied: db-dtypes in /usr/local/lib/python3.10/dist-packages (1.2.0)\nRequirement already satisfied: packaging&gt;=17.0 in /usr/local/lib/python3.10/dist-packages (from db-dtypes) (24.0)\nRequirement already satisfied: pandas&gt;=0.24.2 in /usr/local/lib/python3.10/dist-packages (from db-dtypes) (2.0.3)\nRequirement already satisfied: pyarrow&gt;=3.0.0 in /usr/local/lib/python3.10/dist-packages (from db-dtypes) (14.0.2)\nRequirement already satisfied: numpy&gt;=1.16.6 in /usr/local/lib/python3.10/dist-packages (from db-dtypes) (1.25.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=0.24.2-&gt;db-dtypes) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=0.24.2-&gt;db-dtypes) (2023.4)\nRequirement already satisfied: tzdata&gt;=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=0.24.2-&gt;db-dtypes) (2024.1)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=0.24.2-&gt;db-dtypes) (1.16.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: numpy&gt;=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\nRequirement already satisfied: scipy&gt;=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\nRequirement already satisfied: numpy&gt;=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy&gt;=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.16.0)\n</pre> In\u00a0[2]: Copied! <pre># Need to check if google colab or not to properly show images\nimport sys\nGOOGLE_COLAB ='google.colab' in sys.modules\nprint('Using Google Colab: ', GOOGLE_COLAB)\n\nif GOOGLE_COLAB:\n  from google.colab.patches import cv2_imshow\n</pre> # Need to check if google colab or not to properly show images import sys GOOGLE_COLAB ='google.colab' in sys.modules print('Using Google Colab: ', GOOGLE_COLAB)  if GOOGLE_COLAB:   from google.colab.patches import cv2_imshow <pre>Using Google Colab:  True\n</pre> In\u00a0[3]: Copied! <pre>import io\nimport os\nimport sys\nimport cv2\nimport json\nimport requests\nimport zipfile\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom google.cloud import bigquery\n\n# Set env variables\nos.environ[\"GCLOUD_PROJECT\"] = 'dlh-project-418923'\nos.environ[\"BQ_DATASET_BASE\"] = 'mimiciii_clinical'\nos.environ[\"BQ_DATASET_DERIVED\"] = 'mimiciii_derived'\nos.environ[\"BQ_DATASET_FEATURES\"] = 'models_features'\nos.environ[\"LOG_REG_FEATURES_VIEW\"] = 'logreg_features'\nos.environ[\"XGBOOST_FEATURES_VIEW\"] = 'xgboost_features'\nos.environ[\"SAPSII_FEATURES_VIEW\"] = 'sapsii_features'\nRANDOM_STATE = 42\n\n# URLs for downloading from drive\ngcloud_ids = {'sa-creds': '1qNBEkuIQgj0K-PJ9ETvqF6Cel8wMlVGA',\n              'mimic-views': '13uCK5Go9XvANkeujrDaWY2cK3ocWakVM',\n              'model-features': '1_z6HLDKnHH8iUddS_wMPg3UhexhczpZq',\n              'img-features': '1VTAdcxG0Tz6vLhRWhc2b_kWaHjfeK_am',\n              'graph-roc': '1Hc_vLhGYR4yU4LG-o42p3rVdUEDHXAuR',\n              'graph-dca': '1q6c4b_IWql7hp829K1dDaoS6GEP6Bhcv',\n              'graph-cic': '1x56v9kFDM_dS3JcML3UL0BlOSoJr7XGx',\n              'graph-nomogram': '1yyFhCauWCAgqRyJe5E-DZdkdfiJiOiIC',\n              'my-roc': '1LHFSwLI1X5fOCVtPClQr9od6c3ZfD4GD',\n              'my-dca': '1mVIHMui69o3SRv76cbrrm9CnnbsT7zWn',}\ngcloud_url = 'https://drive.google.com/uc?export=download&amp;id={}'\n</pre> import io import os import sys import cv2 import json import requests import zipfile  import numpy as np import pandas as pd import matplotlib.pyplot as plt from google.cloud import bigquery  # Set env variables os.environ[\"GCLOUD_PROJECT\"] = 'dlh-project-418923' os.environ[\"BQ_DATASET_BASE\"] = 'mimiciii_clinical' os.environ[\"BQ_DATASET_DERIVED\"] = 'mimiciii_derived' os.environ[\"BQ_DATASET_FEATURES\"] = 'models_features' os.environ[\"LOG_REG_FEATURES_VIEW\"] = 'logreg_features' os.environ[\"XGBOOST_FEATURES_VIEW\"] = 'xgboost_features' os.environ[\"SAPSII_FEATURES_VIEW\"] = 'sapsii_features' RANDOM_STATE = 42  # URLs for downloading from drive gcloud_ids = {'sa-creds': '1qNBEkuIQgj0K-PJ9ETvqF6Cel8wMlVGA',               'mimic-views': '13uCK5Go9XvANkeujrDaWY2cK3ocWakVM',               'model-features': '1_z6HLDKnHH8iUddS_wMPg3UhexhczpZq',               'img-features': '1VTAdcxG0Tz6vLhRWhc2b_kWaHjfeK_am',               'graph-roc': '1Hc_vLhGYR4yU4LG-o42p3rVdUEDHXAuR',               'graph-dca': '1q6c4b_IWql7hp829K1dDaoS6GEP6Bhcv',               'graph-cic': '1x56v9kFDM_dS3JcML3UL0BlOSoJr7XGx',               'graph-nomogram': '1yyFhCauWCAgqRyJe5E-DZdkdfiJiOiIC',               'my-roc': '1LHFSwLI1X5fOCVtPClQr9od6c3ZfD4GD',               'my-dca': '1mVIHMui69o3SRv76cbrrm9CnnbsT7zWn',} gcloud_url = 'https://drive.google.com/uc?export=download&amp;id={}' In\u00a0[4]: Copied! <pre># Define a function to show images based on URL\ndef show_image(gcloud_id: str, resize: bool=False) -&gt; None:\n  # Download bytes from URL\n  f = io.BytesIO()\n  # Write to BytesIO and reset stream position\n  f.write(requests.get(gcloud_url.format(gcloud_ids[gcloud_id])).content)\n  f.seek(0)\n  # Write bytes into numpy array, then to a cv2 image\n  file_bytes = np.asarray(bytearray(f.read()), dtype=np.uint8)\n  img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)\n  # Resize if specified\n  if resize: img=cv2.resize(img, (800,600))\n  # Close bytesIO\n  f.close()\n\n  # Show image\n  if GOOGLE_COLAB: cv2_imshow(img)\n  else:\n      # Use plt because cv2 doesn't work well inline\n      plt.imshow(img)\n      plt.show()\n</pre> # Define a function to show images based on URL def show_image(gcloud_id: str, resize: bool=False) -&gt; None:   # Download bytes from URL   f = io.BytesIO()   # Write to BytesIO and reset stream position   f.write(requests.get(gcloud_url.format(gcloud_ids[gcloud_id])).content)   f.seek(0)   # Write bytes into numpy array, then to a cv2 image   file_bytes = np.asarray(bytearray(f.read()), dtype=np.uint8)   img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)   # Resize if specified   if resize: img=cv2.resize(img, (800,600))   # Close bytesIO   f.close()    # Show image   if GOOGLE_COLAB: cv2_imshow(img)   else:       # Use plt because cv2 doesn't work well inline       plt.imshow(img)       plt.show() In\u00a0[5]: Copied! <pre>show_image('img-features')\n# Figure 1\n</pre> show_image('img-features') # Figure 1 In\u00a0[6]: Copied! <pre># 1. Download SQL for views, as well as authenticate BigQuery client\n\n# Read SA credentials for BQ\nf = io.BytesIO()\nf.write(requests.get(gcloud_url.format(gcloud_ids['sa-creds'])).content)\ncreds = json.loads(f.getvalue().decode())\n# Authorize and close\nclient = bigquery.Client.from_service_account_info(creds)\nf.close()\n\n# Read view SQL files\nf = io.BytesIO()\nf.write(requests.get(gcloud_url.format(gcloud_ids['mimic-views'])).content)\n# Parse from memory into dict\nview_data = {}\nwith zipfile.ZipFile(f, 'r') as zipf:\n    for file in zipf.namelist():\n        # Create file name and data from decoded zipf bytes\n        fname = os.path.basename(file)[:-4].lower() #Last 4 chars are .sql\n        # Store into StringIO for pandas to read\n        data = zipf.read(file).decode()\n        view_data[fname] = data\nf.close()\n\n\n# Download model feature SQL files\nf = io.BytesIO()\nf.write(requests.get(gcloud_url.format(gcloud_ids['model-features'])).content)\n# Parse from memory into dict\nmodel_views_data = {}\nwith zipfile.ZipFile(f, 'r') as zipf:\n    for file in zipf.namelist():\n        # Create file name and data from decoded zip bytes\n        fname = os.path.basename(file)[:-4].lower() #Last 4 chars are .sql\n        # Store into StringIO for pandas to read\n        data = zipf.read(file).decode()\n        model_views_data[fname] = data\nf.close()\n</pre> # 1. Download SQL for views, as well as authenticate BigQuery client  # Read SA credentials for BQ f = io.BytesIO() f.write(requests.get(gcloud_url.format(gcloud_ids['sa-creds'])).content) creds = json.loads(f.getvalue().decode()) # Authorize and close client = bigquery.Client.from_service_account_info(creds) f.close()  # Read view SQL files f = io.BytesIO() f.write(requests.get(gcloud_url.format(gcloud_ids['mimic-views'])).content) # Parse from memory into dict view_data = {} with zipfile.ZipFile(f, 'r') as zipf:     for file in zipf.namelist():         # Create file name and data from decoded zipf bytes         fname = os.path.basename(file)[:-4].lower() #Last 4 chars are .sql         # Store into StringIO for pandas to read         data = zipf.read(file).decode()         view_data[fname] = data f.close()   # Download model feature SQL files f = io.BytesIO() f.write(requests.get(gcloud_url.format(gcloud_ids['model-features'])).content) # Parse from memory into dict model_views_data = {} with zipfile.ZipFile(f, 'r') as zipf:     for file in zipf.namelist():         # Create file name and data from decoded zip bytes         fname = os.path.basename(file)[:-4].lower() #Last 4 chars are .sql         # Store into StringIO for pandas to read         data = zipf.read(file).decode()         model_views_data[fname] = data f.close() In\u00a0[7]: Copied! <pre># 2. Drop and reset derived and feature datasets\n\n# Create datasets\nfor dname in [os.environ[\"BQ_DATASET_DERIVED\"], os.environ[\"BQ_DATASET_FEATURES\"]]:\n    # Get a list of datasets\n    datasets = list(client.list_datasets())  # Make an API request.\n    project = client.project\n    dataset_id = \"{}.{}\".format(client.project, dname)\n\n    # Check if the dataset already exists\n    dataset_already_exists = dname in [d.dataset_id for d in datasets]\n\n    # Delete if it exists already\n    if dataset_already_exists:\n        client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)  # Make an API request.\n        print(\"Deleted dataset '{}'\".format(dataset_id))\n\n    # Create new dataset\n    dataset = bigquery.Dataset(dataset_id)\n    dataset.location = \"US\"\n\n    # Send the dataset to the API for creation, with an explicit timeout.\n    # Raises google.api_core.exceptions.Conflict if the Dataset already\n    # exists within the project.\n    dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n    print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n</pre> # 2. Drop and reset derived and feature datasets  # Create datasets for dname in [os.environ[\"BQ_DATASET_DERIVED\"], os.environ[\"BQ_DATASET_FEATURES\"]]:     # Get a list of datasets     datasets = list(client.list_datasets())  # Make an API request.     project = client.project     dataset_id = \"{}.{}\".format(client.project, dname)      # Check if the dataset already exists     dataset_already_exists = dname in [d.dataset_id for d in datasets]      # Delete if it exists already     if dataset_already_exists:         client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)  # Make an API request.         print(\"Deleted dataset '{}'\".format(dataset_id))      # Create new dataset     dataset = bigquery.Dataset(dataset_id)     dataset.location = \"US\"      # Send the dataset to the API for creation, with an explicit timeout.     # Raises google.api_core.exceptions.Conflict if the Dataset already     # exists within the project.     dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.     print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id)) <pre>Deleted dataset 'dlh-project-418923.mimiciii_derived'\nCreated dataset dlh-project-418923.mimiciii_derived\nDeleted dataset 'dlh-project-418923.models_features'\nCreated dataset dlh-project-418923.models_features\n</pre> In\u00a0[8]: Copied! <pre># 3. Create views in BQ from raw data tables\n\n# Create views\nfor view_name, sql in view_data.items():\n    if 'sofa' in view_name: continue #Save sofa for last\n    if 'sapsii' in view_name: continue #Save sapsii for last\n\n    # Specify view ID\n    view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_DERIVED\"], view_name)\n    view = bigquery.Table(view_id)\n    view.view_query = sql\n\n    # Make an API request to create the view.\n    view = client.create_table(view)\n    print(f\"Created {view.table_type}: {str(view.reference)}\")\n\n# Create sofa view\nview_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_DERIVED\"], 'sofa')\nview = bigquery.Table(view_id)\nview.view_query = view_data['sofa']\n# Make an API request to create the view.\nview = client.create_table(view)\nprint(f\"Created {view.table_type}: {str(view.reference)}\")\n\n\n# Create sapsii view\nview_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_DERIVED\"], 'sapsii')\nview = bigquery.Table(view_id)\nview.view_query = view_data['sapsii']\n# Make an API request to create the view.\nview = client.create_table(view)\nprint(f\"Created {view.table_type}: {str(view.reference)}\")\n</pre> # 3. Create views in BQ from raw data tables  # Create views for view_name, sql in view_data.items():     if 'sofa' in view_name: continue #Save sofa for last     if 'sapsii' in view_name: continue #Save sapsii for last      # Specify view ID     view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_DERIVED\"], view_name)     view = bigquery.Table(view_id)     view.view_query = sql      # Make an API request to create the view.     view = client.create_table(view)     print(f\"Created {view.table_type}: {str(view.reference)}\")  # Create sofa view view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_DERIVED\"], 'sofa') view = bigquery.Table(view_id) view.view_query = view_data['sofa'] # Make an API request to create the view. view = client.create_table(view) print(f\"Created {view.table_type}: {str(view.reference)}\")   # Create sapsii view view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_DERIVED\"], 'sapsii') view = bigquery.Table(view_id) view.view_query = view_data['sapsii'] # Make an API request to create the view. view = client.create_table(view) print(f\"Created {view.table_type}: {str(view.reference)}\") <pre>Created VIEW: dlh-project-418923.mimiciii_derived.blood_gas_first_day\nCreated VIEW: dlh-project-418923.mimiciii_derived.blood_gas_first_day_arterial\nCreated VIEW: dlh-project-418923.mimiciii_derived.echo_data\nCreated VIEW: dlh-project-418923.mimiciii_derived.elixhauser_ahrq_v37\nCreated VIEW: dlh-project-418923.mimiciii_derived.explicit_sepsis\nCreated VIEW: dlh-project-418923.mimiciii_derived.gcs_first_day\nCreated VIEW: dlh-project-418923.mimiciii_derived.labs_first_day\nCreated VIEW: dlh-project-418923.mimiciii_derived.urine_output_first_day\nCreated VIEW: dlh-project-418923.mimiciii_derived.ventilation_classifications\nCreated VIEW: dlh-project-418923.mimiciii_derived.ventilation_durations\nCreated VIEW: dlh-project-418923.mimiciii_derived.vitals_first_day\nCreated VIEW: dlh-project-418923.mimiciii_derived.sofa\nCreated VIEW: dlh-project-418923.mimiciii_derived.sapsii\n</pre> In\u00a0[9]: Copied! <pre># 4. Create feature-selection views from all derived views and raw data tables\n\n# Create model feature views\nfor view_name, sql in model_views_data.items():\n    # Specify view ID\n    view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_FEATURES\"], view_name)\n    view = bigquery.Table(view_id)\n    view.view_query = sql\n\n    # Make an API request to create the view.\n    view = client.create_table(view)\n    print(f\"Created {view.table_type}: {str(view.reference)}\")\n</pre> # 4. Create feature-selection views from all derived views and raw data tables  # Create model feature views for view_name, sql in model_views_data.items():     # Specify view ID     view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_FEATURES\"], view_name)     view = bigquery.Table(view_id)     view.view_query = sql      # Make an API request to create the view.     view = client.create_table(view)     print(f\"Created {view.table_type}: {str(view.reference)}\") <pre>Created VIEW: dlh-project-418923.models_features.logreg_features\nCreated VIEW: dlh-project-418923.models_features.sapsii_features\nCreated VIEW: dlh-project-418923.models_features.xgboost_features\n</pre> In\u00a0[10]: Copied! <pre># 5. Download data from the respective view for all models\n\n# Get logistic regression model data\nview_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_FEATURES\"], os.environ[\"LOG_REG_FEATURES_VIEW\"])\nview = client.get_table(view_id)\nquery_job = client.query(view.view_query)\ndf_logreg = query_job.result().to_dataframe()\nprint(f'Successfully downloaded data for logistic regression - shape {df_logreg.shape}')\n\n# Get XGBoost model data\nview_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_FEATURES\"], os.environ[\"XGBOOST_FEATURES_VIEW\"])\nview = client.get_table(view_id)\nquery_job = client.query(view.view_query)\ndf_xgb = query_job.result().to_dataframe()\nprint(f'Successfully downloaded data for XGBoost - shape {df_xgb.shape}')\n\n# Get SAPS-II model data\nview_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_FEATURES\"], os.environ[\"SAPSII_FEATURES_VIEW\"])\nview = client.get_table(view_id)\nquery_job = client.query(view.view_query)\ndf_sapsii = query_job.result().to_dataframe()\nprint(f'Successfully downloaded data for SAPS-II - shape {df_sapsii.shape}')\n</pre> # 5. Download data from the respective view for all models  # Get logistic regression model data view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_FEATURES\"], os.environ[\"LOG_REG_FEATURES_VIEW\"]) view = client.get_table(view_id) query_job = client.query(view.view_query) df_logreg = query_job.result().to_dataframe() print(f'Successfully downloaded data for logistic regression - shape {df_logreg.shape}')  # Get XGBoost model data view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_FEATURES\"], os.environ[\"XGBOOST_FEATURES_VIEW\"]) view = client.get_table(view_id) query_job = client.query(view.view_query) df_xgb = query_job.result().to_dataframe() print(f'Successfully downloaded data for XGBoost - shape {df_xgb.shape}')  # Get SAPS-II model data view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_FEATURES\"], os.environ[\"SAPSII_FEATURES_VIEW\"]) view = client.get_table(view_id) query_job = client.query(view.view_query) df_sapsii = query_job.result().to_dataframe() print(f'Successfully downloaded data for SAPS-II - shape {df_sapsii.shape}') <pre>Successfully downloaded data for logistic regression - shape (2320, 30)\nSuccessfully downloaded data for XGBoost - shape (2222, 15)\nSuccessfully downloaded data for SAPS-II - shape (2019, 22)\n</pre> In\u00a0[11]: Copied! <pre># 6. Drop records that are missing all non-target data\n# For logistic regression, we cannot have any null values, so any records with null values will be dropped\n\ndf_logreg.dropna(how='any', inplace=True)\nprint(f'Successfully purged null values for logistic regression - shape {df_logreg.shape}')\n\ndf_xgb.dropna(thresh=len(df_xgb.columns)-1, inplace=True)\nprint(f'Successfully purged null values data for XGBoost - shape {df_xgb.shape}')\n\ndf_sapsii.dropna(thresh=len(df_sapsii.columns)-1, inplace=True)\nprint(f'Successfully purged null values data for SAPS-II - shape {df_sapsii.shape}')\n</pre> # 6. Drop records that are missing all non-target data # For logistic regression, we cannot have any null values, so any records with null values will be dropped  df_logreg.dropna(how='any', inplace=True) print(f'Successfully purged null values for logistic regression - shape {df_logreg.shape}')  df_xgb.dropna(thresh=len(df_xgb.columns)-1, inplace=True) print(f'Successfully purged null values data for XGBoost - shape {df_xgb.shape}')  df_sapsii.dropna(thresh=len(df_sapsii.columns)-1, inplace=True) print(f'Successfully purged null values data for SAPS-II - shape {df_sapsii.shape}') <pre>Successfully purged null values for logistic regression - shape (2021, 30)\nSuccessfully purged null values data for XGBoost - shape (2193, 15)\nSuccessfully purged null values data for SAPS-II - shape (1670, 22)\n</pre> In\u00a0[12]: Copied! <pre># 7. Drop non-feature columns\nnon_feature_cols = ['subject_id', 'hadm_id', 'icustay_id']\n\ndf_logreg.drop(non_feature_cols, axis=1, inplace=True)\ndf_xgb.drop(non_feature_cols, axis=1, inplace=True)\ndf_sapsii.drop(non_feature_cols, axis=1, inplace=True)\n</pre> # 7. Drop non-feature columns non_feature_cols = ['subject_id', 'hadm_id', 'icustay_id']  df_logreg.drop(non_feature_cols, axis=1, inplace=True) df_xgb.drop(non_feature_cols, axis=1, inplace=True) df_sapsii.drop(non_feature_cols, axis=1, inplace=True) In\u00a0[13]: Copied! <pre># 8. Feature normalization using sklearn MinMaxScaler, in order to scale data between 0 and 1 and prevent negative values\n# We will not be scaling SAPS-II values since the probability is already calculcated, and that is what will be used for predicting.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Scale logreg features\nscaler = MinMaxScaler()\ndf_logreg[df_logreg.columns] = scaler.fit_transform(df_logreg[df_logreg.columns])\n\n# Scale xgb features\nscaler = MinMaxScaler()\ndf_xgb[df_xgb.columns] = scaler.fit_transform(df_xgb[df_xgb.columns])\n</pre> # 8. Feature normalization using sklearn MinMaxScaler, in order to scale data between 0 and 1 and prevent negative values # We will not be scaling SAPS-II values since the probability is already calculcated, and that is what will be used for predicting.  from sklearn.preprocessing import MinMaxScaler  # Scale logreg features scaler = MinMaxScaler() df_logreg[df_logreg.columns] = scaler.fit_transform(df_logreg[df_logreg.columns])  # Scale xgb features scaler = MinMaxScaler() df_xgb[df_xgb.columns] = scaler.fit_transform(df_xgb[df_xgb.columns]) In\u00a0[14]: Copied! <pre># 9. Create train and test splits for data\n\nfrom sklearn.model_selection import train_test_split\n\n# Set test size to 15% of the dataset\ntest_size = 0.1\n\n# Create logistic regression X and y data\nX_logreg = df_logreg.drop('target', axis=1)\ny_logreg = df_logreg[['target']]\nX_train_logreg, X_test_logreg, y_train_logreg, y_test_logreg = train_test_split(X_logreg, y_logreg, test_size=test_size, random_state=RANDOM_STATE)\n\n# Create xgboost X and y data\nX_xgb = df_xgb.drop('target', axis=1)\ny_xgb = df_xgb[['target']]\nX_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_xgb, y_xgb, test_size=test_size, random_state=RANDOM_STATE)\n\n# Create SAPSII X, y_true, and y_pred datasets\nX_sapsii = df_sapsii.drop('target', axis=1)\ny_true_sapsii = df_sapsii[['target']]\ny_pred_sapsii = df_sapsii['sapsii_prob']\n\n# Check sizes\nprint('Logistic regression data shape\\n---------------')\nprint(f'\\tX: {X_logreg.shape}')\nprint(f'\\ty: {y_logreg.shape}')\nprint()\nprint('XGBoost data shape\\n---------------')\nprint(f'\\tX: {X_xgb.shape}')\nprint(f'\\ty: {y_xgb.shape}')\nprint('SAPS-II data shape\\n---------------')\nprint(f'\\ty: {df_sapsii.shape}')\n</pre> # 9. Create train and test splits for data  from sklearn.model_selection import train_test_split  # Set test size to 15% of the dataset test_size = 0.1  # Create logistic regression X and y data X_logreg = df_logreg.drop('target', axis=1) y_logreg = df_logreg[['target']] X_train_logreg, X_test_logreg, y_train_logreg, y_test_logreg = train_test_split(X_logreg, y_logreg, test_size=test_size, random_state=RANDOM_STATE)  # Create xgboost X and y data X_xgb = df_xgb.drop('target', axis=1) y_xgb = df_xgb[['target']] X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_xgb, y_xgb, test_size=test_size, random_state=RANDOM_STATE)  # Create SAPSII X, y_true, and y_pred datasets X_sapsii = df_sapsii.drop('target', axis=1) y_true_sapsii = df_sapsii[['target']] y_pred_sapsii = df_sapsii['sapsii_prob']  # Check sizes print('Logistic regression data shape\\n---------------') print(f'\\tX: {X_logreg.shape}') print(f'\\ty: {y_logreg.shape}') print() print('XGBoost data shape\\n---------------') print(f'\\tX: {X_xgb.shape}') print(f'\\ty: {y_xgb.shape}') print('SAPS-II data shape\\n---------------') print(f'\\ty: {df_sapsii.shape}') <pre>Logistic regression data shape\n---------------\n\tX: (2021, 26)\n\ty: (2021, 1)\n\nXGBoost data shape\n---------------\n\tX: (2193, 11)\n\ty: (2193, 1)\nSAPS-II data shape\n---------------\n\ty: (1670, 19)\n</pre> <p>Sizes can vary slightly between the number of valid records between models because the features selected for each model are different, so there can be more or less records with missing values between the two models.</p> In\u00a0[15]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\n\n# Set parameters for logistic regression\nparams_logreg = {}\nparams_logreg['penalty'] = 'l2'\nparams_logreg['n_jobs'] = None\nparams_logreg['random_state'] = RANDOM_STATE\n\n# Create parameters for XGBoost\nparams_xgb = {}\nparams_xgb['booster'] = 'gbtree'\nparams_xgb['objective'] = 'binary:logistic'\nparams_xgb[\"eval_metric\"] = \"auc\"\nparams_xgb['eta'] = 0.3\nparams_xgb['gamma'] = 0\nparams_xgb['max_depth'] = 6\nparams_xgb['min_child_weight'] = 1\nparams_xgb['max_delta_step'] = 0\nparams_xgb['subsample'] = 1\nparams_xgb['colsample_bytree'] = 1\nparams_xgb['base_score'] = 0.5\nparams_xgb['silent'] = 1\nparams_xgb['seed'] = RANDOM_STATE\n</pre> from sklearn.linear_model import LogisticRegression import xgboost as xgb  # Set parameters for logistic regression params_logreg = {} params_logreg['penalty'] = 'l2' params_logreg['n_jobs'] = None params_logreg['random_state'] = RANDOM_STATE  # Create parameters for XGBoost params_xgb = {} params_xgb['booster'] = 'gbtree' params_xgb['objective'] = 'binary:logistic' params_xgb[\"eval_metric\"] = \"auc\" params_xgb['eta'] = 0.3 params_xgb['gamma'] = 0 params_xgb['max_depth'] = 6 params_xgb['min_child_weight'] = 1 params_xgb['max_delta_step'] = 0 params_xgb['subsample'] = 1 params_xgb['colsample_bytree'] = 1 params_xgb['base_score'] = 0.5 params_xgb['silent'] = 1 params_xgb['seed'] = RANDOM_STATE In\u00a0[16]: Copied! <pre># Create models\nmodel_logreg = LogisticRegression(**params_logreg)\nmodel_xgb = xgb.XGBRegressor(**params_xgb)\n</pre> # Create models model_logreg = LogisticRegression(**params_logreg) model_xgb = xgb.XGBRegressor(**params_xgb) In\u00a0[17]: Copied! <pre># Train models\nmodel_logreg.fit(X_train_logreg, y_train_logreg)\nmodel_xgb.fit(X_train_xgb, y_train_xgb)\n</pre> # Train models model_logreg.fit(X_train_logreg, y_train_logreg) model_xgb.fit(X_train_xgb, y_train_xgb) <pre>/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [19:03:49] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"silent\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n</pre> Out[17]: <pre>XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None, colsample_bytree=1,\n             device=None, early_stopping_rounds=None, enable_categorical=False,\n             eta=0.3, eval_metric='auc', feature_types=None, gamma=0,\n             grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=0,\n             max_depth=6, max_leaves=None, min_child_weight=1, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=None,\n             n_jobs=None, num_parallel_tree=None, ...)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressor<pre>XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None, colsample_bytree=1,\n             device=None, early_stopping_rounds=None, enable_categorical=False,\n             eta=0.3, eval_metric='auc', feature_types=None, gamma=0,\n             grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=0,\n             max_depth=6, max_leaves=None, min_child_weight=1, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=None,\n             n_jobs=None, num_parallel_tree=None, ...)</pre> In\u00a0[18]: Copied! <pre>from sklearn import metrics\n\n# Get probabilistic predictions from each model and use these\npredict_logreg = model_logreg.predict_proba(X_test_logreg)[:, 1]\npredict_xgb = model_xgb.predict(X_test_xgb)\npredict_sapsii = y_pred_sapsii\n\n# Get classifications for each\npredict_logreg_clf = np.where(predict_logreg &gt;= 0.5, 1, 0)\npredict_xgb_clf = np.where(predict_xgb &gt;= 0.5, 1, 0)\npredict_sapsii_clf = np.where(predict_sapsii &gt;= 0.5, 1, 0)\n</pre> from sklearn import metrics  # Get probabilistic predictions from each model and use these predict_logreg = model_logreg.predict_proba(X_test_logreg)[:, 1] predict_xgb = model_xgb.predict(X_test_xgb) predict_sapsii = y_pred_sapsii  # Get classifications for each predict_logreg_clf = np.where(predict_logreg &gt;= 0.5, 1, 0) predict_xgb_clf = np.where(predict_xgb &gt;= 0.5, 1, 0) predict_sapsii_clf = np.where(predict_sapsii &gt;= 0.5, 1, 0) In\u00a0[19]: Copied! <pre># Get precision, recall, and macro fbeta scores\nlogreg_metrics = list(metrics.precision_recall_fscore_support(y_test_logreg, predict_logreg_clf, average='macro'))[:-1]\nxgb_metrics = list(metrics.precision_recall_fscore_support(y_test_xgb, predict_xgb_clf, average='macro'))[:-1]\nsapsii_metrics = list(metrics.precision_recall_fscore_support(y_true_sapsii, predict_sapsii_clf, average='macro'))[:-1]\n\n# Get accuracy scores\nlogreg_metrics.append(metrics.accuracy_score(y_test_logreg, predict_logreg_clf))\nxgb_metrics.append(metrics.accuracy_score(y_test_xgb, predict_xgb_clf))\nsapsii_metrics.append(metrics.accuracy_score(y_true_sapsii, predict_sapsii_clf))\n\n# Get confusion matrix\ncm_logreg = metrics.confusion_matrix(y_test_logreg, predict_logreg_clf)\ncm_xgb = metrics.confusion_matrix(y_test_xgb, predict_xgb_clf)\ncm_sapsii = metrics.confusion_matrix(y_true_sapsii, predict_sapsii_clf)\n</pre> # Get precision, recall, and macro fbeta scores logreg_metrics = list(metrics.precision_recall_fscore_support(y_test_logreg, predict_logreg_clf, average='macro'))[:-1] xgb_metrics = list(metrics.precision_recall_fscore_support(y_test_xgb, predict_xgb_clf, average='macro'))[:-1] sapsii_metrics = list(metrics.precision_recall_fscore_support(y_true_sapsii, predict_sapsii_clf, average='macro'))[:-1]  # Get accuracy scores logreg_metrics.append(metrics.accuracy_score(y_test_logreg, predict_logreg_clf)) xgb_metrics.append(metrics.accuracy_score(y_test_xgb, predict_xgb_clf)) sapsii_metrics.append(metrics.accuracy_score(y_true_sapsii, predict_sapsii_clf))  # Get confusion matrix cm_logreg = metrics.confusion_matrix(y_test_logreg, predict_logreg_clf) cm_xgb = metrics.confusion_matrix(y_test_xgb, predict_xgb_clf) cm_sapsii = metrics.confusion_matrix(y_true_sapsii, predict_sapsii_clf) In\u00a0[20]: Copied! <pre># Get ROC curves\nfpr_logreg, tpr_logreg, _ = metrics.roc_curve(y_test_logreg, predict_logreg)\nfpr_xgb, tpr_xgb, _ = metrics.roc_curve(y_test_xgb, predict_xgb)\nfpr_sapsii, tpr_sapsii, _ = metrics.roc_curve(y_true_sapsii, y_pred_sapsii)\n\n# Get AUC scores\nlogreg_metrics.append(metrics.roc_auc_score(y_test_logreg, predict_logreg))\nxgb_metrics.append(metrics.roc_auc_score(y_test_xgb, predict_xgb))\nsapsii_metrics.append(metrics.roc_auc_score(y_true_sapsii, y_pred_sapsii))\n</pre> # Get ROC curves fpr_logreg, tpr_logreg, _ = metrics.roc_curve(y_test_logreg, predict_logreg) fpr_xgb, tpr_xgb, _ = metrics.roc_curve(y_test_xgb, predict_xgb) fpr_sapsii, tpr_sapsii, _ = metrics.roc_curve(y_true_sapsii, y_pred_sapsii)  # Get AUC scores logreg_metrics.append(metrics.roc_auc_score(y_test_logreg, predict_logreg)) xgb_metrics.append(metrics.roc_auc_score(y_test_xgb, predict_xgb)) sapsii_metrics.append(metrics.roc_auc_score(y_true_sapsii, y_pred_sapsii)) In\u00a0[21]: Copied! <pre># Get the DCA for the models\n\n# Calculate the net benefit based on a given threshold\ndef net_benefit(tp, fp, tn, fn, threshold):\n    benefit = tp - (fp * threshold / (1 - threshold))\n    total = tp + fn  # Total number of actual positives\n    return benefit / total\n\n# DCA\ndef decision_curve_analysis(y_true, y_prob):\n    thresholds = np.linspace(0.01, 0.99, 100)\n    net_benefits = []\n    for threshold in thresholds:\n      y_pred = np.where(y_prob &gt;= threshold, 1, 0)\n      tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n      nb = net_benefit(tp, fp, tn, fn, threshold)\n      net_benefits.append(nb)\n\n    return thresholds, net_benefits\n</pre> # Get the DCA for the models  # Calculate the net benefit based on a given threshold def net_benefit(tp, fp, tn, fn, threshold):     benefit = tp - (fp * threshold / (1 - threshold))     total = tp + fn  # Total number of actual positives     return benefit / total  # DCA def decision_curve_analysis(y_true, y_prob):     thresholds = np.linspace(0.01, 0.99, 100)     net_benefits = []     for threshold in thresholds:       y_pred = np.where(y_prob &gt;= threshold, 1, 0)       tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()       nb = net_benefit(tp, fp, tn, fn, threshold)       net_benefits.append(nb)      return thresholds, net_benefits <p>Looking at results, the XGBoost model performs slightly better than the logistic regression model. Through research, the deterministic nature of XGBoost can usually lead to repeated results, although results have varied slightly in the past.</p> <p>For <code>precision</code>, <code>recall</code>, <code>f1</code>, <code>accuracy</code>, and <code>AUC</code>, XGBoost performs the best, with <code>recall</code>, <code>f1</code>, and <code>accuracy</code> being a sizable difference, and <code>precision</code> and <code>AUC</code> being similar to logistic regression.</p> <p>For the SAPS-II model, all metrics were signficicantly outperformed by the Logistic Regression model, and the XGBoost model. The ROC curve for the SAPS-II model also indicated inferior predictive performance compared to the other models.</p> In\u00a0[22]: Copied! <pre># Display results for metric scores as a dataframe\n# Create dict out of these\ndata = {'metric': ['precision', 'recall', 'f1', 'accuracy', 'AUC'], 'LogReg': logreg_metrics, 'XGBoost': xgb_metrics, 'SAPS-II': sapsii_metrics}\ndf_metrics = pd.DataFrame(data).round(4)\ndf_metrics.set_index('metric', drop=True, inplace=True)\n\n# Display data\ndisplay(df_metrics)\n</pre> # Display results for metric scores as a dataframe # Create dict out of these data = {'metric': ['precision', 'recall', 'f1', 'accuracy', 'AUC'], 'LogReg': logreg_metrics, 'XGBoost': xgb_metrics, 'SAPS-II': sapsii_metrics} df_metrics = pd.DataFrame(data).round(4) df_metrics.set_index('metric', drop=True, inplace=True)  # Display data display(df_metrics) LogReg XGBoost SAPS-II metric precision 0.7018 0.6610 0.4937 recall 0.6548 0.6540 0.4900 f1 0.6590 0.6569 0.4623 accuracy 0.7094 0.7000 0.5473 AUC 0.7281 0.7145 0.4885 In\u00a0[23]: Copied! <pre># Plot AUROC\n# plt.figure()\n# plt.plot(fpr_logreg, tpr_logreg, label='LogReg')\n# plt.plot(fpr_xgb, tpr_xgb, label='XGBoost')\n# plt.plot(fpr_sapsii, tpr_sapsii, label='SAPS-II')\n\n# # Create plot\n# plt.title('AUROC')\n# plt.xlabel('FP Rate')\n# plt.ylabel('TP Rate')\n# plt.legend()\n# plt.grid()\n# plt.show()\n\n# Show my image for faster processing time\nshow_image('my-roc', resize=True)\n</pre> # Plot AUROC # plt.figure() # plt.plot(fpr_logreg, tpr_logreg, label='LogReg') # plt.plot(fpr_xgb, tpr_xgb, label='XGBoost') # plt.plot(fpr_sapsii, tpr_sapsii, label='SAPS-II')  # # Create plot # plt.title('AUROC') # plt.xlabel('FP Rate') # plt.ylabel('TP Rate') # plt.legend() # plt.grid() # plt.show()  # Show my image for faster processing time show_image('my-roc', resize=True) In\u00a0[24]: Copied! <pre># Actually calculate the DCA scores\nthresholds_logreg, net_benefits_logreg = decision_curve_analysis(y_test_logreg, predict_logreg)\nthresholds_xgb, net_benefits_xgb = decision_curve_analysis(y_test_xgb, predict_xgb)\nthresholds_sapsii, net_benefits_sapsii = decision_curve_analysis(y_true_sapsii, y_pred_sapsii)\n\nplt.figure()\nplt.plot(thresholds_logreg, net_benefits_logreg, label='LogReg')\nplt.plot(thresholds_xgb, net_benefits_xgb, label='XGBoost')\nplt.plot(thresholds_sapsii, net_benefits_sapsii, label='SAPS-II')\n\nplt.xlabel('Threshold Probability')\nplt.ylabel('Net Benefit')\nplt.title('Decision Curve Analysis')\nplt.legend()\nplt.grid()\nplt.show()\n</pre> # Actually calculate the DCA scores thresholds_logreg, net_benefits_logreg = decision_curve_analysis(y_test_logreg, predict_logreg) thresholds_xgb, net_benefits_xgb = decision_curve_analysis(y_test_xgb, predict_xgb) thresholds_sapsii, net_benefits_sapsii = decision_curve_analysis(y_true_sapsii, y_pred_sapsii)  plt.figure() plt.plot(thresholds_logreg, net_benefits_logreg, label='LogReg') plt.plot(thresholds_xgb, net_benefits_xgb, label='XGBoost') plt.plot(thresholds_sapsii, net_benefits_sapsii, label='SAPS-II')  plt.xlabel('Threshold Probability') plt.ylabel('Net Benefit') plt.title('Decision Curve Analysis') plt.legend() plt.grid() plt.show() In\u00a0[25]: Copied! <pre>print('From L. et al. [1]')\nshow_image('graph-roc', resize=True)\nprint('From this study')\nshow_image('my-roc', resize=True)\n</pre> print('From L. et al. [1]') show_image('graph-roc', resize=True) print('From this study') show_image('my-roc', resize=True) <pre>From L. et al. [1]\n</pre> <pre>From this study\n</pre> In\u00a0[26]: Copied! <pre>print('From L. et al. [1]')\nshow_image('graph-dca', resize=True)\nprint('From this study')\nshow_image('my-dca', resize=True)\n</pre> print('From L. et al. [1]') show_image('graph-dca', resize=True) print('From this study') show_image('my-dca', resize=True) <pre>From L. et al. [1]\n</pre> <pre>From this study\n</pre> In\u00a0[27]: Copied! <pre>print('From L. et al. [1]')\nshow_image('graph-nomogram', resize=True)\n</pre> print('From L. et al. [1]') show_image('graph-nomogram', resize=True) <pre>From L. et al. [1]\n</pre> In\u00a0[28]: Copied! <pre># Get feature importance for the Logistic Regression model\nfeature_importance_logreg_old = zip(X_train_logreg.columns, np.abs(model_logreg.coef_[0]))\nfeature_importance_logreg_old = {k: v for k, v in feature_importance_logreg_old}\nfeature_importance_xgb_old = model_xgb.get_booster().get_score(importance_type='gain')\n</pre> # Get feature importance for the Logistic Regression model feature_importance_logreg_old = zip(X_train_logreg.columns, np.abs(model_logreg.coef_[0])) feature_importance_logreg_old = {k: v for k, v in feature_importance_logreg_old} feature_importance_xgb_old = model_xgb.get_booster().get_score(importance_type='gain') In\u00a0[29]: Copied! <pre># Drop age columns from dataframes\nX_train_logreg_abl = X_train_logreg.drop(['age', 'sodium_max'], axis=1)\nX_test_logreg_abl = X_test_logreg.drop(['age', 'sodium_max'], axis=1)\nX_train_xgb_abl = X_train_xgb.drop(['age', 'sodium_max'], axis=1)\nX_test_xgb_abl = X_test_xgb.drop(['age', 'sodium_max'], axis=1)\n</pre> # Drop age columns from dataframes X_train_logreg_abl = X_train_logreg.drop(['age', 'sodium_max'], axis=1) X_test_logreg_abl = X_test_logreg.drop(['age', 'sodium_max'], axis=1) X_train_xgb_abl = X_train_xgb.drop(['age', 'sodium_max'], axis=1) X_test_xgb_abl = X_test_xgb.drop(['age', 'sodium_max'], axis=1) In\u00a0[30]: Copied! <pre># Create models\nmodel_logreg_abl = LogisticRegression(**params_logreg)\nmodel_xgb_abl = xgb.XGBRegressor(**params_xgb)\n\n# Train models\nmodel_logreg_abl.fit(X_train_logreg_abl, y_train_logreg)\nmodel_xgb_abl.fit(X_train_xgb_abl, y_train_xgb)\n</pre> # Create models model_logreg_abl = LogisticRegression(**params_logreg) model_xgb_abl = xgb.XGBRegressor(**params_xgb)  # Train models model_logreg_abl.fit(X_train_logreg_abl, y_train_logreg) model_xgb_abl.fit(X_train_xgb_abl, y_train_xgb) <pre>/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [19:03:56] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"silent\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n</pre> Out[30]: <pre>XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None, colsample_bytree=1,\n             device=None, early_stopping_rounds=None, enable_categorical=False,\n             eta=0.3, eval_metric='auc', feature_types=None, gamma=0,\n             grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=0,\n             max_depth=6, max_leaves=None, min_child_weight=1, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=None,\n             n_jobs=None, num_parallel_tree=None, ...)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressor<pre>XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None, colsample_bytree=1,\n             device=None, early_stopping_rounds=None, enable_categorical=False,\n             eta=0.3, eval_metric='auc', feature_types=None, gamma=0,\n             grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=0,\n             max_depth=6, max_leaves=None, min_child_weight=1, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=None,\n             n_jobs=None, num_parallel_tree=None, ...)</pre> In\u00a0[31]: Copied! <pre># Get probabilistic predictions from each model and use these\npredict_logreg = model_logreg_abl.predict_proba(X_test_logreg_abl)[:, 1]\npredict_xgb = model_xgb_abl.predict(X_test_xgb_abl)\n\n# Get classifications for each\npredict_logreg_clf = np.where(predict_logreg &gt;= 0.5, 1, 0)\npredict_xgb_clf = np.where(predict_xgb &gt;= 0.5, 1, 0)\n</pre> # Get probabilistic predictions from each model and use these predict_logreg = model_logreg_abl.predict_proba(X_test_logreg_abl)[:, 1] predict_xgb = model_xgb_abl.predict(X_test_xgb_abl)  # Get classifications for each predict_logreg_clf = np.where(predict_logreg &gt;= 0.5, 1, 0) predict_xgb_clf = np.where(predict_xgb &gt;= 0.5, 1, 0) In\u00a0[32]: Copied! <pre># Get precision, recall, and macro fbeta scores\nlogreg_abl_metrics = list(metrics.precision_recall_fscore_support(y_test_logreg, predict_logreg_clf, average='macro'))[:-1]\nxgb_abl_metrics = list(metrics.precision_recall_fscore_support(y_test_xgb, predict_xgb_clf, average='macro'))[:-1]\n\n# Get accuracy scores\nlogreg_abl_metrics.append(metrics.accuracy_score(y_test_logreg, predict_logreg_clf))\nxgb_abl_metrics.append(metrics.accuracy_score(y_test_xgb, predict_xgb_clf))\n\n# Get confusion matrix\ncm_abl_logreg = metrics.confusion_matrix(y_test_logreg, predict_logreg_clf)\ncm_abl_xgb = metrics.confusion_matrix(y_test_xgb, predict_xgb_clf)\n\n# Get ROC curves\nfpr_abl_logreg, tpr_abl_logreg, _ = metrics.roc_curve(y_test_logreg, predict_logreg)\nfpr_abl_xgb, tpr_abl_xgb, _ = metrics.roc_curve(y_test_xgb, predict_xgb)\n\n# Get AUC scores\nlogreg_abl_metrics.append(metrics.roc_auc_score(y_test_logreg, predict_logreg))\nxgb_abl_metrics.append(metrics.roc_auc_score(y_test_xgb, predict_xgb))\n</pre> # Get precision, recall, and macro fbeta scores logreg_abl_metrics = list(metrics.precision_recall_fscore_support(y_test_logreg, predict_logreg_clf, average='macro'))[:-1] xgb_abl_metrics = list(metrics.precision_recall_fscore_support(y_test_xgb, predict_xgb_clf, average='macro'))[:-1]  # Get accuracy scores logreg_abl_metrics.append(metrics.accuracy_score(y_test_logreg, predict_logreg_clf)) xgb_abl_metrics.append(metrics.accuracy_score(y_test_xgb, predict_xgb_clf))  # Get confusion matrix cm_abl_logreg = metrics.confusion_matrix(y_test_logreg, predict_logreg_clf) cm_abl_xgb = metrics.confusion_matrix(y_test_xgb, predict_xgb_clf)  # Get ROC curves fpr_abl_logreg, tpr_abl_logreg, _ = metrics.roc_curve(y_test_logreg, predict_logreg) fpr_abl_xgb, tpr_abl_xgb, _ = metrics.roc_curve(y_test_xgb, predict_xgb)  # Get AUC scores logreg_abl_metrics.append(metrics.roc_auc_score(y_test_logreg, predict_logreg)) xgb_abl_metrics.append(metrics.roc_auc_score(y_test_xgb, predict_xgb)) In\u00a0[33]: Copied! <pre># Display results for metric scores as a dataframe\n# Create dict out of these\ndata = {'metric': ['precision', 'recall', 'f1', 'accuracy', 'AUC'],\n        'LogReg': logreg_metrics, 'LogReg_Ablation': logreg_abl_metrics,\n        'XGBoost': xgb_metrics, 'XGBoost_Ablation': xgb_abl_metrics,\n        'SAPS-II': sapsii_metrics}\ndf_metrics = pd.DataFrame(data).round(4)\ndf_metrics.set_index('metric', drop=True, inplace=True)\n\n# Display data\ndisplay(df_metrics)\n</pre> # Display results for metric scores as a dataframe # Create dict out of these data = {'metric': ['precision', 'recall', 'f1', 'accuracy', 'AUC'],         'LogReg': logreg_metrics, 'LogReg_Ablation': logreg_abl_metrics,         'XGBoost': xgb_metrics, 'XGBoost_Ablation': xgb_abl_metrics,         'SAPS-II': sapsii_metrics} df_metrics = pd.DataFrame(data).round(4) df_metrics.set_index('metric', drop=True, inplace=True)  # Display data display(df_metrics) LogReg LogReg_Ablation XGBoost XGBoost_Ablation SAPS-II metric precision 0.7018 0.6905 0.6610 0.6484 0.4937 recall 0.6548 0.6418 0.6540 0.6372 0.4900 f1 0.6590 0.6443 0.6569 0.6410 0.4623 accuracy 0.7094 0.6995 0.7000 0.6909 0.5473 AUC 0.7281 0.7323 0.7145 0.7201 0.4885 <p>From the above table, we can see actual improvements from the ablations! This could be a result of several different possibilities - one could be that the features removed were actually causing more noise and making pattern detection harder for the models. Another possibility is that reducing the number of dimensions helps the models focus more clearly on other feature splits.</p> <p>Regardless, we can see significant improvement in the XGBoost model across the board - every single metric improved for this mdoel. For the linear regression model, results were fairly similar to the model without any features missing, showing non-significant results.</p> <p>Below, we can see the new feature importance after features have been removed. From the \"Diff\" column, we can see for Logistic Regression that <code>bun_min</code>, <code>bun_max</code>, <code>heartrate_min</code>, <code>heartrate_mean</code> <code>sysbp_min</code>, <code>tempc_min</code>, and <code>spo2_mean</code> all had large changes in feature importance from the ablation study.</p> <p>For the XGBoost model, we can see <code>urineoutput</code> and <code>metastic_cancer</code> had larger impacts on feature selection after performing the ablation.</p> In\u00a0[34]: Copied! <pre># Get feature importance for the Logistic Regression model\nfeature_importance_logreg = zip(X_train_logreg_abl.columns, np.abs(model_logreg_abl.coef_[0]))\nfeature_importance_logreg = {k: v for k, v in feature_importance_logreg}\nfeature_importance_xgb = model_xgb_abl.get_booster().get_score(importance_type='gain')\n</pre> # Get feature importance for the Logistic Regression model feature_importance_logreg = zip(X_train_logreg_abl.columns, np.abs(model_logreg_abl.coef_[0])) feature_importance_logreg = {k: v for k, v in feature_importance_logreg} feature_importance_xgb = model_xgb_abl.get_booster().get_score(importance_type='gain') In\u00a0[35]: Copied! <pre># Compare each key and see the difference for LogReg\nmax_length = max([len(str(k)) for k in feature_importance_logreg.keys()] + [len(str(k)) for k in feature_importance_xgb.keys()])\nprint('Logistic Regression')\nprint(\"Key\\t\\t\\tNormal\\tAbl.\\tDiff\")\nprint('------------')\nfor k, v in feature_importance_logreg.items():\n  old_val = feature_importance_logreg_old[k]\n  print(\"{}\\t{:.4f}\\t{:.4f}\\t{:.4f}\".format(k.ljust(max_length), v, old_val, v-old_val))\n\n\n# Compare each key and see the difference for XGBoost\nprint('\\n\\nXGBoost')\nprint(\"Key\\t\\t\\tNormal\\tAbl.\\tDiff\")\nprint('------------')\nfor k, v in feature_importance_xgb.items():\n  old_val = feature_importance_xgb_old[k]\n  print(\"{}\\t{:.4f}\\t{:.4f}\\t{:.4f}\".format(k.ljust(max_length), v, old_val, v-old_val))\n</pre> # Compare each key and see the difference for LogReg max_length = max([len(str(k)) for k in feature_importance_logreg.keys()] + [len(str(k)) for k in feature_importance_xgb.keys()]) print('Logistic Regression') print(\"Key\\t\\t\\tNormal\\tAbl.\\tDiff\") print('------------') for k, v in feature_importance_logreg.items():   old_val = feature_importance_logreg_old[k]   print(\"{}\\t{:.4f}\\t{:.4f}\\t{:.4f}\".format(k.ljust(max_length), v, old_val, v-old_val))   # Compare each key and see the difference for XGBoost print('\\n\\nXGBoost') print(\"Key\\t\\t\\tNormal\\tAbl.\\tDiff\") print('------------') for k, v in feature_importance_xgb.items():   old_val = feature_importance_xgb_old[k]   print(\"{}\\t{:.4f}\\t{:.4f}\\t{:.4f}\".format(k.ljust(max_length), v, old_val, v-old_val)) <pre>Logistic Regression\nKey\t\t\tNormal\tAbl.\tDiff\n------------\nsofa             \t2.1553\t2.1704\t-0.0151\naniongap_min     \t1.4000\t1.4062\t-0.0063\naniongap_max     \t0.2809\t0.3206\t-0.0397\ncreatinine_min   \t1.2918\t1.2603\t0.0315\nchloride_min     \t0.2303\t0.3602\t-0.1299\nhematocrit_min   \t0.9021\t0.8711\t0.0310\nhemoglobin_min   \t0.7408\t0.7124\t0.0283\nhemoglobin_max   \t0.6173\t0.6341\t-0.0168\nlactate_min      \t2.7440\t2.7394\t0.0046\npotassium_min    \t1.6426\t1.6877\t-0.0451\nbun_min          \t1.6323\t1.6241\t0.0082\nbun_max          \t0.0107\t0.0252\t-0.0145\nwbc_min          \t0.6759\t0.6638\t0.0121\nwbc_max          \t0.3559\t0.3638\t-0.0079\nheartrate_min    \t0.8797\t0.8817\t-0.0020\nheartrate_mean   \t1.1568\t1.1667\t-0.0099\nsysbp_min        \t1.4146\t1.3701\t0.0445\nmeanbp_min       \t0.7200\t0.7051\t0.0149\nresprate_mean    \t0.8502\t0.8374\t0.0128\ntempc_min        \t0.6645\t0.6804\t-0.0159\ntempc_max        \t2.3776\t2.3767\t0.0010\nspo2_mean        \t1.7387\t1.7465\t-0.0078\ndiabetes         \t0.4086\t0.4257\t-0.0171\nmechvent         \t0.7415\t0.7388\t0.0028\n\n\nXGBoost\nKey\t\t\tNormal\tAbl.\tDiff\n------------\nurineoutput      \t2.1599\t2.3094\t-0.1495\nlactate_min      \t1.6612\t1.7759\t-0.1147\nbun_mean         \t1.4006\t1.4673\t-0.0667\nmetastatic_cancer\t1.1136\t0.8119\t0.3017\ninr_max          \t1.2870\t1.4110\t-0.1240\naniongap_max     \t1.4510\t1.4747\t-0.0236\ncreatinine_min   \t1.2593\t1.2683\t-0.0090\nspo2_mean        \t1.4553\t1.5129\t-0.0576\n</pre> <p>After reviewing the hypothesis in the paper, we can draw a few tentative conclusions:</p> <ol> <li>The XGBoost model has the potential to outperform standard models in predicting 30-day mortality for sepsis patients.</li> <li>Certain features have significant important in creating these mortality predictions, while others are less important or unnecessary altogether.</li> </ol>"},{"location":"research/dlh_study/#short-term-mortality-prediction-in-icu-patients-with-sepsis-3","title":"Short term mortality prediction in ICU patients with sepsis-3\u00b6","text":"<ul> <li>Github: https://github.com/mrmattkennedy/CS598-DLH-Project</li> <li>Original study: https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-020-02620-5</li> </ul>"},{"location":"research/dlh_study/#introduction","title":"Introduction\u00b6","text":"<p>This study focuses on predicting 30-days mortality for MIMIC-III patients diagnosed with sepsis-3 using the XGBoost algorithm. This is a predictive modeling problem aiming to improve mortality prediction over traditional methods. The importance of solving this problem is aimed at tackling sepsis, which is a major cause of mortality especially in ICU patients. Early and accurate prediction of mortality in these patients is important as it can guide time-critical and appropriate treatment for patients, potentially improving survival outcomes. This study aims to enhance the predictive accuracy using machine learning, which could provide clinicians with a powerful tool for risk assessment and management.</p> <p>The complexity of sepsis, including its vague syndrome definitions, unknown sources of infection, as well as high mortality rates, makes establishing a reliable and effective prognostic model challenging. Traditional machine learning models, which are based on small sample sizes or simplistic statistical assumptions, are limited in their predictive power. Traditional methods for diagnosing sepsis include the use of serum markers and scoring systems like APHACHE-II and SAPS-II. However, these have limitations in sensitivity, specificity, and the ability to handle complex interactions within data. XGBoost has shown improved predictive performance by efficiently handling missing data and assembling weak prediction models to create a more accurate composite model.</p> <p>L. et al. [1] proposed a machine learning model using the XGBoost algorithm to predict 30-day mortality among patients with sepsis-3 in the MIMIC-III dataset, comparing its performance against traditional logistic regression and SAPS-II score models. The innovation for this study is found in applying the XGBoost algorithm, known for its efficiency with missing data and capability to enhance predictive accuracy by combining weak models. This study demonstrates the superiority of XGBoost over conventional methods in the context of sepsis mortality prediction.</p> <p>For post-results analysis and scoring, tthe XGBoost model showed superior performance with higher AUCs compared to traditional logistic regression and SAPS-II models, indicating better predictive accuracy. This study significantly contributes to the sepsis research field by showcasing the application of a machine learning algorithm to accurately predict mortality, potentially aiding clinicians in making informed decisions and tailoring patient management strategies effectively.</p>"},{"location":"research/dlh_study/#scope-of-reproducibility","title":"Scope of Reproducibility:\u00b6","text":"<p>Reproducability is possible from a high-level, but there are limitations for a few reasons:</p> <ul> <li>The dataset utilized in this study is the MIMIC-III dataset, which is too large to reproduce fully exactly as the authors used this data.</li> <li>Additionally, access to MIMIC-III is limited, and thus reproducing even certain samples and displaying data is restricted by law. Because of this, to replicate this study, no raw data can be displayed.</li> <li>While there are many aspects of the models discussed at length in this study, there are also many details missing. This is discussed in later points as well, but there are quite a few assumptions made about each model. These are pointed out, but the result is likely a model that differs significantly from the models used by L. et al. [1]</li> </ul> <p>For testable hypotheses, I believe the below hypotheses can be tested and observed from L. et al. [1].</p> <ol> <li>Hypothesis 1: The XGBoost algorithm can outperform traditional logistic regression and SAPS-II scoring models in predicting 30-day mortality among patients with sepsis-3 based on scoring metrics.</li> </ol> <ul> <li>Corresponding experiment: Create a logistic regression model, a SAPS-II dataset, and an XGBoost model, and use the features specified by L. et al. [1] for these models. Then, analyze the results of both models in predicting 30-day mortality due to sepsis-3 and compare.</li> </ul> <ol> <li>Hypothesis 2: The set of features identified by the XGBoost model as significant predictors of 30-day mortality are significant compared to features not used</li> </ol> <ul> <li>Corresponding experiment: Perform several ablations, including dropping several features and adding several others, and compare post-training scores across different models to see which features area most important in each model's decisions, and how effective each model is.</li> </ul>"},{"location":"research/dlh_study/#methodology","title":"Methodology\u00b6","text":""},{"location":"research/dlh_study/#environment","title":"Environment\u00b6","text":"<p>For this notebook, we will be using Python 3. As for dependencies, the below packages are required, and will be installed in the code cell below:</p> <ul> <li>google-cloud-bigquery: In this study replication, we will use Google BigQuery to host the data, as well as create views and preprocess data</li> <li>pandas: The pandas library is used to create a tabular object to hold the data for the models, as well as post-training data and results</li> <li>db-dtypes: This is a required accessory library to use google-cloud-bigquery</li> <li>scikit-learn: The sklearn library has a host of uses, including metrics for evaluating models, different types of models, preprocessing, and much more</li> <li>xgboost: The XGBoost library will be used to create an XGBoost model</li> <li>opencv-python: CV2 can display images for inline</li> <li>matplotlib: The Matplotlib library can create charts and graphs, which will be useful for interpreting results</li> </ul>"},{"location":"research/dlh_study/#data","title":"Data\u00b6","text":""},{"location":"research/dlh_study/#source","title":"Source\u00b6","text":"<p>The data used in this project is the MIMIC III dataset. However, due to the size of the original dataset, a subsample of roughly half of the raw original dataset was loaded into Google Bigquery. The reason BQ was chosen was due to the availability of this GitHub repo from MIT that has easily creatable views and analyses of the MIMIC datasets, and can be processed via BQ.</p> <p>The data is split into 3 datasets in BQ:</p> <ol> <li>mimiciii_clinical: All of the original, raw data. Data was sourced from Physionet</li> <li>mimiciii_derived: Any additional views created from the raw data. SQL for these views was sourced from the mimic-code library mentioned above</li> <li>mimiciii_features: Views created that contain the features for each type of model. These features were chosen from L. et al. [1][p.8] based on the author's chose features, which were, \"identifed by the results of backward stepwise analysis and strongly associated with mortality in 30 days\".</li> </ol>"},{"location":"research/dlh_study/#downloading-data","title":"Downloading data\u00b6","text":"<p>The data already exists in a BigQuery project, as to make creating views easy and working with data much faster. This script will create the necessary views from the raw data, then create feature views based on the derived views.</p> <p>For data access, downloading, and preprocessing, steps are listed in a cell later describing all the necessary code to be run.</p>"},{"location":"research/dlh_study/#statistics","title":"Statistics\u00b6","text":""},{"location":"research/dlh_study/#tables-and-views","title":"Tables and views\u00b6","text":"<p>Tables contained in 'dlh-project-418923.mimiciii_clinical':</p> <ul> <li>dlh-project-418923.mimiciii_clinical.admissions, 58976 rows</li> <li>dlh-project-418923.mimiciii_clinical.callout, 34499 rows</li> <li>dlh-project-418923.mimiciii_clinical.caregivers, 7567 rows</li> <li>dlh-project-418923.mimiciii_clinical.chartevents, 330712483 rows</li> <li>dlh-project-418923.mimiciii_clinical.cptevents, 573146 rows</li> <li>dlh-project-418923.mimiciii_clinical.d_cpt, 134 rows</li> <li>dlh-project-418923.mimiciii_clinical.d_icd_diagnoses, 14567 rows</li> <li>dlh-project-418923.mimiciii_clinical.d_icd_procedures, 3882 rows</li> <li>dlh-project-418923.mimiciii_clinical.d_items, 12487 rows</li> <li>dlh-project-418923.mimiciii_clinical.d_labitems, 753 rows</li> <li>dlh-project-418923.mimiciii_clinical.datetimeevents, 4285647 rows</li> <li>dlh-project-418923.mimiciii_clinical.diagnoses_icd, 651047 rows</li> <li>dlh-project-418923.mimiciii_clinical.drgcodes, 115557 rows</li> <li>dlh-project-418923.mimiciii_clinical.icustays, 61532 rows</li> <li>dlh-project-418923.mimiciii_clinical.inputevents_cv, 14527935 rows</li> <li>dlh-project-418923.mimiciii_clinical.inputevents_mv, 3218991 rows</li> <li>dlh-project-418923.mimiciii_clinical.labevents, 23851932 rows</li> <li>dlh-project-418923.mimiciii_clinical.microbiologyevents, 631726 rows</li> <li>dlh-project-418923.mimiciii_clinical.noteevents, 2083180 rows</li> <li>dlh-project-418923.mimiciii_clinical.outputevents, 4349218 rows</li> <li>dlh-project-418923.mimiciii_clinical.patients, 46520 rows</li> <li>dlh-project-418923.mimiciii_clinical.prescriptions, 4156450 rows</li> <li>dlh-project-418923.mimiciii_clinical.procedureevents_mv, 258066 rows</li> <li>dlh-project-418923.mimiciii_clinical.procedures_icd, 231945 rows</li> <li>dlh-project-418923.mimiciii_clinical.services, 73343 rows</li> <li>dlh-project-418923.mimiciii_clinical.transfers, 261897 rows</li> </ul> <p>Views contained in 'dlh-project-418923.mimiciii_derived', as well as descriptions:</p> <ul> <li>dlh-project-418923.mimiciii_derived.blood_gas_first_day -- Highest and lowest blood gas values in the first 24 hours of a patient's ICU stay.</li> <li>dlh-project-418923.mimiciii_derived.blood_gas_first_day_arterial -- As above, but arterial blood gases only.</li> <li>dlh-project-418923.mimiciii_derived.echo_data -- Text extracted from echocardiography reports using regular expressions.</li> <li>dlh-project-418923.mimiciii_derived.elixhauser_ahrq_v37 -- Comorbidities in categories proposed by Elixhauser et al. AHRQ produced the mapping.</li> <li>dlh-project-418923.mimiciii_derived.explicit_sepsis -- Explicitly coded sepsis (i.e. a list of patients with ICD-9 codes which refer to sepsis).</li> <li>dlh-project-418923.mimiciii_derived.gcs_first_day -- Highest and lowest Glasgow Coma Scale in the first 24 hours of a patient's ICU stay.</li> <li>dlh-project-418923.mimiciii_derived.labs_first_day -- Highest and lowest laboratory values in the first 24 hours of a patient's ICU stay.</li> <li>dlh-project-418923.mimiciii_derived.sofa -- The Sequential Organ Failure Assessment (SOFA) scale.</li> <li>dlh-project-418923.mimiciii_derived.urine_output_first_day -- Total urine output over the first 24 hours of a patient's ICU stay.</li> <li>dlh-project-418923.mimiciii_derived.ventilation_classifications -- Classifies patient settings as implying mechanical ventilation.</li> <li>dlh-project-418923.mimiciii_derived.ventilation_durations -- Start and stop times for mechanical ventilation.</li> <li>dlh-project-418923.mimiciii_derived.vitals_first_day -- Highest and lowest vital signs in the first 24 hours of a patient's ICU stay.</li> <li>dlh-project-418923.mimiciii_derived.sapsii -- Simplified Acute Physiology Score II (SAPS II)</li> </ul> <p>Views contained in 'dlh-project-418923.models_features', which are used for the 2 models in this analysis (logistic regression and XGBoost):</p> <ul> <li>dlh-project-418923.models_features.logreg_features</li> <li>dlh-project-418923.models_features.xgboost_features</li> <li>dlh-project-418923.models_features.sapsii_features</li> </ul>"},{"location":"research/dlh_study/#features-selected","title":"Features selected\u00b6","text":"<p>From L. et al. [1][p.4] regarding feature selection:</p> <p>\"Firstly, the conventional logistic regression model was conducted using these signifcant variables identifed by backward stepwise analysis with Chi-square test. Then we chose an entry probability of p&lt;0.05 by the stepwise selection method. Secondly, in the construction of SAPS II model, we used these time-stamp variables to do prediction based on the methods provided by the original literature of SAPS II. Thirdly, we performed XGBoost model to analysis the contribution (gain) of each variable to 30-days mortality, at the same time, backward stepwise analysis was processed to select the variable with a threshold of p&lt;0.05 according to the Akaike information criterion (AIC).\"</p> <p>Additionally, the below features are used to calculate SAPS-II:</p> <ul> <li>Age, GCS</li> <li>VITALS: Heart rate, systolic blood pressure, temperature</li> <li>FLAGS: ventilation/cpap</li> <li>IO: urine output</li> <li>LABS: PaO2/FiO2 ratio, blood urea nitrogen, WBC, potassium, sodium, HCO3</li> </ul> <p>Below is an image of which features were selected for both models</p>"},{"location":"research/dlh_study/#features-statistics","title":"Features statistics\u00b6","text":"<p>The target classification of this analysis is to accurately predict if a patient will die within 30 days of their first visit from sepsis. From the subsample of data present in BQ, there are 877 positive records (patients who died within 30 days), and 1443 negative records (patients who did not die within 30 days). All features are numerical, either discrete or continuous.</p> <p>For the model hyperparameters, not much is described in the paper such as number of epochs, scoring functions, learning rates, etc, so for this paper we will use standard approaches to each, as the hypothesis tested in the paper are less focused on hyperparameter tuning, and more about model selection strengths.</p>"},{"location":"research/dlh_study/#data-preprocessing","title":"Data preprocessing\u00b6","text":"<p>Again, not much is described in terms of data preparation and pre-processing - the focus of the paper is on parameter selection and model performance. Because of this, we will be using basic model preprocessing, which will involve the below steps:</p> <ol> <li>Download SQL for views, as well as authenticate BigQuery client</li> <li>Drop and reset derived and feature datasets</li> <li>Create views in BQ from raw data tables</li> <li>Create feature-selection views from all derived views and raw data tables</li> <li>Download data from the respective view for all models</li> <li>Drop records that are missing all non-target data</li> <li>Drop non-feature columns</li> <li>Feature normalization using sklearn MinMaxScaler, in order to scale data between 0 and 1 and prevent negative values</li> <li>Create train and test splits for data</li> </ol> <p>To preprocess the data, simply run the below 9 cells for each of the 9 steps specified above.</p>"},{"location":"research/dlh_study/#model","title":"Model\u00b6","text":"<p>The model includes the model definitation which usually is a class, model training, and other necessary parts.</p> <ul> <li>Model architecture: layer number/size/type, activation function, etc</li> <li>Training objectives: loss function, optimizer, weight of each loss term, etc</li> <li>Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc</li> <li>The code of model should have classes of the model, functions of model training, model validation, etc.</li> <li>If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it.</li> </ul>"},{"location":"research/dlh_study/#model-summary","title":"Model summary\u00b6","text":"<p>There are 3 models included in testing for this paper. They are not pretrained, and will be simply defined below:</p> <ol> <li>Linear regression, which will be made using the sklearn library</li> <li>XGBoost model, based on the XGBoost library. This is a type of ensemble method, similar to random forest, but includes features such as efficient regularization, parallel processing, and sparse data handling</li> <li>SAPS-II, which is a probabilistic model and does not require any additional training, as the disease mortality probability is included in the dataset</li> </ol> <p>Models 1 and 3 (Linear Regression and SAPS-II) are considered \"traditional\" approaches in the paper for predicting ICU mortality rates within timeframes. Often times, a certain threshold is applied to SAPS-II scores, but this paper does not reference one. The goal of the study is to show that more advanced machine learning approaches, such as XGBoost, can outperform traditional methods.</p>"},{"location":"research/dlh_study/#model-architecture","title":"Model architecture\u00b6","text":"<p>This study focuses heavily on standard approches and evaluation vs newer, less-tested approaches (XGBoost). However, there is little mentioned about model architecture details.</p> <ol> <li>For the linear regression model, the formula is very basic and has little tuning involved, so no details are specified or changed.</li> <li>For the XGBoost model, there are quite a few architecture parameters that can be changed, such as:</li> </ol> <ul> <li>n_estimators - the number of trees in the model</li> <li>max_depth - maximum depth of trees, which helps control how specialized trees are in the ensemble</li> <li>learning rate - the eta parameter, this controls how much each tree contributes to the overall model</li> </ul> <p>However, within the paper, there are no architecture parameters specified. Instead of trying to give an advantage to the XGBoost model via hyperparameters that were not specified in this paper, we will assume all default values for the model to compare the base-state of the XGBoost model to linear regression and SAPS-II results. 3. For the SAPS-II model, the probability is calculated based on Simplified Acute Physiology Score II, which is a measure of patient severity of illness. This score ranged from 0 and 163, with a predicted mortality of 0 to 1, with 1 being certain death. We will be using this predicted mortality for our model.</p>"},{"location":"research/dlh_study/#training-objectives","title":"Training objectives\u00b6","text":"<ol> <li>Linear regression has a non-customizable loss function based on the linear regression equation and we won't be customizing any loss hyperparameters</li> <li>For XGBoost, we will be using the default squarederror loss, as we do not want to make hyperparameter assumptions the authors didn't inform the readers of. This includes other hyperparameters including optimizers, learning steps, minimum loss, etc.</li> <li>SAPS-II is a probabilistic model and has no training involved</li> </ol>"},{"location":"research/dlh_study/#computation-requirements","title":"Computation requirements\u00b6","text":"<p>There are limited computational requirements for these models - the unchanged regressors have very limited number of weights, as well as hyperparameters. The number of trees, max depth, and other parameters discussed earlier, when left unchanged, have minimal requirements for the model. Any modern computer can host these models. The largest requirements come from being able to hold the data, which is done via bigquery. The dataframes are not holding much.</p>"},{"location":"research/dlh_study/#training","title":"Training\u00b6","text":""},{"location":"research/dlh_study/#hyperparameters","title":"Hyperparameters\u00b6","text":"<p>Within the paper, there is not anything listed for any of the hyperparameters. Because of this, I will be using default values for almost all hyperparameters. These are broken out by each model type below:</p>"},{"location":"research/dlh_study/#logistic-regression","title":"Logistic Regression\u00b6","text":"<ul> <li>penalty: This is the penalty term, \"l2\" is the default value</li> <li>n_jobs: How many jobs the scikit-learn backend should use in training. \"None\" is the default value, and sets the number of jobs to the maximum available</li> <li>random_state: The random state of data shuffling. Used for deterministic results</li> </ul>"},{"location":"research/dlh_study/#xgboost","title":"XGBoost\u00b6","text":"<ul> <li>booster: Specifies the type of model to use. \"gbtree\" is the default value, and uses a tree-based model</li> <li>objective: Sets the objective of the learning task as well as the learning objective</li> <li>eval_metric: Evaluation metrics for validation data. The default is mapped to the training objective</li> <li>eta: Learning rate - the default value is 0.3</li> <li>gamma: Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be. Default value is 0</li> <li>max_depth: Maximum depth of a tree. Default is 6</li> <li>min_child_weight: Minimum sum of instance weight (hessian) needed in a child. Default value is 1</li> <li>max_delta_step: Maximum delta step we allow each leaf output to be. Default value is 0</li> <li>subsample: Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Default is 1</li> <li>colsample_bytree: Subsampling ratio of columns when constructing tree. Default is 1</li> <li>base_score: The initial prediction score of all instances, global bias</li> <li>silent: Makes training mode silent</li> <li>seed: Random state</li> </ul>"},{"location":"research/dlh_study/#saps-ii","title":"SAPS-II\u00b6","text":"<p>This is a predictability model, and the model scores are already calculated using the formula <code>1 / (1 + exp(- (-7.7631 + 0.0737*(sapsii) + 0.9971*(ln(sapsii + 1))) ))</code>, where <code>sapsii</code> is the SAPS-II score.</p>"},{"location":"research/dlh_study/#computational-requirements","title":"Computational requirements\u00b6","text":"<p>Requirements for each model are very minimal due to datasets being limited and models being computationally minimal. Average runtime for training the XGBoost and Logistic Regression model take less than 3 seconds. The SAPS-II model probabilistic scores are created during data preprocessing. No GPus are required.</p>"},{"location":"research/dlh_study/#evaluation","title":"Evaluation\u00b6","text":"<p>For evaluation in this study, the authors used AUROC and DCA curves for comparisons. Additionally, we are going to use standard metrics, including:</p> <ul> <li>accuracy</li> <li>precision</li> <li>recall</li> <li>support</li> <li>F1</li> <li>confusion matrix</li> </ul> <p>The reason for these metrics is they can provide a general comparison and results analysis for our 3 models, and can help us understand the performance of each across certain metrics. Understanding accuracy, precision, sensitivity, recall, etc shows different strengths and weaknesses in each model.</p> <p>For our SAPS-II predictive model, a threshold must be defined for mortality classifications because this is a probabilistic model. The author's do not provide one, so we will assume that anything greater than or equal to probability 0.5 predicts \"1\", and anything less than 0.5 predicts \"0\".</p>"},{"location":"research/dlh_study/#results","title":"Results\u00b6","text":""},{"location":"research/dlh_study/#hypothesis-and-results-from-original-paper","title":"Hypothesis and Results from original paper\u00b6","text":"<p>From the original paper, the claim was that the XGBoost model can outperform Logistical Regression, and the standard probabilistic model SAPS-II. Additionally, I posed the 2 below hypotheses:</p> <ol> <li>Hypothesis 1: The XGBoost algorithm can outperform traditional logistic regression and SAPS-II scoring models in predicting 30-day mortality among patients with sepsis-3 based on scoring metrics.</li> </ol> <ul> <li>Corresponding experiment: Create a logistic regression model, a SAPS-II dataset, and an XGBoost model, and use the features specified by L. et al. [1] for these models. Then, analyze the results of both models in predicting 30-day mortality due to sepsis-3 and compare.</li> </ul> <ol> <li>Hypothesis 2: The set of features identified by the XGBoost model as significant predictors of 30-day mortality are significant compared to features not used</li> </ol> <ul> <li>Corresponding experiment: Perform several ablations, including dropping several features and adding several others, and compare post-training scores across different models to see which features area most important in each model's decisions, and how effective each model is.</li> </ul> <p>From the results acquired from my sub-study, we see below across metrics, as well as AUROC and DCA graphs, that hypotheses 1 is not quite true. Logistic Regression and XGBoost perform similarly, while both outperform the SAPS-II model. However, in the original paper, as discussed below, the XGBoost model does outperform both the Logistic Regression and SAPS-II model on all metrics. So, in this substudy, hypothesis 1 failed, but in the original study, hypothesis 1 held true.</p> <p>For hypothesis 2, we will discuss this further with an ablation study below.</p>"},{"location":"research/dlh_study/#experiments-beyond-the-original-paper","title":"Experiments beyond the original paper\u00b6","text":"<p>The original paper includes metrics solely focused on AUROC, DCA, and feature-analysis related metrics. However, there is quite a bit of value in looking at over-arching raw metrics, such as <code>precision</code>, <code>recall</code>, <code>f1</code>, <code>accuracy</code>, and <code>AUC</code>. Below, these metrics help us paint a story that clearly shows Logistic Regression outperforming both XGBoost and SAPS-II, while SAPS-II performs pretty poorly compared to the other models, and XGBoost is fairly \"middle of the road\". From the paper, we do not have these metrics available to us, but it would be interesting to see how each model would have performed with these summary statistics against each other. From the original study by L. et al. [1], the XGBoost model outperforms both other models - but would this hold true when we look at these summary statistics?</p>"},{"location":"research/dlh_study/#table-of-results","title":"Table of results\u00b6","text":"<p>In the table below, we can see the different metrics used compared against all 3 models. Across these metrics, we see the XGBoost and LogReg model performing fairly well, and the SAPS-II model performing poorly compared to the other models.</p>"},{"location":"research/dlh_study/#roc-curve","title":"ROC Curve\u00b6","text":"<p>Looking at the ROC curves below for each model type, we can see the difference at each threshold between models. The XGBoost model indicates better performance at lower thresholds, but is evenly matched, and sometimes even outperformed, by the Logistic Regression model, at mid-to-higher thresholds. The SAPS-II model indicated average-to-poor predictive performance, providing a minimal ROC curve more closely matching a linear increase.</p>"},{"location":"research/dlh_study/#dca-scores","title":"DCA Scores\u00b6","text":"<p>DCA, or decision-curve-analysis, is used to compare clinical usefulness and net benefits between each model. The analysis helps in understanding tradeoffs between the benefit of true positive predictions, and the harm of false positive predictions within a clinical context.</p> <p>From the DCA curve below, we can see that the Logistic Regression model and XGBoost model perform similarly at each threshold, with some thresholds showing the XGBoost model outperforming. The SAPS-II graph has poor results here, showing low benefit at each threshold.</p>"},{"location":"research/dlh_study/#model-comparison","title":"Model comparison\u00b6","text":"<p>Comparing the models created in my subsample study to the actual full study, results performed similarly for the Logistic Regression and XGBoost models. For the SAPS-II model, results were not as good, but this could be due to multiple factors that will be brought up in the \"Discussion\" section.</p> <p>The actual values for AUC, the ROC curve, and the DCA curves were not as impressive as the actual study - from the graphs below, I will compare the models I created, vs the models from the study. While trends were similar, results were simply better from the study. This could be due to multiple factors, such as the sample selection for my case study failed to capture certain features, or parameter selection differed due to unwritten differences the authors did not provide.</p> <p>The authors do not provide other summary metrics such as accuracy, precision, recall, or F1-scores, but we can see similar trends matching in this study vs from L. et al. [1].</p>"},{"location":"research/dlh_study/#comparing-roc-curves","title":"Comparing ROC curves\u00b6","text":"<p>We can see similar differences in the Logistics Regression model vs the XGBoost model. The SAPS-II model provided outperforms from L. et al. [1] vs in this sub-study.</p>"},{"location":"research/dlh_study/#comparing-dca-curves","title":"Comparing DCA curves\u00b6","text":""},{"location":"research/dlh_study/#nomogram","title":"Nomogram\u00b6","text":"<p>The authors also provide a nomogram, which helps us see how important each feature is to sepsis mortality rates in this study. In the chart below, we can see how each feature is relevant for scoring in mortality predictions, and assign point values to different features. This is helpful in understanding how each feature plays a part in mortality scoring.</p>"},{"location":"research/dlh_study/#ablation-study","title":"Ablation Study\u00b6","text":"<p>For this study by L. et al [1], there is a heavy emphasis placed on feature selection for each model type. In the image directly above this one, we can see the nomogram from L. et al to see how important the range of each feature is in overall predictive power of a model. Additionally, in Figure 1 of this notebook, we can see the features selected as well as related p-scores.</p> <p>For this ablation study, I am taking a practical approach, and will be testing model results if the <code>age</code> and <code>sodium_max</code> columns are removed from the Logistical Regression model and XGBoost model - the SAPS-II model is an additive-probabilstic model, and falls apart somewhat if certain features are removed, and wasn't very impressive to begin with in this sub-study. So for this ablation section, we will focus only on the former two models.</p> <p>Age is a strong predictor for any mortality study, so removing to try and compare results from other indicators could show potentially interesting results. For sodium_max, this is a feature that has a p-value &lt; 0.001 for the Logistic Regression model, but for the XGBoost model, it is far less significant of a feature with a p-value = 0.03835.</p> <p>Below, we can see directly how important each feature is to the model. For Logistic Regression, we can see that <code>age</code> and <code>sodium_max</code> are fairly important, and the same is true for the XGBoost model.</p>"},{"location":"research/dlh_study/#ablation-results","title":"Ablation Results\u00b6","text":""},{"location":"research/dlh_study/#discussion","title":"Discussion\u00b6","text":""},{"location":"research/dlh_study/#reproducability","title":"Reproducability\u00b6","text":"<p>The study by L. et al. [1] was reproducable to an extent - if a decision was made to invest more into creating an accurate data representation, I believe the study would be fully reproducable, consdering the study by L. et al. [1] uses the MIMIC-III dataset. However, due to the size of the dataset, and the hardware constraints coupled with timing constraints, a sample was chosen for this study that provided somewhat different results, although similar trends were seen.</p> <p>The SAPS-II results were fairly different from what was seen by L. et al. [1]. This could be due to multiple factors - a lot of assumptions had to be made regarding all models because there were no discussions on model creation, parameter selection, architectures chosen, probabilistic functions, preprocessing steps, etc. Because of this, the models seen in this notebook could be significantly different from the ones used by L. et al. [1]. That being said, with feature selection done already by the authors, we can at least get somewhat comparable results to what the authors had.</p>"},{"location":"research/dlh_study/#what-was-easy","title":"What was easy\u00b6","text":"<p>Model creation was actually fairly easy from this study - most of this is due to the fact that the models were not specified in any way, other than name. Because of this, minimal assumptions were made regarding model architecture, and training requirements were easy. Additionally, due to taking a sample of MIMIC-III data as opposed to the entire dataset, training times and requirements were minimal.</p> <p>Feature selection was straightforward as well - the authors described their methods for selecting features, and creating datasets based on this left minimal assumptions for recreating this study.</p>"},{"location":"research/dlh_study/#what-was-not-easy","title":"What was not easy\u00b6","text":"<p>Trying to understand what types of model choices the authors made was difficult - while actually creating and training the models was not difficult, trying to create an accurate replica was. Additionally, preprocessing raw MIMIC-III data was a difficult task, as features were called out by L. et al. [1], but having minimally interacted with MIMIC-III data, trying to scrape together the correct features and understand these features took most of the time involved in this study.</p> <p>Additionally, creating an accurate sample of the data, as well as hosting it in an accessable format, took planning and trial-and-error as well. Some databases and flavors of SQL do not support necessary date functions, or don't have portability between a simple notebook that can be ran anywhere.</p>"},{"location":"research/dlh_study/#suggestions-for-the-author","title":"Suggestions for the author\u00b6","text":"<p>The study was an excellent read on understanding mortality in ICU settings, traditional approaches on predicting 30-day mortality, and potential improvements to these predictions. What I would suggest is perhaps talking more on technical choices after feature selection - why use MIMIC-III, for example? Why the XGBoost model, instead of a random forest or some other ensemble method? With XGBoost, what hyperparameters were selected, such as learning rate, epochs, batch size, tree depth, child nodes, etc. XGBoost has a lot of customizability, which leads to some models training on the same dataset with drastically different results. Understanding the author's choices here would be helpful to any reader.</p> <p>Additionally, more baseline evaluation metrics would be helpful - we can see how each feature is relevant repeatedly, and see the ROC graphs and AUC scores, and the DCA analysis and nomogram and CDC graph. However, how do these models perform against one another within different metrics? Trying to understand F1-scores from my sub-study vs the actual study is somewhat fruitless, as there is no F1-score mentioned.</p> <p>Lastly, what thresholds were used for probabilistic predictions? For SAPS-II, for example, mortality probabilities are given, but the threshold decided for binary classification is entirely up to the author. One can assume that 0.5 is standard and was used in this case, but an explicit callout for this would change results drasitically.</p>"},{"location":"research/dlh_study/#references","title":"References\u00b6","text":"<ol> <li>Hou, N., Li, M., He, L. et al. Predicting 30-days mortality for MIMIC-III patients with sepsis-3: a machine learning approach using XGboost. J Transl Med 18, 462 (2020). https://doi.org/10.1186/s12967-020-02620-5</li> </ol>"},{"location":"research/insurance_charges/","title":"Exploring medical bills across an individual's features","text":"<p>It's another day in America - you wake up, enjoy some coffee to fight through that morning groginess, and crack open the paper. After your normal routine, it's time to start your day, and you head outside to warm your vehicle up to get to work. As you take your first step, the ground beneath you disappears and a sinkhole eats your entire right leg, then immediately closes. It looks like the ground has become sentient and has a craving for legs, but only right legs oddly enough. It happens so fast you can't react, but when your brain catches up with what is happening, you scramble away from the sinkhole. This isn't good - you need to go to the hospital, but the thought of that potential hospital bill makes you faint. After a moment, you regain consciousness and decide that it's ok; you have health insurance, and it can't be that bad. What does a sinkhole-bite replacement surgery cost, anyway? As your leg continues to bleed and you really need to get to the hospital, your curious nature continues to preoccupy you, and you can't help but wonder: do you know the price of anything at a hospital? Is there a way to know how much I'm going to pay before hand?</p> <p>These are all great questions that an unfortunate number of Americans don't have the answer to. And while none of us are going to be eaten by a sentient sinkhole tomorrow (I hope), you never know what the day may bring and if a hospital visit is included. Even more nebulous is the cost of that visit, both to you and insurance provider, and what you end up with. Throughout this read, we're going to explore what factors about an individual relate to their average hospital charges, how well can we can predict these charges based off of those factors, and drawing conclusions.</p>"},{"location":"research/insurance_charges/#what-data-will-we-be-using","title":"What data will we be using?","text":"<p>For this article, we'll be using a dataset from the book <code>Machine Learning with R by Brett Lantz</code> - specifically, this book provides a dataset that has medical billing data. The columns in this dataset contain an individual's <code>age</code>, <code>sex</code>, <code>bmi</code>, number of <code>children</code>, <code>smoker</code>, <code>region</code> of the U.S., and the amount their insurance was billed, or <code>charges</code>. This dataset is relatively simple - it's purpose is almost entirely educational. This matches the purpose of this article, to explore the basics of data visualization and predictive modeling.</p>"},{"location":"research/insurance_charges/#exploring-the-data","title":"Exploring the data","text":"<p>Looking at the attributes available, we can start by visualizing how each one is directly correlated to the <code>charges</code> column. To do this, we can create bar charts for the non-continuous variables, which are <code>sex</code>, <code>children</code>, <code>smoker</code>, and <code>region</code>. For the continuous variables, <code>age</code> and <code>bmi</code>, we can review line charts that display the average cost to insurance and identify what the trend is for these variables.</p>"},{"location":"research/insurance_charges/#sex","title":"Sex","text":"<p>Looking at the first categorical variable, <code>sex</code> groups the average cost of medical charges by male and female. We don't immediately see any extreme difference - males are, on average, charged a little higher, although it's a fairly small difference.</p>"},{"location":"research/insurance_charges/#number-of-children","title":"Number of children","text":"<p>Next, we have number of <code>children</code>. Here we can more of a disparity between groups, but not in the way one might expect - individuals with 0 children have smaller bills charged to insurance, on average, than individuals with 1 child. The same applies for individuals with 1 child to individuals with 2 children, and the same for individuals with 2 kids vs individuals with 3 kids. However, people with 4 and 5 kids are billed less on average than their counterparts with fewer kids. So we can't call this a linear correlation, but still an interesting trend. The reason for this could be indirect, or something entirely unrelated to having kids.</p>"},{"location":"research/insurance_charges/#smoker-vs-non-smoker","title":"Smoker vs non smoker","text":"<p>Now we take a look at <code>smoker</code> vs non smoker groups. The difference is pretty shocking - smokers' insurance are billed roughly 4x as much as their non-smoking counterparts. Smoking has been proven to be extremely unhealthy, and the lifestyle can impact medical costs heavily, as seen here.</p>"},{"location":"research/insurance_charges/#regions","title":"Regions","text":"<p>Last for categorical variables is the <code>regions</code>. We have a breakdown of 4 groups - northeast, northwest, southeast, and southwest. In the bar chart above, there are no strong differences between any region. Similar to sex, we do see some range on the averages presented, but there are far too many factors to be able to say what causes this, and the difference is fairly minimal.</p>"},{"location":"research/insurance_charges/#age","title":"Age","text":"<p>Moving on to the first of our continuous variables, we have <code>age</code>. The trend for this shows that as age increases, so do the average medical costs billed. This may seem to make complete sense, but there are additional considerations mentioned below that don't fall into the scope of this dataset.</p>"},{"location":"research/insurance_charges/#bmi","title":"BMI","text":"<p>Lastly, we have <code>bmi</code>, or body mass index. This is measured as a person's weight divided by their height. In the chart above, we can see a strong upward trend; as BMI increases, so does the average medical cost billed.</p>"},{"location":"research/insurance_charges/#thoughts","title":"Thoughts","text":"<p>Shown above, we can see some trends that we would expect to see - that smoking, age, and BMI are all heavily impactful on medical costs. However, the dataset provided here is limited in scope. While it would be easy to assume that age directly increases medical costs on average, this data is looking at the amount charged to insurance. So for a younger population who may not have insurance, or may not be comfortable going to a doctor and will wait 10 years before the problem is unignorable, we may not have a fully accurate representation of these features vs medical costs.</p> <p>To get a better understanding of what group of features may be most responsible for higher bills, we can split this data in deciles based on the amount charged to insurance, then take the top 10% decile and get a row count for each possible group of features. Because the number of features is so small, this is possible for us, but it wouldn't normally be if there were more columns or more possible values for the columns we have. </p>"},{"location":"research/insurance_charges/#highest-average-bills","title":"Highest average bills","text":"<p>After getting the top 10% most expensive charges, which is <code>134 rows</code> of data with an average charge amount of <code>$42,378</code> and getting the number of possible combinations of features out of our 6 columns, the total comes to <code>88,320</code> different unique groups of features. The table below lists what features are most commonly seen, and in how many rows that value is seen.</p> Age Sex BMI # Children Smoker Region Age 37: 7Age 43: 66Age 22: 5Age 60: 5Age 44: 5 Male: 84Female: 50 36 BMI: 1834 BMI: 1531 BMI: 1337 BMI: 1235 BMI: 11 0 children: 482 children: 341 child: 313 children: 194 children: 2 Smoker: 131Non-smoker: 3 Southeast: 55Southwest: 32Northeast: 27Northwest: 20 <p>Some interesting trends to be seen in the table above - for example, almost every individual in the top 10% are smokers. Additionally, without making the page significantly longer, most of the expensive bills have a BMI that labels them as \"obese\". On the other side of the spectrum, the age was fairly diverse, except for 43 year olds - this is likely just due to natural aging, as well as the sample mostly comprising of this age range.</p>"},{"location":"research/insurance_charges/#lowest-average-bills","title":"Lowest average bills","text":"<p>After getting the bottom 10% least expensive charges, which is <code>134 rows</code> of data with an average charge amount of <code>$1,796</code> and getting the number of possible combinations of features out of our 6 columns, the total comes to <code>6,264</code> different unique groups of features. The table below lists what features are most commonly seen, and in how many rows that value is seen.</p> Age Sex BMI # Children Smoker Region Age 18: 46Age 19: 38Age 21: 14Age 20: 12Age 22: 12 Male: 82Female: 52 26 BMI: 930 BMI: 934 BMI: 829 BMI: 828 BMI: 7 0 children: 1181 child: 152 children: 1 Smoker: 0Non-smoker: 134 Southeast: 47Southwest: 38Northeast: 25Northwest: 25 <p>A lot of what we saw in highest cost billed matches for this table as well - this is likely due to how large a sample size is out of the entire population. For example, we can see still more males fall into the bottom 10% of bills than females, and still more individuals from the southeast. However, we also see that the lower age ranges (18-22) are the most common ages for cheapest bills. Additionally, the BMI ranges are slightly lower, and there are absolutely no smokers.</p>"},{"location":"research/insurance_charges/#predictive-modeling","title":"Predictive modeling","text":"<p>After exploring the data and understanding that some features can impact the medical bill more than others, let's see if we can create some probabilstic modeling - first, we can start with a basic feed forward neural network, and see if any other regression models or probabilstic models can perform better.</p>"},{"location":"research/insurance_charges/#neural-network","title":"Neural network","text":"<p>For the neural network, we're going to start by creating an attention model with encoder and decoder layers, and provide multiple transformation layers to... actually, we don't need to do any of that. While AI has been getting incredibly popular and we've made some truly impressive advancements, this is a simple dataset. There's no need to build a crazy network - let's start with a basic model using <code>tensorflow</code>.</p> <p>We'll start by importing our libraries and preprocessing our data using <code>sklearn</code>: <pre><code>#Preprocess data - #Start by converting string to int\ndf['sex'] = pd.Categorical(df['sex']).codes\ndf['children'] = pd.Categorical(df['children']).codes\ndf['smoker'] = pd.Categorical(df['smoker']).codes\ndf['region'] = pd.Categorical(df['region']).codes\n\n#Create transformer\ntransformer = make_column_transformer(\n    (MinMaxScaler(), ['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges'])\n)\n\n#Transform and split data\ntransformer.fit(df)\ndata = transformer.transform(df)\n\n#Split data\ny_data = data[:,-1]\nX_data = df.iloc[:,:-1]\n\ntest_size = 100\nX_train, X_test = X_data[:-test_size], X_data[-test_size:]\ny_train, y_test = y_data[:-test_size], y_data[-test_size:]\n</code></pre></p> <p>In just a few lines of code, we've preprocessed our data to scale between 0 and 1 using the <code>MinMaxScaler</code> of sklearn, and split the data into train samples and 100 test samples. Now, we can make our model - we can create a basic 3 layer network, with a batch size of 8 and 50 epochs. We'll use the standard <code>Adam</code> optimizer, as well as <code>relu</code> activations on each layer except the last - the reason for this is <code>relu</code> will prevent negative output from coming through, and some predictions could potentially be negative.</p> <pre><code>#Create model\nmodel = Sequential()\nmodel.add(Dense(256, input_shape=(X_data.shape[1],), activation=\"relu\"))\nmodel.add(Dense(128, activation=\"relu\"))\nmodel.add(Dense(1))\n\n#Compile and fit model\nmodel.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(), metrics='mse')\nmodel.fit(X_train, y_train, batch_size=8, epochs=50)\n</code></pre> <p>Great! We've run our model, let's see what our mean squared error is to check the accuracy of our model, and how our predictions look.</p> <p></p> <p>These are impressive results - we can see that the predictions of the model almost exactly match that of the true labels. The overall mean squared error here is only 0.01405. For many projects, these results would be above passing and could work for many applications - but can we do better?</p>"},{"location":"research/insurance_charges/#ensemble-methods","title":"Ensemble methods","text":"<p>With the <code>sklearn</code> library available, we can use over 200 different kinds of estimators. This article could be a lot longer if we wanted to step through each one. Instead, we'll use a few basic <code>ensemble</code> methods. Ensembles are groups of classifiers that work together to achieve better performance, and can often perform better than large independent classifiers. These are great for regression too - the ensemble methods we'll use <code>AdaBoost</code>, <code>RandomForest</code>, and <code>GradientBoost</code>.</p> <p>Sklearn also makes it incredibly easy to use any of these models. The below code will create the different ensembles, fit and train them, then create predictions <pre><code>from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor\n\nensembles = [AdaBoostRegressor(), RandomForestRegressor(), GradientBoostingRegressor()]\nfor e in ensembles:\n    e.fit(X_train, y_train)\n    preds = e.predict(X_test)\n</code></pre></p> <p>Now that we have our ensemble methods fit and predicting, let's see how they did</p> <p> </p> <p>Once again, these results are ideal, and they even outperformed the neural network! While the MSE for the neural network was roughly 0.01405, each of these is significantly lower, with the RandomForest dropping under 0.004. With no doubt, if better results were necessary, some hyperparameter optimization and other fine tuning could push this mean squared error value even lower.</p>"},{"location":"research/insurance_charges/#conclusions","title":"Conclusions","text":"<p>After having a bad start to your day and having your leg eaten by a sinkhole, you were faced with the dilemma - what factors go into medical costs? Am I likely to be charged more? To be honest, we can't answer that very well - no one has ever had their leg eaten by a sinkhole like this before. However, we can make some confident predictions based on your <code>age</code>, <code>sex</code>, whether you are a <code>smoke</code>, and <code>BMI</code> as to what your average bill will be. Additionally, we can better understand what information would be beneficial in continuing this exploration of medical prices, such as what group is more likely to have insurance, what group is more likely to actually go to the doctor if there is an issue, etc. With this information in hand, you feel prepared to tackle the day. You are now 10 minutes late for work after reading this. Sorry about that.</p>"}]}